{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23edd61c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jesusvillota/CSS_DataScience_2025/blob/main/Session2/2_3_RAG_II_Document_Splitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee13d33",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 880px; margin: 20px auto 22px; padding: 0px; border-radius: 18px; border: 1px solid #e5e7eb; background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%); box-shadow: 0 8px 26px rgba(0,0,0,0.06); overflow: hidden;\">\n",
    "\n",
    "  <!-- Banner Header -->\n",
    "  <div style=\"padding: 34px 32px 14px; text-align: center; line-height: 1.38;\">\n",
    "    <div style=\"font-size: 13px; letter-spacing: 0.14em; text-transform: uppercase; color: #6b7280; font-weight: bold; margin-bottom: 5px;\">\n",
    "      Session #2\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      RAG with LangChain\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      Part II: Document Splitting\n",
    "    </div>\n",
    "    <div style=\"font-size: 16.5px; color: #374151; font-style: italic; margin-bottom: 0;\">\n",
    "      Using Textual Data in Empirical Monetary Economics\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Logo Section -->\n",
    "  <div style=\"background: none; text-align: center; margin: 30px 0 10px;\">\n",
    "    <img src=\"https://www.cemfi.es/images/Logo-Azul.png\" alt=\"CEMFI Logo\" style=\"width: 158px; filter: drop-shadow(0 2px 12px rgba(56,84,156,0.05)); margin-bottom: 0;\">\n",
    "  </div>\n",
    "\n",
    "  <!-- Name -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1.22em; font-weight: bold; margin-bottom: 0px;\">\n",
    "    Jesus Villota Miranda Â© 2025\n",
    "  </div>\n",
    "\n",
    "  <!-- Contact info -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1em; margin-top: 7px; margin-bottom: 20px;\">\n",
    "    <a href=\"mailto:jesus.villota@cemfi.edu.es\" style=\"color: #38549c; text-decoration: none; margin-right:8px;\" title=\"Email\">\n",
    "      <!-- Email logo -->\n",
    "      <!-- <img src=\"https://cdn-icons-png.flaticon.com/512/11679/11679732.png\" alt=\"Email\" style=\"width:18px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      jesus.villota@cemfi.edu.es\n",
    "    </a>\n",
    "    <span style=\"color:#9fa7bd;\">|</span>\n",
    "    <a href=\"https://www.linkedin.com/in/jesusvillotamiranda/\" target=\"_blank\" style=\"color: #38549c; text-decoration: none; margin-left:7px;\" title=\"LinkedIn\">\n",
    "      <!-- LinkedIn logo -->\n",
    "      <!-- <img src=\"https://1.bp.blogspot.com/-onvhHUdW1Us/YI52e9j4eKI/AAAAAAAAE4c/6s9wzOpIDYcAo4YmTX1Qg51OlwMFmilFACLcBGAsYHQ/s1600/Logo%2BLinkedin.png\" alt=\"LinkedIn\" style=\"width:17px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      LinkedIn\n",
    "    </a>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73939b22",
   "metadata": {},
   "source": [
    "**IMPORTANT**: **Are you running this notebook in Google Colab?**\n",
    "\n",
    "- If so, please make sure that in the cell below `running_in_colab` is set to `True`\n",
    "\n",
    "- And, of course,  make sure to **run the cell**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245f50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARE YOU RUNNING THIS IN GOOGLE COLAB? If YES, type True below\n",
    "running_in_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb515b",
   "metadata": {},
   "source": [
    "# Document Splitting\n",
    "\n",
    "After loading documents into a standard format, the next step in building a Retrieval Augmented Generation (RAG) system is splitting them into smaller, manageable chunks. This process happens after document loading but before storing documents in a vector database.\n",
    "\n",
    "![](images/rag_pipeline.png)\n",
    "\n",
    "While document splitting might sound straightforward, there are many subtleties that significantly impact the effectiveness of your RAG system. If text is split improperly, you might end up with chunks that separate semantically related information, making it difficult to retrieve the complete context needed to answer questions correctly.\n",
    "\n",
    "For example, if a sentence about a car's specifications is split into two separate chunks, when someone asks about those specifications, neither chunk alone would contain the complete answer. Good document splitting ensures that semantically relevant information stays together in the same chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd5b43",
   "metadata": {},
   "source": [
    "## Understanding Text Splitting Parameters\n",
    "\n",
    "All text splitters in LangChain operate on two key parameters:\n",
    "\n",
    "1. **Chunk Size**: This determines the size of each chunk. The size can be measured in different ways, commonly by character count or token count. We define this with the `chunk_size` parameter.\n",
    "\n",
    "2. **Chunk Overlap**: This creates an overlap between adjacent chunks, like a sliding window. Having overlapping text ensures that context isn't lost at the boundaries between chunks. For example, if important information spans the end of one chunk and the beginning of another, the overlap ensures it's captured in both chunks. We define this with the `chunk_overlap` parameter.\n",
    "\n",
    "LangChain text splitters provide two main methods:\n",
    "- `split_text()`: Takes a list of text strings and splits them\n",
    "- `split_documents()`: Takes a list of document objects and splits them while preserving metadata\n",
    "\n",
    "For this demonstration, we'll use small values for chunk size (26) and overlap (4) to clearly illustrate how splitting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d60fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running_in_colab: \n",
    "    ! pip install langchain\n",
    "    ! pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2723bf02",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "chunk_size = 26\n",
    "chunk_overlap = 4\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61167fc",
   "metadata": {},
   "source": [
    "## Basic Splitting Examples\n",
    "\n",
    "Let's start with some simple examples to understand how text splitting works. First, we'll try a string that's exactly the same length as our chunk size.\n",
    "\n",
    "Why doesn't this split the string below? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd21401b",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text1 = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa92a860",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dab008",
   "metadata": {},
   "source": [
    "Notice that the string wasn't split. This is because the string is exactly 26 characters long, which matches our specified chunk size of 26. The text splitter only splits text when it exceeds the chunk size, so in this case, no splitting was necessary.\n",
    "\n",
    "Now let's try a longer string that exceeds our chunk size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b5da0f9",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea0d456",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f44ad",
   "metadata": {},
   "source": [
    "Now we can see the text has been split into two chunks! \n",
    "\n",
    "The first chunk contains the first 26 characters (abcdefghijklmnopqrstuvwxyz), which is exactly our chunk size. The second chunk starts with \"wxyzabcdefg\", where the first 4 characters \"wxyz\" represent our chunk overlap. This overlap creates a sliding window effect between chunks, ensuring that context at the boundaries isn't lost.\n",
    "\n",
    "The 4-character overlap (wxyz) appears at the end of the first chunk and the beginning of the second chunk, helping maintain continuity between the chunks. This is especially important for maintaining context when chunks are later processed independently.\n",
    "\n",
    "Let's try a more complex example with spaces between characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87200022",
   "metadata": {},
   "source": [
    "With spaces between characters, the string takes up more space, leading to different splitting behavior. The recursive character text splitter will count spaces as characters when measuring chunk size. Let's see how our text gets split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce39d64",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d96c0ab9",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0f1b7",
   "metadata": {},
   "source": [
    "With spaces included, the string is split into three chunks because the spaces count as characters, making the total string length exceed our chunk size multiple times. \n",
    "\n",
    "If we look at the overlap between chunks, we can see that the first chunk ends with \"...k l m\" and the second chunk begins with \"l m n...\". These overlapping characters (including spaces) form our chunk overlap of 4 characters. While \"l m\" might appear to be just two letters, the spaces before and after them count toward the total of 4 characters in the overlap.\n",
    "\n",
    "Now let's try using the `CharacterTextSplitter` instead of the `RecursiveCharacterTextSplitter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dbea42c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m n o p q r s t u v w x y z']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4cba9",
   "metadata": {},
   "source": [
    "Interestingly, the `CharacterTextSplitter` doesn't split the text at all! This is because, by default, the `CharacterTextSplitter` uses newline characters (\"\\n\") as separators, and our text doesn't contain any newlines.\n",
    "\n",
    "Unlike the `RecursiveCharacterTextSplitter` which can split on multiple separator types in order, the `CharacterTextSplitter` only splits on a single specified separator character. Let's modify the `CharacterTextSplitter` to use spaces as separators instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e71644",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator = ' '\n",
    ")\n",
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86ac52",
   "metadata": {},
   "source": [
    "Now that we've specified a space as the separator, the `CharacterTextSplitter` splits the text in a similar way to the `RecursiveCharacterTextSplitter`. This demonstrates the importance of choosing the right separator for your specific text.\n",
    "\n",
    "The key difference between these two splitters:\n",
    "- **`RecursiveCharacterTextSplitter`**: Uses a list of separators in order of priority (first double newlines, then single newlines, then spaces, and finally character by character if needed)\n",
    "\n",
    "- **`CharacterTextSplitter`**: Uses only one separator (by default a newline character) and won't split at all if that separator isn't present\n",
    "\n",
    "This is why the `RecursiveCharacterTextSplitter` is generally recommended for generic text as it can adapt to different text structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b613ee6-6d34-4504-b00d-bb0dbf414cfe",
   "metadata": {},
   "source": [
    "Try your own examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4d0ed",
   "metadata": {},
   "source": [
    "## Recursive splitting details\n",
    "\n",
    "`RecursiveCharacterTextSplitter` is recommended for generic text because it intelligently tries different separators in sequence. \n",
    "\n",
    "The recursive splitter works by:\n",
    "1. First trying to split on double newlines (`\\n\\n`), which typically separate paragraphs\n",
    "2. If chunks are still too large, it tries splitting on single newlines (`\\n`)\n",
    "3. If still too large, it splits on spaces, which separate words\n",
    "4. As a last resort, it splits character by character\n",
    "\n",
    "This hierarchy of separators helps maintain the semantic structure of the text, keeping related content together when possible. Let's see how it works on a more realistic text example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96926a42-92e6-4ad4-9946-908a075fb32d",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \n",
      "\n",
      "  Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.\n"
     ]
    }
   ],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\"\n",
    "\n",
    "print(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18706d73",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c25fa3",
   "metadata": {},
   "source": [
    "We can see that this text is about 500 characters long. It contains a natural paragraph break with a double newline (`\\n\\n`), which is a typical separator between paragraphs. \n",
    "\n",
    "Let's set up our splitters with a larger chunk size (450 characters) to see how they handle this more realistic text. We'll explicitly specify the separators for the `RecursiveCharacterTextSplitter` to show how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5083a172-ba0a-4d9f-b561-18f0d2e6d9ce",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 450\n",
    "chunk_overlap = 0\n",
    "\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator = ' '\n",
    ")\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8aa4ed-5347-4819-b3c1-d15696832a65",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,',\n",
       " 'have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6af907f2-4110-4e40-a831-593f8592bd83",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a367d",
   "metadata": {},
   "source": [
    "Notice the difference in how these two splitters handle the text:\n",
    "\n",
    "1. **CharacterTextSplitter**: Splits on spaces, which can result in awkward breaks in the middle of sentences. This produces chunks that might not be semantically cohesive.\n",
    "\n",
    "2. **RecursiveCharacterTextSplitter**: First tries to split on double newlines (`\\n\\n`), which results in two paragraphs being kept intact. This is a more natural and semantically meaningful split, even though the first chunk is shorter than the maximum 450 characters we specified.\n",
    "\n",
    "This demonstrates why the recursive splitter is often better for natural text - it respects the document's structure, keeping paragraphs together rather than splitting arbitrarily in the middle of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e429b",
   "metadata": {},
   "source": [
    "## Real-World Example: Splitting PDF Documents\n",
    "\n",
    "Now let's apply what we've learned to a real-world example. We'll load a PDF document and split it into manageable chunks that can be used in a RAG system.\n",
    "\n",
    "For PDFs, we typically need larger chunk sizes than our toy examples, as PDFs contain substantial amounts of content. We'll use the PyPDFLoader to load a sample PDF document, and then apply our text splitting techniques to it.\n",
    "\n",
    "PDFs present unique challenges because they might have complex layouts, multiple columns, headers, footers, and other elements that can affect how the text should be split. Additionally, preserving the original page number in the metadata is important for attribution and reference.\n",
    "\n",
    "Let's load a PDF file and see how the document splitting works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "783d5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if running_in_colab: \n",
    "    ! pip install pypdf\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    folder_dir = '/content/gdrive/My Drive'\n",
    "else: \n",
    "    folder_dir = 'docs'\n",
    "\n",
    "os.makedirs(folder_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a469e07d",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 303 0 (offset 0)\n",
      "Ignoring wrong pointing object 376 0 (offset 0)\n",
      "Ignoring wrong pointing object 378 0 (offset 0)\n",
      "Ignoring wrong pointing object 658 0 (offset 0)\n",
      "Ignoring wrong pointing object 732 0 (offset 0)\n",
      "Ignoring wrong pointing object 738 0 (offset 0)\n",
      "Ignoring wrong pointing object 740 0 (offset 0)\n",
      "Ignoring wrong pointing object 746 0 (offset 0)\n",
      "Ignoring wrong pointing object 756 0 (offset 0)\n",
      "Ignoring wrong pointing object 761 0 (offset 0)\n",
      "Ignoring wrong pointing object 763 0 (offset 0)\n",
      "Ignoring wrong pointing object 765 0 (offset 0)\n",
      "Ignoring wrong pointing object 770 0 (offset 0)\n",
      "Ignoring wrong pointing object 966 0 (offset 0)\n",
      "Ignoring wrong pointing object 980 0 (offset 0)\n",
      "Ignoring wrong pointing object 983 0 (offset 0)\n",
      "Ignoring wrong pointing object 985 0 (offset 0)\n",
      "Ignoring wrong pointing object 996 0 (offset 0)\n",
      "Ignoring wrong pointing object 1010 0 (offset 0)\n",
      "Ignoring wrong pointing object 1022 0 (offset 0)\n",
      "Ignoring wrong pointing object 1026 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(f\"{folder_dir}/paper.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bba6f05d",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1aef7",
   "metadata": {},
   "source": [
    "For real-world documents like PDFs, we typically use larger chunk sizes. Here we're using a chunk size of 1000 characters with an overlap of 150 characters, which provides a good balance for most applications.\n",
    "\n",
    "We're also explicitly specifying the `length_function` parameter as Python's built-in `len()` function, which counts characters. This is the default, but we include it here for clarity. For PDFs, splitting on newlines is often effective as PDFs naturally have line breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c752a663-896b-4a24-9ffb-7640642b7a3f",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7474a52c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2687cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ð Doc 0: Predicting Market Reactions to News:\n",
      "An LLM-Based Approach Using Spanish Business Articles\n",
      "Jesus Villota â\n",
      "Abstract\n",
      "Markets do not always eï¬ciently incorporate news, particularly when information is complex or\n",
      "ambiguous. Traditional text analysis methods fail to capture the economic structure of information\n",
      "and its ï¬rm-speciï¬c implications. We propose a novel methodology that guides LLMs to systematically\n",
      "identify and classify ï¬rm-speciï¬c economic shocks in news articles according to their type, magnitude,\n",
      "and direction. This economically-informed classiï¬cation allows for a more nuanced understanding of\n",
      "how markets process complex information. Using a simple trading strategy, we demonstrate that our\n",
      "LLM-based classiï¬cation signiï¬cantly outperforms a benchmark based on clustering vector embeddings,\n",
      "generating consistent proï¬ts out-of-sample while maintaining transparent and durable trading signals.\n",
      "\n",
      "ð Doc 1: generating consistent proï¬ts out-of-sample while maintaining transparent and durable trading signals.\n",
      "The results suggest that LLMs, when properly guided by economic frameworks, can eï¬ectively identify\n",
      "persistent patterns in how markets react to diï¬erent types of ï¬rm-speciï¬c news. Our ï¬ndings contribute\n",
      "to understanding market eï¬ciency and information processing, while oï¬ering a promising new tool for\n",
      "analyzing ï¬nancial narratives.\n",
      "JEL Codes: G12, G14, C45, C58, C63, D83\n",
      "Keywords: Large Language Models, Business News, Stock Market Reaction, Market Eï¬ciency\n",
      "â CEMFI, Calle Casado del Alisal, 5, 28014, Madrid, Spain. I am deeply grateful to Enrique Sentana, Manuel Arellano, Dante\n",
      "Amengual, Rafael Repullo, Javier SuÃ¡rez, David MartÃ­nez-Miera, Julio Crego, and Francisco PeÃ±aranda for their comments. I am\n",
      "especially indebted to NicolÃ¡s Forteza and MatÃ­as Covarrubias, whose invaluable guidance and insights have greatly enriched this\n",
      "\n",
      "ð Doc 2: especially indebted to NicolÃ¡s Forteza and MatÃ­as Covarrubias, whose invaluable guidance and insights have greatly enriched this\n",
      "work. I also thank discussants for their helpful comments, as well as the participants of the Banking & Finance Seminar (CEMFI), the\n",
      "Generative AI in Finance Conference (Concordia University), the Mirian AndrÃ©s Seminar (University of La Rioja), the 3rd Contemporary\n",
      "Issues in Financial Markets and Banking (Nottingham Trent University), the Sao Paulo School of Advanced Science in High Dimensional\n",
      "Models (FGV) and the Barcelona Summer Forum: Machine Learning in Economics (BSE). Finally, I gratefully acknowledge ï¬nancial\n",
      "support from Banco de EspaÃ±a (BdE).\n",
      "\n",
      "ð Doc 3: 1. Introduction\n",
      "In ï¬nancial markets, news play a pivotal role in shaping stock prices. Every day, market participants\n",
      "respond to a broad spectrum of news ranging from ï¬rm-speciï¬c announcements, such as earnings releases,\n",
      "to macroeconomic events, such as central bank interest rate announcements, or geopolitical developments,\n",
      "like international trade conï¬icts or political elections. The Eï¬cient Market Hypothesis (EMH), formalized\n",
      "by [1] posits that markets eï¬ciently incorporate new information almost instantaneously. Both theoret-\n",
      "ical perspectives and empirical observations indicate that markets do not always exhibit such eï¬ciency,\n",
      "particularly when the information is complex or ambiguous. This discrepancy between theory and reality\n",
      "suggests signiï¬cant room for improvement in understanding how news is processed by market participants\n",
      "and how it inï¬uences asset prices. A substantial body of literature has tried to predict market reactions\n",
      "\n",
      "ð Doc 4: and how it inï¬uences asset prices. A substantial body of literature has tried to predict market reactions\n",
      "to news, yet some important gaps persist. Our review of the literature reveals three critical limitations in\n",
      "current approaches to analyzing ï¬nancial news: a lack of economic focus in textual analysis methodology,\n",
      "insuï¬cient attention to ï¬rm-speciï¬c eï¬ects, and over-reliance on headlines.\n",
      "First, we examine the lack of economic focus in current methodological approaches to analyzing\n",
      "ï¬nancial news. This limitation is evident across three main streams of literature.\n",
      "Sentiment Analysis. Traditional approaches frequently rely on sentiment analysis, reduc-\n",
      "ing the richness of news content to binary classiï¬cations of positive or negative sentiment. The\n",
      "seminal work of [2] demonstrated the predictive power of media sentiment in ï¬nancial markets,\n",
      "showing that negative media coverage leads to downward pressure on market prices, followed\n",
      "\n",
      "ð Doc 5: showing that negative media coverage leads to downward pressure on market prices, followed\n",
      "by a reversion to fundamentals. This ï¬nding sparked signiï¬cant interest in sentiment-based\n",
      "approaches, with [3] extending the analysis to ï¬rm-speciï¬c news and revealing that negative\n",
      "word content not only forecasts poor ï¬rm earnings but also indicates a temporary underreac-\n",
      "tion in stock prices. Despite these early successes, the methodology of sentiment analysis has\n",
      "faced important challenges. [4] highlighted a fundamental issue: general-purpose dictionar-\n",
      "ies often misclassify words in ï¬nancial contexts, leading them to develop specialized ï¬nancial\n",
      "word lists. Building on this insight, [5] demonstrated that the weighting scheme applied to\n",
      "these words is as crucial as the word lists themselves, introducing a more nuanced approach\n",
      "to content analysis. The emergence of social media and machine learning has driven fur-\n",
      "\n",
      "ð Doc 6: to content analysis. The emergence of social media and machine learning has driven fur-\n",
      "ther methodological innovations in sentiment analysis. [6] leveraged Twitter data to predict\n",
      "DJIA movements, while [7] revealed that sentimentâs predictive power is particularly pro-\n",
      "nounced during recessions, suggesting time-varying importance of news sentiment. Recent\n",
      "advances in machine learning have pushed the boundaries further, with [8] developing a so-\n",
      "phisticated supervised learning framework speciï¬cally designed for return prediction. The\n",
      "advent of transformer-based models has enabled even more sophisticated approaches, with [9]\n",
      "and [10] applying BERT-based architectures to ï¬nancial sentiment analysis. However, despite\n",
      "1\n",
      "\n",
      "ð Doc 7: their widespread adoption and continued methodological reï¬nements, sentiment analysis ap-\n",
      "proaches remain fundamentally limited. They often miss the intricacy inherent in news by\n",
      "focusing on linguistic patterns rather than economically relevant considerations.\n",
      "Topic modeling. Beyond sentiment analysis, researchers have also explored topic modeling\n",
      "as an alternative approach to categorize text into broader themes. The pioneering work of\n",
      "[11] demonstrated that computational linguistics methods could reveal important patterns in\n",
      "market reactions to news, ï¬nding that stock prices do not immediately and consistently reï¬ect\n",
      "news, with eï¬ects varying signiï¬cantly across diï¬erent types of stories and market conditions.\n",
      "Topic modeling approaches have since been applied across ï¬nancial research domains. [12]\n",
      "used these techniques to analyze Federal Reserve communications, while [13] developed a\n",
      "topic model analyzing over 800,000 Wall Street Journal articles to track news attention to\n",
      "\n",
      "ð Doc 8: topic model analyzing over 800,000 Wall Street Journal articles to track news attention to\n",
      "diï¬erent economic themes. [14] further integrated topic modeling with asset pricing models\n",
      "to derive systematic risk factors from news. However, these models are limited in adapting\n",
      "to new and evolving information and lack the speciï¬city needed to assess the precise impact\n",
      "of news on individual ï¬rms or sectors. While topic models can identify broad themes, they\n",
      "struggle to capture the changing context of ï¬nancial news, particularly when new narratives\n",
      "emerge, such as unexpected geopolitical events or technological disruptions.\n",
      "Vector-based models. Vector-based models have emerged as an alternative approach to ad-\n",
      "dress the limitations of both sentiment analysis and topic modeling. The foundational models\n",
      "in this domain, Word2Vec and GloVe, established the paradigm of mapping words to contin-\n",
      "uous vector spaces based on their co-occurrence patterns, enabling mathematical operations\n",
      "\n",
      "ð Doc 9: uous vector spaces based on their co-occurrence patterns, enabling mathematical operations\n",
      "on words and capturing semantic relationships. [15] pioneered their application in ï¬nance by\n",
      "developing time-varying measures of product similarity from ï¬rmsâ10-K descriptions, demon-\n",
      "strating how vector representations could capture nuanced competitive relationships that tra-\n",
      "ditional industry classiï¬cations miss. The advent of transformer architectures marked a signif-\n",
      "icant advancement, leading to more sophisticated models such as BERT, RoBERTa, or GPT.\n",
      "These models process text through multiple attention layers, generating context-aware embed-\n",
      "dings by considering relationships between all words simultaneously. [16] demonstrated their\n",
      "superior performance in predicting stock movements following ï¬nancial news events, while\n",
      "[17] leveraged BERT to develop a novel measure of bond âgreennessâ, revealing how subtle\n",
      "\n",
      "ð Doc 10: [17] leveraged BERT to develop a novel measure of bond âgreennessâ, revealing how subtle\n",
      "textual diï¬erences in bond documentation translate into measurable price eï¬ects. Recent ap-\n",
      "plications have further expanded the scope of these methods. [18] analyzed ï¬nance sentiment\n",
      "across multiple countries and centuries, while [19] integrated sentiment analysis from GPT and\n",
      "BERT into traditional asset pricing models. [20] introduced âasset embeddingsâ, showing how\n",
      "these techniques can uncover latent ï¬rm characteristics from investorsâholdings data. How-\n",
      "ever, even when ï¬ne-tuned with domain-speciï¬c training data (e.g: FinBERT), these methods\n",
      "2\n",
      "\n",
      "ð Doc 11: cannot inherently incorporate economic structure, which limits their ability to comprehend\n",
      "the economic implications of news articles.\n",
      "Having examined the limitations of current methodological approaches, we now turn to a second crit-\n",
      "ical gap in the literature: there is an insuï¬cient focus on ï¬rm-speciï¬c analysis in existing research. Many\n",
      "studies examine the impact of news on broader market indices such as the S&P500 or DJIA, rather than\n",
      "on individual ï¬rms. For example, [21] and [22] analyzed comprehensive news coverage to understand ag-\n",
      "gregate market movements, while more recent work has leveraged increasingly sophisticated data sources.\n",
      "[6] developed novel mood tracking tools for Twitter messages to predict DJIA movements, and [7] ex-\n",
      "amined a century of New York Times ï¬nancial columns to study market-wide returns during recessions.\n",
      "[23], [24] and [25] constructed innovative news-based indices that have enhanced our understanding of\n",
      "\n",
      "ð Doc 12: [23], [24] and [25] constructed innovative news-based indices that have enhanced our understanding of\n",
      "market-wide uncertainty and volatility. While these and other similar studies provide valuable insights\n",
      "into market-wide reactions, they fall short in elucidating how speciï¬c ï¬rms are aï¬ected by news events.\n",
      "Firm-speciï¬c impacts are often masked when aggregated at the index level, leading to a loss of criti-\n",
      "cal information about how particular entities are inï¬uenced by speciï¬c news. For example, during the\n",
      "COVID-19 pandemic, market indices masked substantial heterogeneity in ï¬rm-level responses with some\n",
      "sectors like technology and healthcare experiencing positive returns, while others, such as hospitality,\n",
      "travel, and retail, experiencing signiï¬cant negative impacts due to widespread lockdowns and reduced\n",
      "consumer spending. Such diï¬erences are often obscured when focusing solely on market indices. Tools\n",
      "\n",
      "ð Doc 13: consumer spending. Such diï¬erences are often obscured when focusing solely on market indices. Tools\n",
      "like Named Entity Recognition (NER), which could help identify ï¬rms impacted by particular events,\n",
      "remain underutilized in ï¬nancial research, further contributing to the lack of ï¬rm-level granularity.\n",
      "The third and ï¬nal critical issue is the over-reliance on headlines as the basis for news analysis.\n",
      "Headlines are often used due to their availability and the simplicity of extracting sentiment from them,\n",
      "making them convenient but insuï¬cient for comprehensive analysis. [26] provided early evidence of\n",
      "this limitation, showing distinct market reactions to headline news versus no-news events, particularly in\n",
      "terms of drift after bad news and reversals after extreme price movements. As natural language processing\n",
      "techniques evolved, researchers continued to focus primarily on headlines: [27] and [10] applied increasingly\n",
      "\n",
      "ð Doc 14: techniques evolved, researchers continued to focus primarily on headlines: [27] and [10] applied increasingly\n",
      "sophisticated deep learning and BERT models to headline analysis, while recent work by [28] and [29] has\n",
      "extended this approach using large language models to extract contextualized representations from news\n",
      "headlines. While these studies have advanced our understanding of market reactions to news, headlines\n",
      "are designed to capture attention rather than provide comprehensive information. Consequently, relying\n",
      "solely on headlines can lead to overly simplistic analyses that fail to capture critical contextual details\n",
      "necessary for accurately predicting market reactions.\n",
      "This paper seeks to address these three limitations by leveraging Large Language Models (LLMs) to\n",
      "facilitate an economically-structured, granular and ï¬rm-speciï¬c analysis of complete news articles. LLMs\n",
      "are particularly suited for economic interpretation due to their extensive training on human-generated\n",
      "3\n",
      "\n",
      "ð Doc 15: text, including ï¬nancial and economic discourse. This exposure enables them to âunderstandâ economic\n",
      "concepts, cause-and-eï¬ect relationships, and market mechanisms in ways that mirror human economic\n",
      "reasoning. Unlike purely statistical approaches, LLMs can recognize economic patterns and implications\n",
      "that would be evident to market participants, making them powerful tools for ï¬nancial analysis. For\n",
      "example, LLMs could simulate human analysis of news articles, understanding the economic shocks that\n",
      "a news article describes upon a speciï¬c ï¬rm âsuch as supply chain disruptions aï¬ecting manufacturing,\n",
      "shifts in consumer demand impacting retail, or policy changes inï¬uencing energy sectorsâ and quantifying\n",
      "both the magnitude and direction of these impacts on speciï¬c ï¬rms. In this study, we leverage LLMs\n",
      "to parse a dataset of Spanish business news articles from DowJones Newswires, spanning June 2020 to\n",
      "\n",
      "ð Doc 16: to parse a dataset of Spanish business news articles from DowJones Newswires, spanning June 2020 to\n",
      "September 2021, a particularly unstable period marked by economic disruptions due to the COVID-\n",
      "19 pandemic. This period was purposefully chosen for its inherent complexity and market instability.\n",
      "Testing our methodology during such a challenging period allows us to rigorously evaluate its robustness\n",
      "and eï¬ectiveness. While many methodologies can perform adequately during stable market conditions,\n",
      "their true capabilities are revealed when faced with unprecedented market dynamics and rapid economic\n",
      "changes.\n",
      "Our methodology consists of deï¬ning a schema with which we guide an LLM to detect ï¬rm-speciï¬c\n",
      "shocks from business news and to further classify them by their type (demand, supply, technological,\n",
      "policy, ï¬nancial), magnitude (minor, major) and direction (positive, negative). Through their ability\n",
      "\n",
      "ð Doc 17: policy, ï¬nancial), magnitude (minor, major) and direction (positive, negative). Through their ability\n",
      "to categorize and comprehend the economic implications of news, LLMs generate insights that surpass\n",
      "traditional methodologies, revealing the underlying mechanisms driving market behavior. This allows\n",
      "for a more detailed assessment of how speciï¬c pieces of information inï¬uence particular ï¬rms, providing\n",
      "a richer and more precise picture of market dynamics. As our benchmark, we employ a vector-based\n",
      "approach that represents each news article as a high-dimensional embedding vector using a sentence\n",
      "transformer. This benchmark choice serves two key purposes. First, it oï¬ers greater granularity and\n",
      "sophistication compared to traditional methods like sentiment analysis and topic modeling. Second, it\n",
      "provides theoretical consistency with our LLM-based approach, as vector embeddings constitute the ï¬rst\n",
      "\n",
      "ð Doc 18: provides theoretical consistency with our LLM-based approach, as vector embeddings constitute the ï¬rst\n",
      "layer of an LLMâs architecture. This parallel allows us to eï¬ectively compare the predictive power of\n",
      "the LLMâs initial representation (vector embeddings) with its ï¬nal output (economically structured news\n",
      "classiï¬cation). Through this comparison, we can assess whether incorporating economic structure in the\n",
      "LLM processing step enhances our ability to predict market reactions to news.\n",
      "To evaluate the timing ability of our proposed methodology, we develop a trading strategy that builds\n",
      "on the traditional portfolio sorting approach. While conventional strategies sort stocks based on ï¬rm\n",
      "characteristics, we instead sort based on news clusters. For the benchmark (vector embeddings), we\n",
      "employ KMeans clustering, while our LLM methodology clusters articles by shock categories. We identify\n",
      "the best and worst-performing clusters by analyzing the stock price responses of aï¬ected ï¬rms, then\n",
      "\n",
      "ð Doc 19: the best and worst-performing clusters by analyzing the stock price responses of aï¬ected ï¬rms, then\n",
      "construct a long-short portfolio strategy that takes long positions in the best-performing clusters and\n",
      "4\n",
      "\n",
      "ð Doc 20: short positions in the worst-performing ones. The proï¬tability of this strategy serves as a measure of\n",
      "each clustering methodologyâs ability to identify economically meaningful news patterns that translate\n",
      "into improved market timing abilities. Our ï¬ndings reveal that while the vector-based model successfully\n",
      "identiï¬es ï¬rm- and industry-speciï¬c clusters, its trading signals lack persistence. The modelâs reliance\n",
      "on historical ï¬rm and industry performance patterns generates ephemeral signals that do not translate\n",
      "well to future market conditions. In contrast, our LLM-based methodology produces clusters based on\n",
      "economically meaningful shock classiï¬cations, resulting in more persistent trading signals. The superior\n",
      "out-of-sample performance of our LLM-based trading strategy demonstrates its enhanced capability to\n",
      "capture and interpret market reactions to news, underscoring the advantages of incorporating economic\n",
      "structure into news analysis.\n",
      "\n",
      "ð Doc 21: capture and interpret market reactions to news, underscoring the advantages of incorporating economic\n",
      "structure into news analysis.\n",
      "The objective of this paper is not to parse the largest dataset available or to develop a realistic trading\n",
      "strategy with commercial application. Rather, it aims to introduce a novel methodology for analyzing news\n",
      "articles in a granular and ï¬rm-speciï¬c manner, demonstrating its utility through a reduced dataset. By\n",
      "focusing on a smaller, high-quality dataset, the study emphasizes methodological rigor and interpretability.\n",
      "The ï¬ndings are intended to contribute to a more nuanced understanding of how market participants\n",
      "process news, using a simple trading strategy to illustrate the potential of this approach in capturing the\n",
      "complexities of information processing in ï¬nancial markets. This methodological contribution lays the\n",
      "groundwork for future research that could extend these techniques to larger datasets and more complex\n",
      "\n",
      "ð Doc 22: groundwork for future research that could extend these techniques to larger datasets and more complex\n",
      "trading applications, ultimately enhancing our ability to understand and predict market behavior in\n",
      "response to news.\n",
      "The remainder of this paper is organized as follows: Section 2 presents the dataset and preprocessing\n",
      "steps. Section 3 provides a mathematical framework for analyzing news articles. In Section 4, we focus\n",
      "on clustering news articles â ï¬rst presenting the benchmark framework using KMeans clustering of vector\n",
      "embeddings, followed by our novel LLM-based methodology. Section 5 details the construction of a simple\n",
      "trading strategy, including market-beta-neutral positions for each ï¬rm-article pair, extraction of cluster-\n",
      "average Sharpe Ratios, and selection of optimal clusters based on two proposed algorithms. In Section 6,\n",
      "we perform robustness checks by examining the sensitivity of our results to hyperparameter variations.\n",
      "\n",
      "ð Doc 23: we perform robustness checks by examining the sensitivity of our results to hyperparameter variations.\n",
      "Finally, Section 7 concludes and discusses the implications of our ï¬ndings\n",
      "2. Data\n",
      "This paper employs a dataset of Spanish business news articles sourced from Dow Jones Newswires,\n",
      "covering the period from June 24, 2020, to September 30, 2021. The selection of this timeframe is\n",
      "deliberate, driven by two key considerations. First, given the substantial computational demands of\n",
      "LLM-based analysis, we strategically focus on a smaller, carefully curated dataset. This deliberate scope\n",
      "reduction allows us to thoroughly demonstrate our novel methodologyâs eï¬ectiveness in decoding market-\n",
      "5\n",
      "\n",
      "ð Doc 24: news relationships while keeping computational costs manageable. Second, we speciï¬cally chose the Covid-\n",
      "19 era to test our methodologyâs extrapolative capabilities during periods of signiï¬cant market instability\n",
      "and volatility. While existing textual algorithms typically perform well in stable market conditions, they\n",
      "often struggle to generalize eï¬ectively during periods of heightened uncertainty. By focusing on this\n",
      "volatile period, we can better assess our methodologyâs robustness and its ability to maintain predictive\n",
      "power under challenging market conditions.\n",
      "The dataset consists of high-quality articles that have been ï¬ltered to include only those mentioning\n",
      "Spanish publicly traded ï¬rms listed on the IBEX-35 index. These 35 companies represent the largest ï¬rms\n",
      "in Spain by market capitalization and are typically the most liquid and actively traded Spanish stocks.\n",
      "Moreover, these companies tend to receive the most consistent media coverage, making them ideal for the\n",
      "\n",
      "ð Doc 25: Moreover, these companies tend to receive the most consistent media coverage, making them ideal for the\n",
      "scope of our analysis.\n",
      "The use of Dow Jones Newswires as our news source is also intentional. Dow Jones has a standard\n",
      "practice of including the stock market ticker of ï¬rms directly aï¬ected by the article in parentheses, while\n",
      "excluding ï¬rms mentioned for secondary purposes from ticker speciï¬cation. This feature signiï¬cantly facil-\n",
      "itates the extraction of named entities (i.e., Named Entity Recognition, or NER). The tickers used by Dow\n",
      "Jones align with those from Yahoo Finance, enabling seamless integration between our NER algorithm and\n",
      "subsequent ï¬rm-speciï¬c trading operations via the Yahoo Finance API. We employ a pattern recognition\n",
      "algorithm through the regex library in Python to identify speciï¬c mentions of publicly traded companies\n",
      "in the Spanish stock exchange. The algorithm searches for patterns of the form â(<WORD>.MC)â for any\n",
      "\n",
      "ð Doc 26: in the Spanish stock exchange. The algorithm searches for patterns of the form â(<WORD>.MC)â for any\n",
      "<WORD>. For instance, consider the following example article (translated into English for convenience):\n",
      "Example 1: An article about ACS and Acciona (translated into English)\n",
      "ACS and Acciona Secure Contracts for New Australian Airport\n",
      "A consortium of Actividades de ConstrucciÃ³n y Servicios SA (ACS.MC) and Acciona\n",
      "SA (ANA.MC) has won a contract to build the operations area of the Western Sydney\n",
      "International Airport (Nancy-Bird Walton) and carry out paving works, amounting to\n",
      "AUD265 million (EUR164 million) for the Australian subsidiary CIMIC Group Ltd\n",
      "(CIM.AU). CIMIC will carry out the work through its subsidiary CPB Contractors, as\n",
      "stated in a press release. This is the third project awarded by Western Sydney Airport\n",
      "to the joint venture after being selected to carry out earthworks. Construction will take\n",
      "two years, and the Western Sydney airport is expected to open in 2026.\n",
      "\n",
      "ð Doc 27: two years, and the Western Sydney airport is expected to open in 2026.\n",
      "Our NER algorithm applied to Example 1 successfully identiï¬es the Spanish ï¬rms ACS.MC (Actividades\n",
      "de ConstrucciÃ³ n y Servicios SA) and ANA.MC (Acciona SA) while disregarding the Australian CIM.AU\n",
      "(CIMIC Groups Ltd). To further ensure the reliability of ï¬rm identiï¬cation, we validate the extracted\n",
      "entities using a Large Language Model (LLM). In particular, we feed the articles to the LLM, which parses\n",
      "6\n",
      "\n",
      "ð Doc 28: them according to a predeï¬ned schema. As we will see later, the ï¬rst task in this schema is to identify\n",
      "the listed Spanish ï¬rms directly aï¬ected by the events described in the article. Finally, the identiï¬ed\n",
      "ï¬rms are ï¬ltered against a dynamic list of IBEX-35 members. Due to the high quality of the dataset, the\n",
      "correlation between entities identiï¬ed by the LLM and those extracted via pattern recognition is almost\n",
      "exact.\n",
      "For subsequent analysis, we partition the dataset into three splits: Train, Validation, and Test. Each\n",
      "split serves a distinct purpose that will be explained in detail as we progress through the paper. Summary\n",
      "statistics for each data split are provided in Table 1.\n",
      "[Insert Table 1 about here]\n",
      "The most frequently used words in the whole dataset are depicted in Figure 1 by means of a WordCloud.\n",
      "As shown, the most prominent words include âempresaâ (ï¬rm), âcompaÃ±Ã­aâ (company), and âespaÃ±aâ\n",
      "\n",
      "ð Doc 29: As shown, the most prominent words include âempresaâ (ï¬rm), âcompaÃ±Ã­aâ (company), and âespaÃ±aâ\n",
      "(Spain), reinforcing that the dataset primarily comprises Spanish business news, with a prevalence of\n",
      "technical terms such as âbeneï¬cio netoâ (net proï¬t), âprecio objetivoâ (target price), âproyectoâ (project),\n",
      "and âoperaciÃ³nâ (operation).\n",
      "[Insert Figure 1 about here]\n",
      "The distribution of the number of articles published per day is illustrated in Figure 2a, showing\n",
      "that the most frequent publication rate is between 5 and 10 articles per day, though some days exhibit\n",
      "unusually high publication counts. Figure 2b shows the distribution of the number of words per article,\n",
      "with the majority of articles containing between 70 and 280 words. This indicates that the articles are\n",
      "relatively succinct, providing direct information. However, the long right tail points to instances of more\n",
      "comprehensive coverage.\n",
      "[Insert Figure 2 about here]\n",
      "\n",
      "ð Doc 30: comprehensive coverage.\n",
      "[Insert Figure 2 about here]\n",
      "The time series of the number of articles published per day throughout the sample period is shown in\n",
      "Figure 3. The series exhibits considerable variability, with frequent ï¬uctuations from fewer than 5 articles\n",
      "per day to sudden spikes exceeding 20 articles. The 30-day moving average smooths the series, conï¬rming\n",
      "the previous observation that, on average, between 5 and 10 articles are published daily.\n",
      "[Insert Figure 3 about here]\n",
      "Data Availability. The dataset used in this study contains conï¬dential information provided under\n",
      "agreements with the Bank of Spain and Dow Jones Newswires, and cannot be shared publicly or with\n",
      "third parties. Interested readers may access the same data from Dow Jones Newswires for a fee.\n",
      "7\n",
      "\n",
      "ð Doc 31: 3. Mathematical Treatment of News Articles\n",
      "Our dataset consists of N = 2, 613 Spanish business news articles sourced from DowJones and spanning\n",
      "the period from 2020/06/24 to 2021/09/30. We denote as D the set of all articles in our sample. These\n",
      "articles have been speciï¬cally ï¬ltered to reference ï¬rms listed on the IBEX-35. Let FIBEX35 denote the\n",
      "universe of such ï¬rms. Each article i â D is a textual document detailing an event that directly pertains\n",
      "to a subset of ï¬rms Fi â FIBEX35. The publication date and time of each article are represented as\n",
      "ãdi\n",
      "0, ti\n",
      "0ã, where di\n",
      "0 captures the date (YYYY-MM-DD) and ti\n",
      "0 captures the time (HH:MM) of publication.\n",
      "Therefore we observe the moment at which Fi receives the âtreatmentâ of public news dissemination.\n",
      "Eï¬ective treatment day\n",
      "We are interested in examining the impact of each news article i â D on the stock price of the ï¬rms\n",
      "directly aï¬ected by it (i.e., all j â Fi). Since publication datetime may not coincide with trading hours,\n",
      "\n",
      "ð Doc 32: directly aï¬ected by it (i.e., all j â Fi). Since publication datetime may not coincide with trading hours,\n",
      "we deï¬ne an eï¬ective treatment date, denoted Ëdi\n",
      "0. This maps the news article publication datetime to the\n",
      "nearest trading date where the stock price can reï¬ect the news impact.\n",
      "Let d denote the set of all dates in our sample timeline and let Ëd â d denote the subset of Spanish\n",
      "trading days. We deï¬ne a function Î : d â Ëd that ï¬nds the next trading date after a given date:\n",
      "Î(d) := min{ Ëd â Ëd | Ëd > d}. We set Ëdi\n",
      "0 to the publication date if the article was published on a trading\n",
      "day before market close (17:30 in Spain), and to the next trading day otherwise. Formally,\n",
      "Ëdi\n",
      "0 :=\n",
      "ó°»\n",
      "ó°¿\n",
      "ó°½\n",
      "di\n",
      "0 if di\n",
      "0 â Ëd â§ ti\n",
      "0 < 17:30\n",
      "Î(di\n",
      "0) if di\n",
      "0 ââ Ëd â¨ ti\n",
      "0 â¥ 17:30\n",
      ".\n",
      "The two possible cases are illustrated in Figure 4.\n",
      "[Insert Figure 4 about here]\n",
      "Data Splitting\n",
      "For robust model development and evaluation, the dataset is partitioned into three sequential subsets:\n",
      "\n",
      "ð Doc 33: [Insert Figure 4 about here]\n",
      "Data Splitting\n",
      "For robust model development and evaluation, the dataset is partitioned into three sequential subsets:\n",
      "training, validation, and test: D := Dtr âª Dval âª Dtest. Deï¬ne Nsplit := |Dsplit| for split â {tr, val, test},\n",
      "where |Â· | denotes the cardinality of a set. The training and validation sets collectively comprise 80%\n",
      "of the total dataset (Ntr+Nval\n",
      "N = 0.8) and are instrumental in constructing and ï¬ne-tuning the trading\n",
      "strategy. The remaining 20% (Ntest\n",
      "N = 0.2) is reserved for out-of-sample testing to assess the performance\n",
      "and generalizability of the strategy under unseen conditions.\n",
      "8\n",
      "\n",
      "ð Doc 34: 4. Clustering News Articles\n",
      "In this section we present our clustering methodology based on news-implied ï¬rm-speciï¬c shock classi-\n",
      "ï¬cations and we compare it against a benchmark based on clustering the vector embedding representations\n",
      "of the articles. For ease of exposition, we will ï¬rst present the benchmark model.\n",
      "4.1 Benchmark: KMeans clustering of vector embeddings\n",
      "4.1.1 Why this benchmark?\n",
      "In evaluating our novel Large Language Model (LLM) methodology for classifying news-implied ï¬rm-\n",
      "speciï¬c shocks, we selected KMeans clustering of high-dimensional vector embeddings as the benchmark\n",
      "over alternatives like sentiment analysis and topic modeling. Sentiment analysis, while straightforward,\n",
      "lacks the necessary granularity, oï¬ering only positive, negative, or neutral classiï¬cations, which is insuf-\n",
      "ï¬cient to compare with our granular LLM-based economic shock classiï¬cation. Additionally, sentiment\n",
      "\n",
      "ð Doc 35: ï¬cient to compare with our granular LLM-based economic shock classiï¬cation. Additionally, sentiment\n",
      "analysis focuses on the emotional tone rather than the economic impact, it is prone to inconsistencies\n",
      "due to linguistic nuances and it can deliver very diï¬erent outcomes depending on the speciï¬c sentiment\n",
      "analysis tool employed.\n",
      "On the other hand, topic modeling provides more detailed classiï¬cations than sentiment analysis but\n",
      "relies on bag-of-words representations that fail to capture complex semantic relationships and contex-\n",
      "tual nuances essential for identifying economic shocks accurately. Vector embeddings, particularly those\n",
      "generated by transformer-based models, oï¬er enhanced semantic representation by capturing context-\n",
      "dependent meanings and scaling eï¬ciently with large datasets, making them more ï¬exible and adaptable\n",
      "for clustering and classiï¬cation. Although embeddings lack inherent interpretability, this issue is ad-\n",
      "\n",
      "ð Doc 36: for clustering and classiï¬cation. Although embeddings lack inherent interpretability, this issue is ad-\n",
      "dressed by clustering, which allows us to infer meaningful ï¬rm-speciï¬c or industry-speciï¬c patterns from\n",
      "the grouped articles.\n",
      "Lastly, using embeddings as a benchmark is particularly compelling because they represent the foun-\n",
      "dational layer of an LLM. Namely, the ï¬rst step in an LLMâs processing pipeline is to transform the text\n",
      "that it is fed into embeddings for further processing. By benchmarking against embeddings, we ensure\n",
      "a direct and relevant comparison between the foundational representations used by LLMs and our spe-\n",
      "cialized classiï¬cation methodology. This comparison highlights the added value of the LLMâs capacity to\n",
      "convert these semantic representations (i.e: the vector embeddings) into economically meaningful classi-\n",
      "ï¬cations. (i.e: our news-implied ï¬rm-speciï¬c shock classiï¬cations). Consequently, KMeans clustering of\n",
      "\n",
      "ð Doc 37: ï¬cations. (i.e: our news-implied ï¬rm-speciï¬c shock classiï¬cations). Consequently, KMeans clustering of\n",
      "vector embeddings provides a robust, scalable, and economically pertinent benchmark, superior to senti-\n",
      "ment analysis and topic modeling, for assessing our LLM-based classiï¬cation of news-implied ï¬rm-speciï¬c\n",
      "shocks. A more detailed discussion can be found in A.7.\n",
      "9\n",
      "\n",
      "ð Doc 38: 4.1.2 Vector embeddings: âTransforming text into high-dimensional vectorsâ\n",
      "Any piece of text can be represented as a high-dimensional vector embedding by using a transformer.\n",
      "Transformers are a type of deep learning architecture introduced by [30] which have revolutionized natural\n",
      "language processing (NLP). The core idea behind them is the self-attention mechanism, which allows the\n",
      "model to weigh the importance of diï¬erent words in a sentence when generating a representation for\n",
      "each word. This mechanism enables transformers to capture long-range dependencies and contextual\n",
      "relationships within the text more eï¬ectively than previous models like recurrent neural networks (RNNs).\n",
      "A transformer model consists of an encoder (and potentially, a decoder as well) composed of multiple\n",
      "layers of self-attention and feedforward neural networks. In our context, we primarily use the encoder to\n",
      "convert a piece of text into a ï¬xed-size vector, known as an embedding. Since our articles are written\n",
      "\n",
      "ð Doc 39: convert a piece of text into a ï¬xed-size vector, known as an embedding. Since our articles are written\n",
      "in Spanish, we employ a Multilingual Sentence Transformer, which has been trained on text from\n",
      "multiple languages.\n",
      "For every news article i â D, we obtain a representative vector embedding ei â R512 that provides\n",
      "a numerical representation of various aspects of the text, such as syntactic structure, semantic content,\n",
      "and contextual nuances. While it is challenging to assign a speciï¬c human-readable meaning to each of\n",
      "the 512 components, we can interpret the vector as a whole in various ways:\n",
      "â¢ Semantic Similarity: Similar articles will have similar embeddings. For instance, if one article\n",
      "discusses a companyâs quarterly earnings and another article discusses the same companyâs annual\n",
      "earnings, their embeddings will be close in the 512-dimensional space.\n",
      "â¢ Topic Clustering: Articles on similar topics will cluster together. For example, articles about\n",
      "\n",
      "ð Doc 40: â¢ Topic Clustering: Articles on similar topics will cluster together. For example, articles about\n",
      "ï¬nancial markets might cluster in one region of the embedding space, while articles about mergers\n",
      "and acquisitions cluster in another.\n",
      "â¢ Sentiment Analysis: Diï¬erent regions of the embedding space can implicitly represent diï¬erent\n",
      "sentiments. Articles with positive news might cluster in one area, while those with negative news\n",
      "cluster in another.\n",
      "4.1.3 Clustering embeddings with KMeans\n",
      "With the numerical representation of each article in the form of embeddings {ei}iâD, we now seek to\n",
      "identify groups of similar articles. Namely, we use the KMeans algorithm, a popular clustering method\n",
      "that assigns a set of vectors into k clusters GKMeans := {0, 1, ..., k â 1} to minimize the within-cluster\n",
      "sum of squares (WCSS). The implementation of this clustering algorithm is methodically presented in\n",
      "Appendix Algorithm 1. Each cluster g â GKMeans deï¬nes a centroid cg, which is the average vector of all\n",
      "\n",
      "ð Doc 41: Appendix Algorithm 1. Each cluster g â GKMeans deï¬nes a centroid cg, which is the average vector of all\n",
      "10\n",
      "\n",
      "ð Doc 42: the members of a cluster. In the ï¬rst step, we apply the algorithm to the training data (Dtr).\n",
      "min{Dtrg },{cg}\n",
      "ó°k\n",
      "g=1\n",
      "ó°\n",
      "iâDtrg ó°ei â cgó°2\n",
      "2\n",
      "s.t.\n",
      "ó°k\n",
      "g=1 Dtr\n",
      "g = Dtr\n",
      "Dtr\n",
      "g â© Dtr\n",
      "h = â âg, h â GKMeans : g â= h\n",
      ".\n",
      "The optimal number of clusters kâ in this algorithm is to be set exogenously. Here, we take it to\n",
      "maximize the average silhouette score in the training sample over some grid k of cluster sizes k:\n",
      "kâ := arg maxkâk\n",
      "1\n",
      "|Dtr|\n",
      "ó°\n",
      "iâDtr\n",
      "sk(ei) .\n",
      "The silhouette score sk(ei) â [â1, 1] measures how well an embedding is clustered by comparing its\n",
      "similarity to its own cluster (intra-cluster distance) with its similarity to the nearest other cluster (inter-\n",
      "cluster distance). A clustering conï¬guration with a higher average silhouette score (close to +1) is\n",
      "considered better because it indicates that clusters are dense and well-separated. Formally, the silhouette\n",
      "score is deï¬ned as\n",
      "sk(ei) := bk\n",
      "ó°\n",
      "eió°\n",
      "â ak\n",
      "ó°\n",
      "eió°\n",
      "max {ak (ei) , bk (ei)} ,\n",
      "where, for i â Dtr\n",
      "\n",
      "ð Doc 43: score is deï¬ned as\n",
      "sk(ei) := bk\n",
      "ó°\n",
      "eió°\n",
      "â ak\n",
      "ó°\n",
      "eió°\n",
      "max {ak (ei) , bk (ei)} ,\n",
      "where, for i â Dtr\n",
      "g , the intra-cluster distance is deï¬ned as ak(ei) := (|Dtr\n",
      "g | â 1)â1 ó°\n",
      "mâDtrg ,mâ=i ó°ei â emó°2\n",
      "and it represents the average distance from an embedding ei to all other embeddings in the same cluster,\n",
      "while the inter-cluster distance is bk(ei) := minlâ=g(|Dtr\n",
      "l |)â1 ó°\n",
      "mâDtr\n",
      "l\n",
      "ó°ei â emó°2 and it represents the\n",
      "minimum average distance from an embedding ei to all embeddings in the nearest diï¬erent cluster.\n",
      "In Figure 5 we plot the average silhouette score for Dtr computed over a grid k ranging from 2 to 100.\n",
      "The vertical dashed green line signals the maximizer of the grid, which corresponds to a cluster size of\n",
      "kâ = 26.\n",
      "[Insert Figure 5 about here]\n",
      "Given the optimal number of clusters kâ, we ï¬t the KMeans algorithm on the training embeddings\n",
      "{ei | i â Dtr} to obtain the centroids {ctr\n",
      "1 , ctr\n",
      "2 , . . . , ctr\n",
      "kâ}. Following Algorithm 1. (detailed in A.1):\n",
      "{ctr\n",
      "1 , ctr\n",
      "2 , . . . , ctr\n",
      "\n",
      "ð Doc 44: {ei | i â Dtr} to obtain the centroids {ctr\n",
      "1 , ctr\n",
      "2 , . . . , ctr\n",
      "kâ}. Following Algorithm 1. (detailed in A.1):\n",
      "{ctr\n",
      "1 , ctr\n",
      "2 , . . . , ctr\n",
      "kâ} = KMeans({e1, e2, . . . , eNtr }, kâ) .\n",
      "We then ï¬nd the cluster associated to each embedding ei in the validation set {ei | i â Dval} according\n",
      "to the centroids resulting from clustering the training data {ctr\n",
      "1 , ..., ctr\n",
      "kâ}. This allows us to obtain the\n",
      "clustering of the news articles in the validation sample\n",
      "Dval\n",
      "g =\n",
      "ó°\n",
      "i â Dval\n",
      "ó°ó°ó°ó° g = arg minââG\n",
      "ó°ei â ctr\n",
      "â ó°2\n",
      "2\n",
      "ó°\n",
      "âg â GKMeans.\n",
      "11\n",
      "\n",
      "ð Doc 45: Similarly, by assigning each embedding ei â {ei | i â Dtest} to the nearest centroid ctr\n",
      "g , we obtain the\n",
      "clusters in the test set\n",
      "Dtest\n",
      "g =\n",
      "ó°\n",
      "i â Dtest\n",
      "ó°ó°ó°ó° g = arg minââG\n",
      "ó°ei â ctr\n",
      "â ó°2\n",
      "2\n",
      "ó°\n",
      "âg â GKMeans.\n",
      "[Insert Figure 6 about here]\n",
      "In Figure 6 we can see that the distribution of articles in the whole sample (D) is fairly homogenous\n",
      "across the 26 clusters, with each cluster containing between 50 and 250 articles on average. The notable\n",
      "exceptions are cluster 3, which contains only 24 articles, and cluster 4, which concentrates 428 articles.\n",
      "However, the distribution proï¬le is not consistent over data splits, which indicates that this classiï¬cation\n",
      "procedure is unstable over time.\n",
      "Although not directly interpretable, by looking at the articles pooled in a certain cluster, we can\n",
      "provide some intuition of what it represents. In most cases, each cluster contains articles involving a ï¬rm\n",
      "\n",
      "ð Doc 46: provide some intuition of what it represents. In most cases, each cluster contains articles involving a ï¬rm\n",
      "or set of ï¬rms in the same sector. For example, cluster 3 pools articles about TelefÃ³ nica and Cellnex\n",
      "(telecoms), cluster 4 contains articles about CaixaBank, cluster 9 concentrates articles about Repsol,\n",
      "cluster 12 about Iberdrola, cluster 15 gathers articles on Infrastructure (led by ACS and Acciona) and so\n",
      "on.\n",
      "However, there are some exceptions to this general rule, for example, cluster 0 is a âmiscellanousâ\n",
      "cluster: it covers articles about diï¬erent ï¬rms with no apparent relation between them. Another example\n",
      "is cluster 1, which pools articles related to the quarterly or semiannual publication of results by diï¬erent\n",
      "ï¬rms. In Appendix Table A1 we provide a sample of 3 articles for each cluster and propose a name for\n",
      "each one based on the articles they pool.\n",
      "4.2 LLM-based approach: âWhat if an LLM reads the news?â\n",
      "\n",
      "ð Doc 47: each one based on the articles they pool.\n",
      "4.2 LLM-based approach: âWhat if an LLM reads the news?â\n",
      "One may wonder whether empowering an LLM to parse news articles according to a predeï¬ned schema\n",
      "that guides it in ellucidating news-implied ï¬rm-speciï¬c shocks can deliver better insights on how markets\n",
      "react to new information. In this section we will brieï¬y introduce what Large Language Models are,\n",
      "how they have evolved and then, we will dive into how we can guide them to produce an economically\n",
      "structured analysis of business news.\n",
      "4.2.1 Large Language Models\n",
      "In natural language processing (NLP), Large Language Models (LLMs) are designed to âunderstandâ\n",
      "and generate human-like text. These models utilize the transformer architecture, which excels in modeling\n",
      "complex language tasks by capturing long-range dependencies and contextual relationships.\n",
      "At the heart of LLMs lies the concept of tokens, which serve as the elemental units of text. Tokens\n",
      "\n",
      "ð Doc 48: At the heart of LLMs lies the concept of tokens, which serve as the elemental units of text. Tokens\n",
      "can be individual words, subword units, or characters. Let x1:n := {x1, x2, . . . , xn} represent a sequence of\n",
      "12\n",
      "\n",
      "ð Doc 49: tokens. The goal of an LLM is to estimate the probability distribution of the next token xn+1 conditioned\n",
      "on the previous tokens x1:n\n",
      "P[xn+1 | {x1, x2, . . . , xn}] .\n",
      "An LLM is a neural network architecture designed to learn and approximate this conditional probabil-\n",
      "ity distribution over sequences of tokens with a large number of parameters Î. Namely, we can formulate\n",
      "an LLM as a parameterized function fÎ that maps a sequence of tokens {x1, x2, . . . , xn} to a probability\n",
      "distribution over the vocabulary, where the parameters Î are learned from a large corpus of text training\n",
      "data.\n",
      "fÎ : {x1, x2, . . . , xn} â P[xn+1 | {x1, x2, . . . , xn} ; Î]\n",
      "Interacting with an LLM involves specifying a preï¬x sequence x1:n, termed the âpromptâ, and sampling\n",
      "the subsequent tokens xn+1:z, known as the âcompletionâ. This process enables users to guide and control\n",
      "the generation of text according to desired contexts and constraints.\n",
      "{x1, . . . , xn}ó°¿ ó°¾ó°½ ó°\n",
      "prompt\n",
      "ââ {xn+1, . . . , xz}ó°¿ ó°¾ó°½ ó°\n",
      "completion\n",
      "\n",
      "ð Doc 50: the generation of text according to desired contexts and constraints.\n",
      "{x1, . . . , xn}ó°¿ ó°¾ó°½ ó°\n",
      "prompt\n",
      "ââ {xn+1, . . . , xz}ó°¿ ó°¾ó°½ ó°\n",
      "completion\n",
      "4.2.2 Evolution of LLMs\n",
      "The transformer architecture, introduced in the seminal work âAttention Is All You Needâ ([30]),\n",
      "revolutionized LLM development due to its superior handling of long-range dependencies and eï¬cient\n",
      "parallelization of computations. Subsequent advancements include the encoder-only BERT model ([31]),\n",
      "showcasing the power of pre-training on large datasets for ï¬ne-tuning on speciï¬c tasks.\n",
      "Conversely, OpenAIâs GPT series ([32]) demonstrated the potential of decoder-only models for gener-\n",
      "ative tasks. In particular, the release of GPT-3 marked a signiï¬cant leap in LLM capabilities with its\n",
      "175 billion parameters and remarkable few-shot learning abilities. This model highlighted the impor-\n",
      "tance of prompt engineering, where carefully crafted prompts can guide model outputs without extensive\n",
      "ï¬ne-tuning.\n",
      "\n",
      "ð Doc 51: tance of prompt engineering, where carefully crafted prompts can guide model outputs without extensive\n",
      "ï¬ne-tuning.\n",
      "The trend towards open-source models like BLOOM ([33]), Mixtral and Metaâs Llama series ([34])\n",
      "emphasizes accessibility and transparency in LLM development. The latest models, including Ope-\n",
      "nAIâs GPT-4.5, GPT-o1 and o3, Googleâs Gemini 2.5 Pro, Anthropicâs Claude 3.7 Sonnet, and Metaâs\n",
      "Llama-3 series continue to push boundaries with improved accuracy, multimodal capabilities, and larger\n",
      "context windows.\n",
      "4.2.3 Function Calling with Llama-3\n",
      "In our endeavor we will employ Llama-3, developed by Meta AI and released on April 18, 20241 . This\n",
      "model has been pre-trained on approximately 15 trillion tokens of text gathered from âpublicly available\n",
      "1 âIntroducing Meta Llama 3: The most capable openly available LLM to dateâ [April 18, 2024]\n",
      "13\n",
      "\n",
      "ð Doc 52: sourcesâ and it comes in two sizes: 8 billion and 70 billion parameters. In this application, we will employ\n",
      "the 70B version, which we will access through an API via GroqCloud.\n",
      "Moreover, we will employ a function calling approach to streamline the process of interacting with\n",
      "the LLM. This implies prespecifying a set of functions to the LLM that will then be passed through our\n",
      "dataset of news articles to obtain a structured output in JSON format. The formal procedure is thoroughly\n",
      "described in Appendix Algorithm 4.\n",
      "Each article i â D implies a conversation with the LLM. The structure of the conversation consists\n",
      "of deï¬ning ï¬rst a âsystem messageâ, which provides a general context and purpose to the model. In our\n",
      "case:\n",
      "â You are a function calling LLM that analyses business news in Spanish.\n",
      "â For every article, you must identify the ï¬rms directly aï¬ected by the news. Do not include\n",
      "every ï¬rm mentioned in the article, only include those that are directly aï¬ected by the shocks\n",
      "\n",
      "ð Doc 53: every ï¬rm mentioned in the article, only include those that are directly aï¬ected by the shocks\n",
      "narrated therein.\n",
      "â The identiï¬ed ï¬rms must be Spanish and should be publicly listed in the Spanish exchange\n",
      "(their ticker is of the form âTICKER.MCâ). Do not include non-Spanish foreign ï¬rms. Do\n",
      "not include Spanish ï¬rms that are not publicly traded.\n",
      "â For each identiï¬ed ï¬rm, classify the shocks that aï¬ect them (type, magnitude, category). The\n",
      "type of shock can be âdemandâ, âsupplyâ, âï¬nancialâ, âpolicyâ, or âtechnologyâ. The magnitude\n",
      "can be âminorâor âmajorâ. The direction can be âpositiveâor ânegativeâ.\n",
      "â If a ï¬rm is aï¬ected neutrally by the news article, donât include it in the analysis.\n",
      "Then, a news article is fed to the LLM. For illustration purposes, we will work with Example 2:\n",
      "Example 2: An article about Cellnex and TelefÃ³ nica (translated into English)\n",
      "Cellnex will face more competition in Europe\n",
      "TelefÃ³nicaâs (TEF.MC) subsidiary, Telxius Telecom, has agreed to sell its telecommuni-\n",
      "\n",
      "ð Doc 54: Cellnex will face more competition in Europe\n",
      "TelefÃ³nicaâs (TEF.MC) subsidiary, Telxius Telecom, has agreed to sell its telecommuni-\n",
      "cations tower division in Europe and Latin America to American Tower (AMT), which\n",
      "will expand the latterâs presence in Europe and increase competition for the Spanish\n",
      "wireless telecommunications group Cellnex Telecom (CLNX.MC), according to Equita\n",
      "Sim. The transaction \"represents the entry of a new independent tower operator into\n",
      "the Spanish market and potentially more competition for future growth in the European\n",
      "market as well,\" says the brokerage ï¬rm.\n",
      "Next, we deï¬ne an umbrella function âï¬rmsâ, which asks the LLM to identify the set Fi\n",
      "LLM for each\n",
      "i â D. Then, for each j â Fi\n",
      "LLM we ask the LLM to categorize the type, expected magnitude, and\n",
      "expected direction that the shock described in the article implies in that particular ï¬rm j.\n",
      "14\n",
      "\n",
      "ð Doc 55: [Insert Table 2 about here]\n",
      "The function calling schema is outlined in Table 2. First, we need to prompt the LLM, and then we\n",
      "need to specify the desired format of its response. The âOptionsâ column imposes the answer format that\n",
      "the LLM must follow. For example, in firms, the âarrayâ option indicates that the answer must be an\n",
      "enumeration of ï¬rms, while the âstringâ option in the subfunctions firm and ticker indicates that the\n",
      "answer must be a single name. Finally, the shock_ subfunctions ask the LLM to choose from a predeï¬ned\n",
      "set of possible responses.\n",
      "Note that the ï¬rms identiï¬ed by the LLM are used to validate the ï¬rms identiï¬ed by the pattern\n",
      "recognition algorithm (those extracted with regex by exploiting the pattern <WORD>.MC). As mentioned\n",
      "earlier, given the high quality of the ï¬ltered dataset (the ticker of the ï¬rms that are actively involved in\n",
      "the article are explicitly stated), they are almost identical. Hence, we indistinctively use Fi to simplify\n",
      "notation.\n",
      "\n",
      "ð Doc 56: the article are explicitly stated), they are almost identical. Hence, we indistinctively use Fi to simplify\n",
      "notation.\n",
      "The LLM provides two outputs: structured data (âStructured Outputâ) and a explanatory text de-\n",
      "scribing its reasoning (âUnstructured Ouptutâ). The explanations help us verify if the model correctly\n",
      "understands how to use the function-calling schema and follow system instructions. To assess the LLMâs\n",
      "understanding, we review a random sample of these explanations and look for patterns of misinterpreta-\n",
      "tion, confusion, or hallucination. If we identify such issues, we reï¬ne the system prompts and function\n",
      "descriptions to provide clearer guidance. This iterative prompt reï¬nement continues until the LLM reli-\n",
      "ably generates correct outputs across multiple test scenarios.\n",
      "1) Structured Output:\n",
      "firm ticker shock_type shock_magnitude shock_direction\n",
      "Cellnex Telecom CLNX.MC supply minor negative\n",
      "TelefÃ³ nica TEF.MC ï¬nancial minor positive\n",
      "2) Unstructured Output (justiï¬cation)\n",
      "\n",
      "ð Doc 57: Cellnex Telecom CLNX.MC supply minor negative\n",
      "TelefÃ³ nica TEF.MC ï¬nancial minor positive\n",
      "2) Unstructured Output (justiï¬cation)\n",
      "The news about American Towerâs expansion in Europe may increase competition for Cellnex,\n",
      "which is why itâs classiï¬ed as a negative supply shock. On the other hand, TelefÃ³nica beneï¬ts\n",
      "from the sale of its tower division, which is why itâs classiï¬ed as a positive ï¬nancial shock.\n",
      "This procedure is run iteratively from beginning (deï¬ning system prompt) to end (getting the output)\n",
      "for every i â D.2\n",
      "2 This procedure was run on a MacBook Pro M2 with 16GB RAM, 12-core central processing units (CPU), 19-core\n",
      "graphics processing units (GPU), and 16-core Neural Engine.\n",
      "15\n",
      "\n",
      "ð Doc 58: 4.2.4 Clustering with the LLM\n",
      "Formally, we can deï¬ne the set B := {(i, j) | i â D â§ j â Fi} containing all the unique pairs of articles\n",
      "and identiï¬ed ï¬rms. The LLM assigns each pair (i, j) â B with a choice from each of the following sets:\n",
      "âshock typeâ ST := {demand, supply, ï¬nancial, technology, policy}\n",
      "âshock magnitudeâ SM := {minor, major}\n",
      "âshock directionâ SD := {positive, negative}\n",
      "The clustering of news articles follows naturally by taking the Cartesian product of these three sets:\n",
      "GLLM := ST Ã SM Ã SD, and the total number of clusters is now kLLM = |GLLM | = 20. Consequently, a\n",
      "news article to which the LLM assigns sT â ST , sM â SM , sD â SD will belong to cluster (sT , sM , sD) â\n",
      "GLLM . Formally, the set of all possible clusters is deï¬ned as:\n",
      "GLLM := {(sT , sM , sD) | sT â ST , sM â SM , sD â SD} ,\n",
      "and each cluster can then be mapped to a positive integer as GLLM â {k â N0 | 0 â¤ k â¤ 19}. A\n",
      "representative sample of 3 articles from each cluster is provided in Appendix Table A2.\n",
      "\n",
      "ð Doc 59: representative sample of 3 articles from each cluster is provided in Appendix Table A2.\n",
      "In Figure 7 we plot the distribution of news articles through clusters. As we can see, most articles are\n",
      "assigned to clusters 8, 9, 10, and 11, which are the clusters referred to ï¬nancial events or shocks. Such\n",
      "clusters are mostly composed of articles about the publication of quarterly and semiannual results. More\n",
      "speciï¬cally, cluster 8 (ï¬nancial, minor, positive) concentrates around 1/3 of the sample and is associated\n",
      "to the publication of results that mildly surpass the expectations of investors, hence, making this cluster\n",
      "a good candidate for a long trading signal.\n",
      "On the other hand, other clusters such as 16 (policy, minor, positive) and 0 (demand, minor, positive)\n",
      "also concentrate a big share of news. Note that no cluster has been assigned to cluster 13 (technology,\n",
      "minor, negative). Compared to KMeans clustering with embeddings, the distribution of articles across\n",
      "\n",
      "ð Doc 60: minor, negative). Compared to KMeans clustering with embeddings, the distribution of articles across\n",
      "these reï¬ned clusters is now remarkably stable across diï¬erent data splits. This consistency indicates\n",
      "that clustering based on a thorough analysis of the shocks implied by each article for the aï¬ected ï¬rms\n",
      "yields a robust, time-invariant categorization. This is an encouraging ï¬nding for subsequent research and\n",
      "applications.\n",
      "[Insert Figure 7 about here]\n",
      "5. Trading Strategy\n",
      "5.1 Beta-neutral positions on every (i, j) â B\n",
      "Since we are interested in the individual eï¬ect of an article i â D in each of the aï¬ected ï¬rms j â Fi,\n",
      "we work with the set B := ó°\n",
      "(i, j) | i â D â§ j â Fió°\n",
      ", where |B| = 3410 > |D| = 2613. We then ï¬t a\n",
      "16\n",
      "\n",
      "ð Doc 61: market model to each unique pair (i, j) â B on a lookback window of 100 days with a buï¬er of 10 days\n",
      "before the eï¬ective treatment date Ëdi\n",
      "0.\n",
      "rj\n",
      "d = Î±(i,j) + Î²(i,j)rM\n",
      "d + ó°(i,j)\n",
      "d ,\n",
      "where rj\n",
      "d denotes the return of ï¬rm j at trading day d in excess of the risk-free asset, which we take to\n",
      "be the daily euro short-term rate (eSTR), and rM\n",
      "d denotes the excess return of the market (IBEX-35).\n",
      "These returns are obtained from adjusted close prices, which correct the price evolution for corporate\n",
      "actions such as dividends, stock splits, and new stock issuance. The notation overload in the regression\n",
      "coeï¬cients (Î±(i,j), Î²(i,j)) emphasizes the fact that Î± and Î² are speciï¬c to each pair (i, j) â B since the\n",
      "market model is computed for each ï¬rm j â FIBEX-35 on a lookback window of time which is particular\n",
      "to each article i â D.\n",
      "The reason why we ï¬t a market model to each (i, j) â B is to then apply a market-neutral strategy\n",
      "\n",
      "ð Doc 62: to each article i â D.\n",
      "The reason why we ï¬t a market model to each (i, j) â B is to then apply a market-neutral strategy\n",
      "as in [26] and [35]. This is an investment approach designed to minimize or eliminate exposure to overall\n",
      "market movements, isolating the performance of a speciï¬c ï¬rm. In particular, we employ a beta-neutral\n",
      "strategy by buying one unit of ï¬rm jâs stock and shorting Î²(i,j) units of the market index (i.e.: an ETF\n",
      "replicating the IBEX-35). This hedged position harvests the idiosyncratic returns from the market model\n",
      "and it only makes sense when ï¬rm jâs returns are expected to outperform or underperform the market.3\n",
      "The position delivers abnormal returns AR(i,j)\n",
      "d at some trading day d â¥ Ëdi\n",
      "0 given by\n",
      "rj\n",
      "d â Î²(i,j)rM\n",
      "d = Î±(i,j) + ó°(i,j)\n",
      "d =: AR(i,j)\n",
      "d .\n",
      "The position is taken at the eï¬ective treatment date Ëdi\n",
      "0 and is maintained over a holding window consisting\n",
      "of L â N trading days after Ëdi\n",
      "0, where L is set to 4 trading days. The justiï¬cation for this choice of L\n",
      "\n",
      "ð Doc 63: of L â N trading days after Ëdi\n",
      "0, where L is set to 4 trading days. The justiï¬cation for this choice of L\n",
      "results from the maximization of the Sharpe Ratio of the portfolio in the train and validation samples for\n",
      "both KMeans and LLM-based clustering.4 Finally, we compute the Sharpe Ratio of each position SR(i,j),\n",
      "which we will subsequently employ to optimize cluster selection.\n",
      "5.2 Optimal Cluster Selection\n",
      "After taking beta-neutral positions on each pair (i, j) â B and holding them over L days, we can obtain\n",
      "a measure of how proï¬table the positions are on average for articles that belong to the same cluster. For\n",
      "this purpose, let Bg denote the set of all article-ï¬rm pairs such that the article belongs to some cluster\n",
      "g â G.\n",
      "Bg := {(i, j) | (i, j) â B â§ i â Dg}.\n",
      "3 For expected underperformance of ï¬rm j, reverse the beta-neutral positions: sell one unit of ï¬rm j and buy Î²(i,j) units\n",
      "of the market index. However, note that this will be handled later by a Trading Rule (T R).\n",
      "\n",
      "ð Doc 64: of the market index. However, note that this will be handled later by a Trading Rule (T R).\n",
      "4 The choice of L is justiï¬ed in detail in A.2, and the sensitivity of the trading strategyâs out-of-sample performance to\n",
      "diï¬erent values of L is examined in Section 6 (âRobustness Checksâ).\n",
      "17\n",
      "\n",
      "ð Doc 65: The average Sharpe Ratio associated to each cluster is\n",
      "SRg = 1\n",
      "|Bg|\n",
      "ó°\n",
      "(i,j)âBg\n",
      "SR(i,j),\n",
      "and it provides a measure of the performance of the beta-neutral positions in each cluster. The distribution\n",
      "of cluster-average Sharpe Ratios across the diï¬erent clusters is shown in Appendix Figure A5.\n",
      "We then focus on developing two algorithms that optimally leverage the cluster information for our\n",
      "trading strategy. Our approach draws parallels with traditional portfolio sorting methods, where assets\n",
      "are typically arranged into deciles based on speciï¬c characteristics, and trading positions are established\n",
      "by going long on top deciles and short on bottom ones. Similarly, our strategy will construct self-ï¬nancing\n",
      "portfolios based on clusters rather than individual assets: taking long positions in clusters expected to\n",
      "outperform and short positions in those expected to underperform. To identify the optimal clusters for\n",
      "\n",
      "ð Doc 66: outperform and short positions in those expected to underperform. To identify the optimal clusters for\n",
      "trading, we propose two distinct algorithmic approaches. The ï¬rst approach, which we term âgreedyâ,\n",
      "selects clusters by maximizing the Sharpe Ratio within the validation dataset. The second approach,\n",
      "termed âstableâ, utilizes a broader information set by incorporating both training and validation data,\n",
      "aiming to identify clusters that maintain consistent performance across both splits. In both algorithms, we\n",
      "impose sign restrictions to ensure that our trading positions align with the expected direction of returns.\n",
      "5.2.1 Greedy Algorithm\n",
      "The greedy selection of clusters is done in the validation sample Bval := {(i, j) â B | i â Dval} , from\n",
      "where we compute the cluster-average SRval\n",
      "g for each g â G. Deï¬ne Gval\n",
      "SR+ := {g â G | SRval\n",
      "g > 0} and\n",
      "Gval\n",
      "SRâ := {g â G | SRval\n",
      "g < 0} as the sets of clusters with positive and negative Sharpe Ratios in the\n",
      "\n",
      "ð Doc 67: SR+ := {g â G | SRval\n",
      "g > 0} and\n",
      "Gval\n",
      "SRâ := {g â G | SRval\n",
      "g < 0} as the sets of clusters with positive and negative Sharpe Ratios in the\n",
      "validation sample. Obviously, we will be interested in taking long positions when reading an article that\n",
      "is clustered in some g â Gval\n",
      "SR+ , and short positions in clusters g â Gval\n",
      "SR+ . However, our trading strategy\n",
      "will not trade every cluster g â G. Instead, it will select the clusters from GSR+ and GSRâ that lead the\n",
      "to most proï¬table trades. To identify such clusters, we rank them by their average Sharpe Ratio. Deï¬ne\n",
      "the ranking function R : G â {1, . . . , kâ} such that\n",
      "Rval\n",
      "g =\n",
      "ó°\n",
      "hâG\n",
      "1\n",
      "ó°\n",
      "SRval\n",
      "h â¥ SRval\n",
      "g\n",
      "ó°\n",
      ",\n",
      "where 1(Â· ) is the indicator function which equals 1 if the condition inside is true and 0 otherwise.\n",
      "The number of traded clusters on either side (long and short) will be upper-bounded by some hyper-\n",
      "parameter of our choice Î¸ â N which we set proportional to the number of clusters. Namely, Î¸ = âÏkâ\n",
      "\n",
      "ð Doc 68: parameter of our choice Î¸ â N which we set proportional to the number of clusters. Namely, Î¸ = âÏkâ\n",
      "for some Ï â (0, 1), which has been set to Ï = 0.5 to maximize the Sharpe Ratio of the trading strategy\n",
      "in the training and validation samples 5. The actual number of traded clusters will not be exactly Î¸ as\n",
      "5 The choice of Î¸ is justiï¬ed in detail in A.2. The sensitivity of the trading strategyâs out-of-sample performance to\n",
      "diï¬erent values of Î¸ is examined in Section 6 (âRobustness Checksâ).\n",
      "18\n",
      "\n",
      "ð Doc 69: there is a natural bound coming from the cardinalities of GSR+ and GSRâ. Hence, the actual number of\n",
      "long and short-traded clusters will be Î¸+ := min(Î¸, |GSR+ |) and Î¸â := min(Î¸, |GSRâ|). The set of traded\n",
      "clusters GÎ¸ is deï¬ned as\n",
      "GÎ¸ :=\n",
      "ó°±\n",
      "g â G | 1 â¤ Rval\n",
      "g â¤ Î¸+ â¨ kâ â Î¸â < Rval\n",
      "g â¤ kâ\n",
      "ó°²\n",
      "= G+\n",
      "Î¸ âª Gâ\n",
      "Î¸ ,\n",
      "where G+\n",
      "Î¸ := {g â G | 1 â¤ Rval\n",
      "g â¤ Î¸+} is the set of long-traded clusters, Gâ\n",
      "Î¸ := {g â G | kâ â Î¸â < Rval\n",
      "g â¤\n",
      "kâ} is the set of short-traded clusters and, clearly, |GÎ¸| = Î¸+ + Î¸â.6 In Appendix Algorithm 2., we can\n",
      "ï¬nd the formal design of this algorithm.\n",
      "5.2.2 Stable Algorithm\n",
      "In this case, we prioritize the stability of the cluster rankings by ensuring that the traded clusters\n",
      "minimize the rank diï¬erence of the cluster-average Sharpe Ratios between the training and validation\n",
      "samples. To begin, we compute the rank of each cluster based on the average Sharpe Ratios in both the\n",
      "training and validation samples. This delivers {Rtr\n",
      "g }gâG and {Rval\n",
      "g }gâG, which provides a measure of the\n",
      "\n",
      "ð Doc 70: training and validation samples. This delivers {Rtr\n",
      "g }gâG and {Rval\n",
      "g }gâG, which provides a measure of the\n",
      "relative performance of the clusters within each sample.\n",
      "Next, we calculate the absolute diï¬erence in ranks between the training and validation samples for\n",
      "each cluster, which allows us to measure the stability of each clusterâs performance between the two\n",
      "samples\n",
      "Î´g := |Rtr\n",
      "g â Rval\n",
      "g | .\n",
      "Clusters are then sorted based on their rank diï¬erences Î´g in descending order. To do this, we can\n",
      "simply compute the ranking of the ranking diï¬erences as\n",
      "R(Î´g) :=\n",
      "ó°\n",
      "hâG\n",
      "1 (Î´g â¥ Î´h) .\n",
      "Next, we select the top 2Î¸ â N clusters with the smallest rank diï¬erences, indicating the most stable\n",
      "clusters across the training and validation samples. The selected clusters now are\n",
      "GÎ¸ = {g â G | 1 â¤ R(Î´g) â¤ 2Î¸} .\n",
      "Finally, we determine the sets of long and short-traded clusters based on the average Sharpe Ratios\n",
      "in both the training and validation samples. In particular, the set of long-traded clusters (G+\n",
      "\n",
      "ð Doc 71: in both the training and validation samples. In particular, the set of long-traded clusters (G+\n",
      "Î¸ ) are the\n",
      "ones that have positive average Sharpe Ratios in both, training and validation samples\n",
      "G+\n",
      "Î¸ = {g â GÎ¸ | SRtr\n",
      "g > 0 â§ SRval\n",
      "g > 0},\n",
      "6 Alternatively, we could trade the same number of clusters in the long and short side by deï¬ning a unique Î¸â :=\n",
      "min (Î¸, |GSR+|, |GSRâ|) such that GÎ¸ :=\n",
      "ó°g â G | 1 â¤ Rval\n",
      "g â¤ Î¸â â¨ kâ â Î¸â < Rval\n",
      "g â¤ kâó° and |GÎ¸| = 2Î¸â.\n",
      "19\n",
      "\n",
      "ð Doc 72: and by symmetry, short-traded clusters (Gâ\n",
      "Î¸ ) are the ones that have negative average Sharpe Ratios in\n",
      "both, training and validation samples\n",
      "Gâ\n",
      "Î¸ = {g â GÎ¸ | SRtr\n",
      "g < 0 â§ SRval\n",
      "g < 0} .\n",
      "This approach ensures that we select the most stable clusters for trading, reducing the risk associated\n",
      "with rank variability between the training and validation samples, and ensuring that the direction of\n",
      "the signal is consistent across the two splits. The ï¬nal output consists of the sets of long-traded and\n",
      "short-traded clusters, which are then used to implement the trading strategy. The implementation of the\n",
      "algorithm is methodically presented in Appendix Algorithm 3.\n",
      "[Insert Table 3 about here]\n",
      "In Table 3 we show the 26 clusters with their proposed names (based on the articles they pool together\n",
      "as shown in Appendix Table A1) and the selection of long and short-traded clusters according to each\n",
      "algorithm: âgreedyâ and âstableâ. We write âlongâ for those clusters g â G+\n",
      "Î¸ and âshortâ for g â Gâ\n",
      "Î¸ .\n",
      "\n",
      "ð Doc 73: algorithm: âgreedyâ and âstableâ. We write âlongâ for those clusters g â G+\n",
      "Î¸ and âshortâ for g â Gâ\n",
      "Î¸ .\n",
      "As we can see, trading clusters of news articles based on this procedure is quite risky, as there is a high\n",
      "reliance of the signal on the past performance of a cluster. For example, clusters 21 and 22 are linked to\n",
      "the ï¬nancial performance of Repsol and Aena, respectively, during the training and validation samples.\n",
      "Evidently, the future performance of these ï¬rms can change, but the signal provided by the algorithm\n",
      "will still indicate âlongâ. Additionally, some clusters are heavily built on speciï¬c events of the period\n",
      "of time they were constructed upon. For example, cluster 17 pools articles related to the challenges of\n",
      "the tourism industry in Spain in Covid times, and cluster 25 is related to the post-covid developments of\n",
      "Inditex and Acerinox. Thus, a clustering approach based on embeddings is not generalizable over time. As\n",
      "\n",
      "ð Doc 74: Inditex and Acerinox. Thus, a clustering approach based on embeddings is not generalizable over time. As\n",
      "the world evolves, clusters become outdated and require constant recalibration to maintain their relevance\n",
      "and predictive power. Hence, any trading strategy based solely on historical cluster performance is likely\n",
      "to produce misguided trading signals over time\n",
      "[Insert Table 4 about here]\n",
      "In contrast, our LLM-based clustering methodology oï¬ers signiï¬cant advantages by focusing on the\n",
      "fundamental nature of economic shocks rather than historical patterns. This approach provides more\n",
      "robust and generalizable signals that are less susceptible to temporal changes in market conditions. More-\n",
      "over, unlike the black box nature of vector embeddings, our methodology oï¬ers transparency and inter-\n",
      "pretability in signal generation. This is evident in how the Greedy algorithmâs cluster selection closely\n",
      "\n",
      "ð Doc 75: pretability in signal generation. This is evident in how the Greedy algorithmâs cluster selection closely\n",
      "aligns with the direction of economic shocks: negative shocks typically correspond to price decreases and\n",
      "positive shocks to increases.\n",
      "Looking at Table 4, we observe that both algorithms consistently short articles classiï¬ed as policy\n",
      "shocks, regardless of direction, while going long on cluster 8, which contains approximately one-third\n",
      "20\n",
      "\n",
      "ð Doc 76: of news articles (those categorized as undergoing ï¬nancial minor and positive shocks). This consistent\n",
      "shorting of policy shocks likely reï¬ects marketsâgeneral aversion to policy uncertainty, as policy changes\n",
      "âeven positive onesâ often create implementation uncertainty and take time for market participants to\n",
      "fully price in. Interestingly, both algorithms also exhibit seemingly counter-intuitive behavior by going\n",
      "long on negative major demand shocks and short on positive major demand shocks. This pattern might\n",
      "suggest a âmean reversionâ expectation in the algorithms, where major demand shocks are viewed as\n",
      "temporary deviations that will eventually correct: negative shocks present buying opportunities, while\n",
      "positive shocks signal potential overvaluation.\n",
      "5.3 Trading Rule & Portfolio Construction\n",
      "For a given selection of clusters G+\n",
      "Î¸ and Gâ\n",
      "Î¸ , we launch trades and hold them for L = 4 trading days.\n",
      "Formally, the trading rule for a pair (i, j) â B at trading day d â Ëd is\n",
      "\n",
      "ð Doc 77: Î¸ and Gâ\n",
      "Î¸ , we launch trades and hold them for L = 4 trading days.\n",
      "Formally, the trading rule for a pair (i, j) â B at trading day d â Ëd is\n",
      "T RL,Î¸ã(i, j), dã :=\n",
      "ó°»\n",
      "ó°ó°ó°¿\n",
      "ó°ó°ó°½\n",
      "+1 if [(i, j) â Bg â§ g â G+\n",
      "Î¸ ] â§ d â ( Ëdi\n",
      "0, Ëdi\n",
      "0 + L]\n",
      "0 if [(i, j) â Bg â§ g ââ GÎ¸ ] â¨ d ââ ( Ëdi\n",
      "0, Ëdi\n",
      "0 + L]\n",
      "â1 if [(i, j) â Bg â§ g â Gâ\n",
      "Î¸ ] â§ d â ( Ëdi\n",
      "0, Ëdi\n",
      "0 + L]\n",
      ".\n",
      "In this context, a portfolio is a collection of positions taken in a ï¬rmâs stocks according to T RL,Î¸ã(i, j), dã.\n",
      "In other words, it is the set of all ã(i, j), dã for which a trade is executed.\n",
      "P := ó°\n",
      "ã(i, j), dã ó°ó° (i, j) â B â§ d â Ëd â§ T RL,Î¸ã(i, j), dã â= 0ó°\n",
      ".\n",
      "The set of open positions on a particular day d â Ëd is deï¬ned as\n",
      "Pd := {(i, j) â B | T RL,Î¸ã(i, j), dã â= 0} ,\n",
      "and the portfolio is rebalanced every day, so each position (i, j) â Pd receives a weight that is inversely\n",
      "proportional to the total amount of open positions in that day (i.e. 1/|Pd|).7 This produces an equally-\n",
      "\n",
      "ð Doc 78: proportional to the total amount of open positions in that day (i.e. 1/|Pd|).7 This produces an equally-\n",
      "weighted rolling-portfolio similar to [36] and [26]. The overlapping returns of the portfolio at d â Ëd can\n",
      "be obtained as an average of the abnormal returns weighted by the trading rule, which determines the\n",
      "direction of each position (long or short), and scaled by the number of open positions in that day,\n",
      "rP\n",
      "d := 1\n",
      "|Pd|\n",
      "ó°\n",
      "(i,j),âPd\n",
      "T RL,Î¸ã(i, j), dã Â· AR(i,j)\n",
      "d .\n",
      "In Figure 8 we plot the cumulative gross returns of trading strategies based on KMeans clustering\n",
      "(Panel A) and LLM clustering (Panel B) across diï¬erent data splits\n",
      "7 Note that the cardinality of the set of open positions at day d â Ëd, denoted as |Pd|, can be computed as the sum of the\n",
      "absolute values of the trading rule over all pairs (i, j) â B for a given trading day d â Ëd.\n",
      "|Pd| =\n",
      "ó°\n",
      "(i,j)âB\n",
      "|T RL,Î¸ã(i, j), dã| .\n",
      "21\n",
      "\n",
      "ð Doc 79: [Insert Figure 8 about here]\n",
      "KMeans. In panel A of Table 5 we show the portfolio statistics of the benchmark model. As we can\n",
      "see, both algorithms work well on the data splits they were trained on: the Stable algorithm works well on\n",
      "both, training and validation data, while the Greedy algorithm does a good job only on validation data as\n",
      "expected. However, this doesnât say anything about any of these algorithms, as it is easy to make proï¬table\n",
      "trades in-sample. The generalizability of the strategy is determined out-of-sample in the test data. The\n",
      "empirical analysis reveals signiï¬cant challenges in the strategyâs ability to maintain consistent performance\n",
      "across diï¬erent time periods. During the training and validation phases, the methodology shows promising\n",
      "results with annualized returns ranging from 26.6% to 47.7% and strong risk-adjusted performance metrics\n",
      "(Sharpe ratios between 2.0 and 3.2). However, this performance deteriorates substantially in the test\n",
      "\n",
      "ð Doc 80: (Sharpe ratios between 2.0 and 3.2). However, this performance deteriorates substantially in the test\n",
      "period, where returns drop to modest levels (2.9% to 4.9% annually) with signiï¬cantly lower Sharpe\n",
      "ratios (0.2 to 0.7), suggesting that the strategyâs alpha-generating capability does not generalize well\n",
      "out of sample. The distributional properties of returns in the test period provide additional insights\n",
      "into the strategyâs behavior under true out-of-sample conditions. The shift from negative to strongly\n",
      "positive skewness (1.85 to 2.46) coupled with high excess kurtosis (5.50 to 14.57) suggests that the\n",
      "strategyâs return distribution has fundamentally changed, characterized by more frequent small losses\n",
      "oï¬set by occasional large gains. This asymmetric return pattern, while potentially appealing from a risk\n",
      "preference perspective, diï¬ers markedly from the training period characteristics. The tail risk measures\n",
      "\n",
      "ð Doc 81: preference perspective, diï¬ers markedly from the training period characteristics. The tail risk measures\n",
      "further illuminate the strategyâs risk proï¬le, with annualized 95% VaR ranging from -7.8% to -18.9%\n",
      "and corresponding CVaR from -9.7% to -26.8% in the test period. These statistical properties, combined\n",
      "with the strong dependence on historical cluster-speciï¬c performance, indicate that the strategy fails to\n",
      "identify stable and generalizable trading signals, likely due to its reliance on ï¬rm and industry-speciï¬c\n",
      "clustering patterns that do not persist out of sample. As we can see in the plot, neither algorithm is\n",
      "able to generate a consistent proï¬le of earnings, and the statistics conï¬rm that proï¬ts are negligible, and\n",
      "would likely be eaten away by exogenous market frictions (e.g. trading costs).\n",
      "[Insert Table 5 about here]\n",
      "LLM. Panel B of Table 5 presents the performance metrics for our LLM-based approach. As before,\n",
      "\n",
      "ð Doc 82: [Insert Table 5 about here]\n",
      "LLM. Panel B of Table 5 presents the performance metrics for our LLM-based approach. As before,\n",
      "both algorithms perform really well on âseenâ data. However, diï¬erent from before, the Greedy algorithm\n",
      "works well also on the Training Split (which it was not trained on). More importantly, both algorithms\n",
      "do a great job in the test data. As we can see, both are able to achieve a consistent proï¬le of earnings\n",
      "through the split. The portfolio statistics reveal notable consistency in the strategyâs performance across\n",
      "diï¬erent time periods. During the training and validation phases, the methodology demonstrates solid\n",
      "performance with annualized returns ranging from 16.0% to 28.3% and Sharpe ratios between 1.4 and 2.9.\n",
      "This performance strengthens in the test period, where returns increase to 30.8%-37.2% annually with\n",
      "Sharpe ratios of 4.3-4.4, indicating that the strategyâs alpha-generating capability successfully generalizes\n",
      "22\n",
      "\n",
      "ð Doc 83: to out-of-sample conditions. The distributional properties of returns provide evidence for the strategyâs\n",
      "robustness. The test period maintains positive skewness (0.84 to 1.49) and moderate to high excess\n",
      "kurtosis (1.95 to 8.30), indicating an asymmetric return pattern with more frequent small losses oï¬set\n",
      "by larger gains. This return distribution is complemented by contained maximum drawdowns (1.1%\n",
      "to 1.5%) and strong Calmar ratios (21.0 to 34.5) in the test period. The tail risk measures further\n",
      "support the strategyâs risk management properties, with annualized 95% VaR ranging from -6.9% to -\n",
      "9.5%, and CVaR ranging from -9.9% to -11.3% in the test period. Taken together, the strategyâs ability\n",
      "to sustain consistent out-of-sample performance metrics demonstrates that the LLM-based clustering\n",
      "approach identiï¬es enduring trading signals that transcend speciï¬c market regimes.\n",
      "While our primary focus has been on developing a methodology to anticipate market reactions to\n",
      "\n",
      "ð Doc 84: While our primary focus has been on developing a methodology to anticipate market reactions to\n",
      "news (i.e., identifying winners and losers to assess the predictive power of our LLM-based approach),\n",
      "we also analyze the trading intensity and implementation costs of the resulting strategies. The detailed\n",
      "examination in A.8 reveals that, after accounting for transaction costs, the LLM-based approach maintains\n",
      "its superior performance relative to KMeans, though with attenuated proï¬tability. Therefore, practitioners\n",
      "interested in the practical implementation of this strategy would beneï¬t from optimizing the trading\n",
      "strategy by incorporating transaction costs into their framework.\n",
      "6. Robustness Checks\n",
      "In our applications we have worked with a holding period of L = 4 trading days and an upper bound\n",
      "on traded clusters of Î¸ = â0.5kâ. As shown in A.2, such choices result from the maximization of the\n",
      "\n",
      "ð Doc 85: on traded clusters of Î¸ = â0.5kâ. As shown in A.2, such choices result from the maximization of the\n",
      "Sharpe Ratios in the train and validation samples. All that is left is to check whether our out-of-sample\n",
      "results are sensitive to the choice of hyperparameters (L, Î¸). For this purpose, we evaluate the variability\n",
      "of the Sharpe Ratios of the test portfolio (SRPtest\n",
      ") to changes in L and Î¸.\n",
      "First, we focus on the holding period length of the beta-neutral strategy (L). For this purpose, we\n",
      "ï¬x Î¸ = â0.5kâ and, for each clustering method, obtain the series of Sharpe Ratios over a grid L (which\n",
      "ranges from 1 to 20 trading periods). This delivers the series {SRPtest\n",
      "(L)}LâL, which we then plot in\n",
      "two formats. On the left side of Figure 9 we plot the distribution of Sharpe Ratios in the grid, and in the\n",
      "right side, we show the mapping L ó°â SRPtest\n",
      "(L) over L.\n",
      "[Insert Figure 9 about here]\n",
      "From Figure 9a it follows that KMeans clustering produces a distribution that is clearly left-skewed,\n",
      "\n",
      "ð Doc 86: (L) over L.\n",
      "[Insert Figure 9 about here]\n",
      "From Figure 9a it follows that KMeans clustering produces a distribution that is clearly left-skewed,\n",
      "while the distribution of SRPtest\n",
      "for LLM clustering is clearly right-skewed (Figure 9c). This conï¬rms the\n",
      "fact that LLM clustering generates Sharpe Ratios that are statistically higher than those generated by\n",
      "KMeans. The plots in the right-hand-side substantiate this observation: KMeans is only able to produce\n",
      "positive SRPtest\n",
      "for really short holding window lengths (Figure 9b), while LLM clustering, although\n",
      "23\n",
      "\n",
      "ð Doc 87: not always stable, is, in general, able to produce positive Sharpe Ratios more consistently over the grid\n",
      "(Figure 9d).\n",
      "We then turn to analyze the sensitivity of SRPtest\n",
      "to diï¬erent values for the upper bound on the number\n",
      "of traded clusters (Î¸). Now we ï¬x L = 4 and deï¬ne a grid Î¸, from where we can obtain {SRPtest\n",
      "(Î¸)}Î¸âÎ¸.\n",
      "[Insert Figure 10 about here]\n",
      "The results of this exercise are shown in Figure 10. As we can see, in Figure 10a the results are mixed\n",
      "for the case of KMeans clustering. Namely, the Stable algorithm is able to generate positive Sharpe Ratios\n",
      "but the Greedy algorithm struggles to do so. In Figure 10b we see what is happening: Stable works well\n",
      "with for low values of Î¸, while Greedy only works for high values of Î¸. This high reliance of the algorithms\n",
      "on speciï¬c values of Î¸ points to the instability of the trading strategy when employing KMeans clustering.\n",
      "On the other hand, Figure 10c shows a clear pattern for the case of LLM clustering. Namely, the mass\n",
      "\n",
      "ð Doc 88: On the other hand, Figure 10c shows a clear pattern for the case of LLM clustering. Namely, the mass\n",
      "accumulates at high and positive Sharpe Ratios. This observation is further substantiated by Figure 10b,\n",
      "which shows that leaving aside the fact that the greedy algorithm does bad for really low values of Î¸ (i.e.:\n",
      "Î¸ â¤ 3), in general, the trading strategy is now able to produce high, positive and stable Sharpe Ratios\n",
      "across diï¬erent values of Î¸.\n",
      "All in all, our results are robust to hyperparameter variability, showing that LLM clustering consis-\n",
      "tently beats a strategy based on clustering embeddings with KMeans.\n",
      "7. Conclusion\n",
      "This paper investigates how information from business news aï¬ects stock market prices. We analyze\n",
      "a dataset of Spanish business articles during a particularly volatile period-the COVID-19 pandemic-\n",
      "and examine ï¬rm-speciï¬c stock market reactions to news. We show that transforming text into vector\n",
      "\n",
      "ð Doc 89: and examine ï¬rm-speciï¬c stock market reactions to news. We show that transforming text into vector\n",
      "embeddings and clustering them using KMeans yields clusters that are ï¬rm-speciï¬c and industry-speciï¬c.\n",
      "However, the distribution of articles across clusters is unstable over sequential data splits, indicating\n",
      "temporal instability. When we implement a cluster-based trading strategy-similar to portfolio sorts-on\n",
      "the KMeans clusters, we observe an over-reliance on the past performance of a cluster. That is, signals\n",
      "are short-lived due to temporal instability. Consequently, the out-of-sample proï¬tability of the trading\n",
      "strategy is negligible, evidencing the methodâs poor temporal generalizability. Therefore, a model based\n",
      "on embeddings is superï¬cial and is not able to anticipate market trends.\n",
      "As an alternative, we develop a novel approach by guiding a Large Language Model (LLM) through a\n",
      "structured news-parsing schema, enabling it to analyze news-implied ï¬rm-speciï¬c economic shocks. The\n",
      "\n",
      "ð Doc 90: structured news-parsing schema, enabling it to analyze news-implied ï¬rm-speciï¬c economic shocks. The\n",
      "schema involves identifying the ï¬rms aï¬ected by the articles and classifying the implied shocks on such\n",
      "ï¬rms by their type, magnitude, and direction. This LLM-based methodology demonstrates several advan-\n",
      "tages over the traditional clustering approach. Even in a volatile period, it produces stable distributions\n",
      "24\n",
      "\n",
      "ð Doc 91: of articles across clusters in sequential splits, demonstrating robust temporal stability. Moreover, the re-\n",
      "sulting trading signals are both long-lasting and economically relevant, as they are based on fundamental\n",
      "economic shocks rather than statistical patterns. The results show that the LLM-based trading strategy\n",
      "eï¬ectively identiï¬es winners and losers, illustrating the parserâs ability to anticipate market trends by\n",
      "comprehending the economic implications of ï¬rm-speciï¬c shocks. This approach generates a consistent\n",
      "proï¬le of earnings in the test set, with results robust to the choice of hyperparameters-the holding period\n",
      "length of the trading strategy and the number of selected clusters for trading. Our ï¬ndings demonstrate\n",
      "a promising avenue: LLMs, when guided by appropriate economic frameworks, can help predict market\n",
      "reactions to news through systematic classiï¬cation of economic shocks embedded in ï¬nancial narratives.\n",
      "References\n",
      "\n",
      "ð Doc 92: reactions to news through systematic classiï¬cation of economic shocks embedded in ï¬nancial narratives.\n",
      "References\n",
      "[1] E. F. Fama, Eï¬cient capital markets: A review of theory and empirical work, J. Finance 25 (2)\n",
      "(1970) 383. doi:10.2307/2325486.\n",
      "URL http://dx.doi.org/10.2307/2325486\n",
      "[2] P. C. Tetlock, Giving content to investor sentiment: The role of media in the stock market, J. Finance\n",
      "62 (3) (2007) 1139â1168. doi:10.1111/j.1540-6261.2007.01232.x.\n",
      "URL http://dx.doi.org/10.1111/j.1540-6261.2007.01232.x\n",
      "[3] P. C. Tetlock, M. Saar-Tsechansky, S. Macskassy, More than words: Quantifying language to measure\n",
      "ï¬rmsâfundamentals, J. Finance 63 (3) (2008) 1437â1467. doi:10.1111/j.1540-6261.2008.01362.\n",
      "x.\n",
      "URL http://dx.doi.org/10.1111/j.1540-6261.2008.01362.x\n",
      "[4] T. Loughran, B. McDonald, When is a liability not a liability? textual analysis, dictionaries, and\n",
      "10ks, J. Finance 66 (1) (2011) 35â65. doi:10.1111/j.1540-6261.2010.01625.x.\n",
      "URL http://dx.doi.org/10.1111/j.1540-6261.2010.01625.x\n",
      "\n",
      "ð Doc 93: 10ks, J. Finance 66 (1) (2011) 35â65. doi:10.1111/j.1540-6261.2010.01625.x.\n",
      "URL http://dx.doi.org/10.1111/j.1540-6261.2010.01625.x\n",
      "[5] N. Jegadeesh, D. Wu, Word power: A new approach for content analysis, J. Financ. Econ. 110 (3)\n",
      "(2013) 712â729. doi:10.1016/j.jfineco.2013.08.018.\n",
      "URL http://dx.doi.org/10.1016/j.jfineco.2013.08.018\n",
      "[6] J. Bollen, H. Mao, X. Zeng, Twitter mood predicts the stock market, J. Comput. Sci. 2 (1) (2011)\n",
      "1âÅ8. doi:10.1016/j.jocs.2010.12.007.\n",
      "URL http://dx.doi.org/10.1016/j.jocs.2010.12.007\n",
      "[7] D. Garcia, Sentiment during recessions, J. Finance 68 (3) (2013) 1267â1300. doi:10.1111/jofi.\n",
      "12027.\n",
      "URL http://dx.doi.org/10.1111/jofi.12027\n",
      "25\n",
      "\n",
      "ð Doc 94: [8] Z. T. Ke, B. Kelly, D. Xiu, Predicting returns with text data, Tech. rep., National Bureau of Economic\n",
      "Research (Aug. 2019). doi:10.3386/w26186.\n",
      "URL http://dx.doi.org/10.3386/w26186\n",
      "[9] C.-C. Lee, Z. Gao, C.-L. Tsai, Bert-based stock market sentiment analysis, in: 2020 IEEE Inter-\n",
      "national Conference on Consumer Electronics - Taiwan (ICCE-Taiwan), IEEE, 2020, pp. 1âÅ2.\n",
      "doi:10.1109/icce-taiwan49838.2020.9258102.\n",
      "URL http://dx.doi.org/10.1109/icce-taiwan49838.2020.9258102\n",
      "[10] F. Wei, U. Nguyen, Stock trend prediction using ï¬nancial market news and bert, in: Proceedings\n",
      "of the 12th International Joint Conference on Knowledge Discovery, Knowledge Engineering and\n",
      "Knowledge Management, SCITEPRESS - Science and Technology Publications, 2020, pp. 319â326.\n",
      "doi:10.5220/0010172103190326.\n",
      "URL http://dx.doi.org/10.5220/0010172103190326\n",
      "[11] W. Antweiler, M. Z. Frank, Do us stock markets typically overreact to corporate news stories?, SSRN\n",
      "Electron. J.doi:10.2139/ssrn.878091.\n",
      "\n",
      "ð Doc 95: [11] W. Antweiler, M. Z. Frank, Do us stock markets typically overreact to corporate news stories?, SSRN\n",
      "Electron. J.doi:10.2139/ssrn.878091.\n",
      "URL http://dx.doi.org/10.2139/ssrn.878091\n",
      "[12] S. Hansen, M. McMahon, A. Prat, Transparency and deliberation within the fomc: A computational\n",
      "linguistics approach, Q. J. Econ. 133 (2) (2018) 801âÅ870. doi:10.1093/qje/qjx045.\n",
      "URL http://dx.doi.org/10.1093/qje/qjx045\n",
      "[13] L. Bybee, B. Kelly, A. Manela, D. Xiu, Business news and business cycles, J. Finance 79 (5) (2024)\n",
      "3105â3147. doi:10.1111/jofi.13377.\n",
      "URL http://dx.doi.org/10.1111/jofi.13377\n",
      "[14] L. Bybee, B. Kelly, Y. Su, Narrative asset pricing: Interpretable systematic risk factors from news\n",
      "text, The Rev. Financ. Stud. 36 (12) (2023) 4759â4787. doi:10.1093/rfs/hhad042.\n",
      "URL http://dx.doi.org/10.1093/rfs/hhad042\n",
      "[15] G. Hoberg, G. Phillips, Text-based network industries and endogenous product diï¬erentiation, J.\n",
      "Polit. Econ. 124 (5) (2016) 1423â1465. doi:10.1086/688176.\n",
      "\n",
      "ð Doc 96: Polit. Econ. 124 (5) (2016) 1423â1465. doi:10.1086/688176.\n",
      "URL http://dx.doi.org/10.1086/688176\n",
      "[16] Q. Chen, Stock movement prediction with ï¬nancial news using contextualized embedding from bert,\n",
      "arXiv preprint arXiv:2107.08721doi:10.48550/ARXIV.2107.08721.\n",
      "URL https://arxiv.org/abs/2107.08721\n",
      "[17] E. Benincasa, J. Fu, M. Mishra, A. Paranjape, Diï¬erent shades of green: Estimating the green bond\n",
      "premium using natural language processing, SSRN Electron. J.doi:10.2139/ssrn.4198065.\n",
      "URL http://dx.doi.org/10.2139/ssrn.4198065\n",
      "26\n",
      "\n",
      "ð Doc 97: [18] M. Jha, H. Liu, A. Manela, Does ï¬nance beneï¬t society? a language embedding approach, Rev.\n",
      "Financ. Stud., Forthcomingdoi:10.2139/ssrn.3655263.\n",
      "URL http://dx.doi.org/10.2139/ssrn.3655263\n",
      "[19] C. L. Zhang, Feel the market: An attempt to identify additional factor in the capital asset pricing\n",
      "model (capm) using generative pre-trained transformer (gpt) and bidirectional encoder representa-\n",
      "tions from transformers (bert), SSRN Electron. J.doi:10.2139/ssrn.4521946.\n",
      "URL http://dx.doi.org/10.2139/ssrn.4521946\n",
      "[20] X. Gabaix, R. S. J. Koijen, M. Yogo, Asset embeddings, SSRN Electron. J.doi:10.2139/ssrn.\n",
      "4507511.\n",
      "URL http://dx.doi.org/10.2139/ssrn.4507511\n",
      "[21] D. Cutler, J. Poterba, L. Summers, What Moves Stock Prices?, 1988. doi:10.3386/w2538.\n",
      "URL http://dx.doi.org/10.3386/w2538\n",
      "[22] M. L. Mitchell, J. H. Mulherin, The impact of public information on the stock market, J. Finance\n",
      "49 (3) (1994) 923â950. doi:10.2307/2329211.\n",
      "URL http://dx.doi.org/10.2307/2329211\n",
      "\n",
      "ð Doc 98: 49 (3) (1994) 923â950. doi:10.2307/2329211.\n",
      "URL http://dx.doi.org/10.2307/2329211\n",
      "[23] S. R. Baker, N. Bloom, S. J. Davis, Measuring economic policy uncertainty, Q. J. Econ. 131 (4)\n",
      "(2016) 1593â1636. doi:10.1093/qje/qjw024.\n",
      "URL http://dx.doi.org/10.1093/qje/qjw024\n",
      "[24] S. Baker, N. Bloom, S. Davis, M. Sammon, What Triggers Stock Market Jumps?, 2021. doi:\n",
      "10.3386/w28687.\n",
      "URL http://dx.doi.org/10.3386/w28687\n",
      "[25] A. Manela, A. Moreira, News implied volatility and disaster concerns, J. Financ. Econ. 123 (1) (2017)\n",
      "137â162. doi:10.1016/j.jfineco.2016.01.032.\n",
      "URL http://dx.doi.org/10.1016/j.jfineco.2016.01.032\n",
      "[26] W. S. Chan, Stock price reaction to news and no-news: drift and reversal after headlines, J. Financ.\n",
      "Econ. 70 (2) (2003) 223â260. doi:10.1016/s0304-405x(03)00146-6.\n",
      "URL http://dx.doi.org/10.1016/S0304-405X(03)00146-6\n",
      "[27] P. Oncharoen, P. Vateekul, Deep learning for stock market prediction using event embedding and\n",
      "\n",
      "ð Doc 99: [27] P. Oncharoen, P. Vateekul, Deep learning for stock market prediction using event embedding and\n",
      "technical indicators, in: 2018 5th International Conference on Advanced Informatics: Concept Theory\n",
      "and Applications (ICAICTA), IEEE, 2018, pp. 19âÅ24. doi:10.1109/icaicta.2018.8541310.\n",
      "URL http://dx.doi.org/10.1109/icaicta.2018.8541310\n",
      "27\n",
      "\n",
      "ð Doc 100: [28] A. Lopez-Lira, Y. Tang, Can chatgpt forecast stock price movements? return predictability and large\n",
      "language models, SSRN Electron. J.doi:10.2139/ssrn.4412788.\n",
      "URL http://dx.doi.org/10.2139/ssrn.4412788\n",
      "[29] Y. Chen, B. T. Kelly, D. Xiu, Expected returns and large language models, Available at SSRN\n",
      "4416687.\n",
      "URL https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4416687\n",
      "[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, I. Polosukhin,\n",
      "Attention is all you need, Adv. Neural Inf. Process. Syst. 30. doi:10.48550/ARXIV.1706.03762.\n",
      "URL https://doi.org/10.48550/arxiv.1706.03762\n",
      "[31] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers\n",
      "for language understanding, arXiv preprint arXiv:1810.04805doi:10.48550/ARXIV.1810.04805.\n",
      "URL https://arxiv.org/abs/1810.04805\n",
      "[32] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., Improving language understanding by\n",
      "generative pre-training.\n",
      "\n",
      "ð Doc 101: [32] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., Improving language understanding by\n",
      "generative pre-training.\n",
      "[33] T. Le Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili, D. Hesslow, R. Castagn, A. S. Luccioni, F. Yvon,\n",
      "M. Gall, et al., Bloom: A 176b-parameter open-access multilingual language modeldoi:10.48550/\n",
      "ARXIV.2211.05100.\n",
      "URL https://arxiv.org/abs/2211.05100\n",
      "[34] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ© re, N. Goyal,\n",
      "E. Hambro, F. Azhar, et al., Llama: Open and eï¬cient foundation language models, arXiv preprint\n",
      "arXiv:2302.13971.\n",
      "[35] H. Jiang, S. Z. Li, H. Wang, Pervasive underreaction: Evidence from high-frequency data, J. Financ.\n",
      "Econ. 141 (2) (2021) 573â599. doi:10.1016/j.jfineco.2021.04.003.\n",
      "URL http://dx.doi.org/10.1016/j.jfineco.2021.04.003\n",
      "[36] N. Jegadeesh, S. Titman, Returns to buying winners and selling losers: Implications for stock market\n",
      "\n",
      "ð Doc 102: [36] N. Jegadeesh, S. Titman, Returns to buying winners and selling losers: Implications for stock market\n",
      "eï¬ciency, J. Finance 48 (1) (1993) 65â91. doi:10.1111/j.1540-6261.1993.tb04702.x.\n",
      "URL http://dx.doi.org/10.1111/j.1540-6261.1993.tb04702.x\n",
      "28\n",
      "\n",
      "ð Doc 103: Table 1: Summary Statistics of Articles by Data Split\n",
      "Data Split Time Period # Articles # Words Vocabulary Size\n",
      "Train 24/06/2020 â 12/02/2021 1254 327413 26762\n",
      "Validation 12/02/2021 â 21/06/2021 836 232912 22265\n",
      "Test 21/06/2021 â 30/09/2021 523 140495 16474\n",
      "All 24/06/2020 â 30/09/2021 2613 700820 42603\n",
      "Note: Summary statistics by data splits and for the whole sample. We provide the period spanned by each data\n",
      "split, the number of articles, the number of words, and the vocabulary size. Articles have been preprocessed following\n",
      "standard NLP practices.\n",
      "29\n",
      "\n",
      "ð Doc 104: Table 2: Function calling schema\n",
      "Function Prompt Options\n",
      "1. firms âList all the ï¬rms aï¬ected by the events\n",
      "narrated in the articleâ array\n",
      "1.1. firm âIterate over each firm in firmsâ string\n",
      "1.2. ticker âState the stock market ticker of firm â string\n",
      "1.3. shock_type âWhat type of shock does this article\n",
      "imply on firm ?â\n",
      "{demand, supply, ï¬nancial,\n",
      "technology, policy}\n",
      "1.4. shock_magnitude âHow much impact is this shock\n",
      "expected to have on firm?â {minor, major}\n",
      "1.5. shock_direction âIn what direction is this shock expected\n",
      "to impact firm?â {positive, negative}\n",
      "This table outlines the structure of the function calling schema we designed to guide the LLM through the analysis\n",
      "of news-implied ï¬rm-speciï¬c economic shocks. The âFunctionâ column speciï¬ces the name of the tool passed to\n",
      "the LLM. We can understand the umbrella function firms as running a loop over each of its arguments, with the\n",
      "\n",
      "ð Doc 105: the LLM. We can understand the umbrella function firms as running a loop over each of its arguments, with the\n",
      "indented subfunctions being referred to the speciï¬c argument passed to them. The âPromptâ column provides an\n",
      "example of the simpliï¬ed instructions given to the LLM (the actual prompts are longer as the LLM needs clear\n",
      "and detailed instructions, with useful examples for context). Finally, the âOptionsâ column imposes the answer\n",
      "format that the LLM must follow. For example, in firms, the âarrayâ option indicates that the answer must be\n",
      "an enumeration of ï¬rms, while the âstringâ option in the subfunctions firm and ticker indicates that the answer\n",
      "must be a single string. Finally, the shock_ subfunctions ask the LLM to choose from a predeï¬ned set of possible\n",
      "responses.\n",
      "30\n",
      "\n",
      "ð Doc 106: Table 3: Mapping of embeddings-based KMeans clusters to Trading Signals\n",
      "Cluster Greedy Stable\n",
      "0 Miscellaneous (Colonial, Acciona, Amadeus, Grifols, Endesa, IAG,\n",
      "Bankinter...) short\n",
      "1 Quarterly & Semi-Annual Earnings Reports short\n",
      "2 BBVA & Sabadell: Financial Performance & Strategic Movements short\n",
      "3 TelefÃ³nica & Cellnex: Telecommunications Tower Sales & Market Dynamics long long\n",
      "4 CaixaBank: Mergers and Strategic Moves in the Banking Sector\n",
      "5 TelefÃ³nica, Indra, & MÃ¡sMÃ³vil: Regulatory and Strategic Moves in Telecom long\n",
      "6 Siemens Gamesa: Supply Agreements, Proï¬tability Targets in Renewable\n",
      "Energy short\n",
      "7 Cellnex: Strategic Acquisitions and Financial Moves in Telecom Infrastructure long\n",
      "8 Acciona, Endesa, EnagÃ¡s & Naturgy: Strategic Moves & Regulatory\n",
      "Developments in the Energy Sector long\n",
      "9 Repsol: Strategic Moves and Challenges in the Energy Sector long\n",
      "10 Ferrovial, Acciona: Strategic Expansions and Financial Maneuvers in\n",
      "Infrastructure short short\n",
      "\n",
      "ð Doc 107: 10 Ferrovial, Acciona: Strategic Expansions and Financial Maneuvers in\n",
      "Infrastructure short short\n",
      "11 Solaria: Strategic Moves and Market Challenges in Renewable Energy long long\n",
      "12 Iberdrola: Strategic Collaborations and Renewable Energy Developments short\n",
      "13 IAG: Financial Performance long\n",
      "14 Santander & CaixaBank: Financial Moves and Sustainability Initiatives short\n",
      "15 ACS & Acciona: Strategic Movements and Infrastructure Projects short short\n",
      "16 TelefÃ³nica: Financial Performance and Strategic Moves long\n",
      "17 MeliÃ¡ and Spanish Tourism Sector: Challenges Amidst the Pandemic short\n",
      "18 Takeover Bids for Naturgy and MÃ¡sMÃ³vil short\n",
      "19 Naturgy: Financial Performance short short\n",
      "20 PharmaMar, Grifols: Regulatory Approvals and Market Moves in the\n",
      "Pharmaceutical Sector long long\n",
      "21 Repsol: Financial Performance long long\n",
      "22 Aena: Financial Performance long long\n",
      "23 EnagÃ¡s, Endesa, Iberdrola, Red ElÃ©ctrica: Regulatory and Market Challenges\n",
      "in the Energy Sector short\n",
      "\n",
      "ð Doc 108: 22 Aena: Financial Performance long long\n",
      "23 EnagÃ¡s, Endesa, Iberdrola, Red ElÃ©ctrica: Regulatory and Market Challenges\n",
      "in the Energy Sector short\n",
      "24 BBVA, CaixaBank, Banco Sabadell: Layoï¬s and Restructuring long long\n",
      "25 Inditex, Acerinox: Market Performance and Strategic Developments in the\n",
      "Post-Covid Context short short\n",
      "Note: Mapping of embeddings-based KMeans clusters to their Trading Signal (long/short) for the two proposed\n",
      "cluster-selection algorithms (Greedy and Stable). The Greedy algorithm longs (shorts) clusters that maximize (mini-\n",
      "mize) the cluster-average-SR in the validation sample subject to a positivity (negativity) constraint, while the Stable\n",
      "algorithm longs (shorts) clusters that minimize the rank diï¬erence between the training and validation rankings of\n",
      "the cluster-average-SRâs subject to a positivity (negativity) constraint, which is now imposed on both sample splits.\n",
      "\n",
      "ð Doc 109: the cluster-average-SRâs subject to a positivity (negativity) constraint, which is now imposed on both sample splits.\n",
      "In both algorithms, the cardinality of each leg is upper-bounded by a hyperparameter Î¸. Cluster labels are proposed\n",
      "based on the articles they pool.\n",
      "31\n",
      "\n",
      "ð Doc 110: Table 4: Mapping of LLM-based clusters to Trading Signals\n",
      "Cluster Greedy Stable\n",
      "0 (demand, minor, positive)\n",
      "1 (demand, minor, negative) short\n",
      "2 (demand, major, positive) short short\n",
      "3 (demand, major, negative) long long\n",
      "4 (supply, minor, positive) long\n",
      "5 (supply, minor, negative) short\n",
      "6 (supply, major, positive) long\n",
      "7 (supply, major, negative) short\n",
      "8 (ï¬nancial, minor, positive) long long\n",
      "9 (ï¬nancial, minor, negative) short\n",
      "10 (ï¬nancial, major, positive) long\n",
      "11 (ï¬nancial, major, negative) short\n",
      "12 (technology, minor, positive) long\n",
      "13 (technology, minor, negative)\n",
      "14 (technology, major, positive) short\n",
      "15 (technology, major, negative)\n",
      "16 (policy, minor, positive) short short\n",
      "17 (policy, minor, negative) short short\n",
      "18 (policy, major, positive) short short\n",
      "19 (policy, major, negative) short short\n",
      "Note: Mapping of LLM-based clusters to their Trading Signal (long/short) for the two proposed cluster-selection\n",
      "\n",
      "ð Doc 111: Note: Mapping of LLM-based clusters to their Trading Signal (long/short) for the two proposed cluster-selection\n",
      "algorithms (Greedy and Stable). The Greedy algorithm longs (shorts) clusters that maximize (minimize) the cluster-\n",
      "average-SR in the validation sample subject to a positivity (negativity) constraint, while the Stable algorithm longs\n",
      "(shorts) clusters that minimize the rank diï¬erence between the training and validation rankings of the cluster-\n",
      "average-SRâs subject to a positivity (negativity) constraint, which is now imposed on both sample splits. In both\n",
      "algorithms, the cardinality of each leg is upper-bounded by a hyperparameter Î¸. Each cluster corresponds to a type\n",
      "of news-implied ï¬rm-speciï¬c shock identiï¬ed by our LLM according to the function calling schema.\n",
      "32\n",
      "\n",
      "ð Doc 112: Table 5: Portfolio Statistics Comparison: KMeans vs LLM Clustering\n",
      "(a) Panel A: Statistics of PKMeans\n",
      "Split Algo. Cum.\n",
      "Ret.\n",
      "Avg.\n",
      "Ret.\n",
      "St.\n",
      "Dev.\n",
      "Sharpe\n",
      "Ra-\n",
      "tio\n",
      "Sortino\n",
      "Ra-\n",
      "tio\n",
      "Max.\n",
      "DD\n",
      "Calmar\n",
      "Ratio\n",
      "Skew. Exc.\n",
      "Kurt.\n",
      "VaR\n",
      "95%\n",
      "CVaR\n",
      "95%\n",
      "All Greedy 1.070 5.3 9.7 0.5 0.6 -6.9 0.8 -0.45 4.04 -13.7 -22.9\n",
      "Stable 1.489 35.8 16.8 1.8 2.2 -7.6 4.7 0.19 5.08 -22.6 -36.1\n",
      "Train Greedy 0.959 -6.2 11.7 -0.5 -0.5 -6.9 -0.9 -0.52 2.72 -18.3 -28.5\n",
      "Stable 1.250 40.4 19.6 1.7 2.0 -7.6 5.3 -0.22 3.24 -29.3 -43.1\n",
      "Validation Greedy 1.088 26.8 7.3 3.3 3.7 -3.5 7.8 -0.47 1.17 -9.5 -15.9\n",
      "Stable 1.149 47.6 13.3 2.9 3.5 -3.6 13.1 -0.19 1.76 -18.3 -28.1\n",
      "Test Greedy 1.014 4.9 6.9 0.7 1.0 -3.6 1.4 1.85 5.50 -7.8 -9.7\n",
      "Stable 1.008 2.9 14.3 0.2 0.3 -4.6 0.6 2.46 14.57 -18.9 -26.8\n",
      "(b) Panel B: Statistics of PLLM\n",
      "Split Algo. Cum.\n",
      "Ret.\n",
      "Avg.\n",
      "Ret.\n",
      "St.\n",
      "Dev.\n",
      "Sharpe\n",
      "Ra-\n",
      "tio\n",
      "Sortino\n",
      "Ra-\n",
      "tio\n",
      "Max.\n",
      "DD\n",
      "Calmar\n",
      "Ratio\n",
      "Skew. Exc.\n",
      "Kurt.\n",
      "VaR\n",
      "95%\n",
      "CVaR\n",
      "95%\n",
      "All Greedy 1.310 23.1 9.6 2.2 2.9 -6.3 3.7 1.47 9.93 -13.6 -18.9\n",
      "\n",
      "ð Doc 113: Sharpe\n",
      "Ra-\n",
      "tio\n",
      "Sortino\n",
      "Ra-\n",
      "tio\n",
      "Max.\n",
      "DD\n",
      "Calmar\n",
      "Ratio\n",
      "Skew. Exc.\n",
      "Kurt.\n",
      "VaR\n",
      "95%\n",
      "CVaR\n",
      "95%\n",
      "All Greedy 1.310 23.1 9.6 2.2 2.9 -6.3 3.7 1.47 9.93 -13.6 -18.9\n",
      "Stable 1.365 27.0 8.6 2.8 3.4 -5.9 4.6 0.28 2.24 -11.9 -16.9\n",
      "Train Greedy 1.112 17.6 11.4 1.4 1.9 -6.3 2.8 1.65 9.00 -15.7 -21.0\n",
      "Stable 1.177 28.3 9.9 2.5 3.0 -5.9 4.8 0.16 1.71 -13.5 -19.6\n",
      "Validation Greedy 1.091 28.0 8.2 3.0 4.0 -3.1 9.1 0.14 1.37 -10.6 -16.8\n",
      "Stable 1.048 14.2 7.0 1.9 2.1 -1.9 7.4 0.25 1.37 -11.1 -14.7\n",
      "Test Greedy 1.084 30.8 6.2 4.3 6.0 -1.5 21.0 1.49 8.30 -6.9 -9.9\n",
      "Stable 1.100 37.2 7.1 4.4 7.2 -1.1 34.5 0.84 1.95 -9.5 -11.3\n",
      "Note: Portfolio statistics of trading strategies based on clusters obtained from KMeans (Panel A) and LLM (Panel\n",
      "B) approaches. The statistics provided include performance metrics (Cumulative Return, Average Return (%)), risk\n",
      "measures (Standard Deviation (%), Maximum Drawdown (%), Value at Risk (%), Conditional Value at Risk (%)),\n",
      "\n",
      "ð Doc 114: measures (Standard Deviation (%), Maximum Drawdown (%), Value at Risk (%), Conditional Value at Risk (%)),\n",
      "risk-adjusted performance ratios (Sharpe Ratio, Sortino Ratio, Calmar Ratio), and return distribution characteristics\n",
      "(Skewness, Excess Kurtosis). These statistics are provided for both cluster-selection algorithms: Greedy and Stable.\n",
      "Except for the Cumulative Return, all returns are annualized. The Sharpe Ratio is computed using the daily returns,\n",
      "assuming 252 trading days in a year. The Sortino Ratio is calculated using the daily downside returns. The\n",
      "Maximum Drawdown is the maximum loss from a peak to a trough. The Calmar Ratio is the ratio of the annualized\n",
      "return to the maximum drawdown. Skewness measures the asymmetry of the return distribution, while Kurtosis\n",
      "quantiï¬es the tailsâthickness. The Value at Risk (VaR) and Conditional Value at Risk (CVaR) are calculated at a\n",
      "\n",
      "ð Doc 115: quantiï¬es the tailsâthickness. The Value at Risk (VaR) and Conditional Value at Risk (CVaR) are calculated at a\n",
      "95% conï¬dence level. The Greedy algorithm longs (shorts) clusters that maximize (minimize) the cluster-average-\n",
      "SR in the validation sample subject to a positivity (negativity) constraint, while the Stable algorithm longs (shorts)\n",
      "clusters that minimize the rank diï¬erence between the training and validation rankings of the cluster-average-SRâs\n",
      "subject to a positivity (negativity) constraint, which is now imposed on both sample splits. In both algorithms, the\n",
      "cardinality of each leg is upper-bounded by a hyperparameter Î¸. The holding period of the beta-neutral positions is\n",
      "set to L = 4 trading days for both approaches. The number of traded clusters is Î¸ = 0.5k = 13 for KMeans (kâ = 26\n",
      "clusters) and Î¸ = 0.5k = 10 for LLM (kâ = 20 clusters). The selection criteria for these hyperparameters (L, Î¸) is\n",
      "based on maximizing the Sharpe Ratios of the train and validation samples.\n",
      "33\n",
      "\n",
      "ð Doc 116: Figure 1: Word Cloud of all the dataset\n",
      "Note: This Word Cloud visualizes the most frequent words in our dataset of Spanish business news articles. Larger\n",
      "words correspond to higher frequencies. The color of the words is purely for visual diï¬erentiation and holds no\n",
      "additional meaning. The most prominent words include âempresaâ (ï¬rm), âcompaÃ±Ã­aâ (company), and âespaÃ±aâ\n",
      "(Spain), reinforcing that the dataset primarily comprises Spanish business news, with a prevalence of technical\n",
      "terms such as âbeneï¬cio netoâ (net proï¬t), âprecio objetivoâ (target price), âproyectoâ (project), and âoperaciÃ³nâ\n",
      "(operation).\n",
      "34\n",
      "\n",
      "ð Doc 117: Figure 2: Histogram of # News Articles per Day and # Words per Article\n",
      "0 5 10 15 20 25 300\n",
      "10\n",
      "20\n",
      "30Frequency\n",
      "HistogramDensity\n",
      "0â¿00\n",
      "0â¿02\n",
      "0â¿04\n",
      "0â¿06\n",
      "0â¿08\n",
      "0â¿10Density\n",
      "(a) Number of News Articles per Day\n",
      "0 250500750100012500\n",
      "200\n",
      "400\n",
      "600Frequency\n",
      "HistogramDensity\n",
      "0â¿000\n",
      "0â¿001\n",
      "0â¿002\n",
      "0â¿003\n",
      "0â¿004\n",
      "Density\n",
      "(b) Number of Words per Article\n",
      "Note: Panel (a) displays the distribution of the number of news articles published per day, with most days having\n",
      "between 5 and 10 articles. Panel (b) shows the distribution of the number of words per article, where the majority\n",
      "are between 70 and 280 words, suggesting concise reporting. However, the long right tail indicates instances of more\n",
      "comprehensive coverage.\n",
      "35\n",
      "\n",
      "ð Doc 118: Figure 3: Time Series of Number of Articles per Day and 30-Period Moving Average\n",
      "Jul2020 Sep Nov Jan2021 Mar May Jul Sep0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30# Articles Published\n",
      "# of Articles Pulished Per DayMA (30)\n",
      "Note: The time series shows the daily number of news articles published, characterized by signiï¬cant variability with\n",
      "occasional sharp spikes. The 30-day moving average smooths these ï¬uctuations, revealing an average publication\n",
      "rate of 5 to 10 articles per day.\n",
      "36\n",
      "\n",
      "ð Doc 119: Figure 4: Imputation of the eï¬ective treatment date ( Ëdi\n",
      "0)\n",
      "Case 1: Treatment date is the same as the publication date; Ëdi\n",
      "0 = di\n",
      "0\n",
      "Case 1a: News article published in a trading date and\n",
      "before the market opens; Ëdi\n",
      "0 â Ëd â§ ti\n",
      "0 < 09:30\n",
      "09:3000:00 17:30 00:00\n",
      "Closing\n",
      "price\n",
      "Case 1b: News article published in a trading date\n",
      "and during market hours; Ëdi\n",
      "0 â Ëd â§ ti\n",
      "0 â [09:30, 17:30]\n",
      "09:3000:00 17:30 00:00\n",
      "Closing\n",
      "price\n",
      "Case 2: Treatment date is the next closest trading day to publication; Ëdi\n",
      "0 = Î(di\n",
      "0)\n",
      "Case 2a: News article published in a trading day but\n",
      "after the market is closed for that day; di\n",
      "0 â Ëd â§ ti\n",
      "0 >\n",
      "17:30\n",
      "09:3000:00 17:30 00:00\n",
      "Closing\n",
      "price\n",
      "09:3000:00 17:30\n",
      "Closing\n",
      "price\n",
      "Case 2b: News article published in a non-trading\n",
      "day; di\n",
      "0 ââ Ëd\n",
      "00:00 09:30 17:30 00:00\n",
      "Closing\n",
      "price\n",
      "00:00\n",
      "Note: This ï¬gure illustrates how we determine the eï¬ective treatment date for news articles based on their publication\n",
      "\n",
      "ð Doc 120: Closing\n",
      "price\n",
      "00:00\n",
      "Note: This ï¬gure illustrates how we determine the eï¬ective treatment date for news articles based on their publication\n",
      "timing relative to market hours. The Spanish stock market operates from 09:30 to 17:30 on trading days. News\n",
      "published during a trading day during or before trading hours aï¬ects stock prices on the same day (Cases 1a and\n",
      "1b), while news published after market close or on non-trading days aï¬ects prices on the next available trading day\n",
      "(Cases 2a and 2b). This temporal mapping ensures we correctly align news publication with the ï¬rst opportunity\n",
      "for market reaction.\n",
      "37\n",
      "\n",
      "ð Doc 121: Figure 5: Average Silhouette Scores in the Training data\n",
      "0 10 20 30 40 50 60 70 80 90 100\n",
      "Cluster Size (k)\n",
      "0â¿050\n",
      "0â¿055\n",
      "0â¿060\n",
      "0â¿065\n",
      "0â¿070\n",
      "0â¿075\n",
      "Average Silhouette Score\n",
      "kÂ§= 26\n",
      "Note: The plot presents the average silhouette scores calculated on the training data Dtr for various cluster sizes\n",
      "k ranging from 2 to 100. The silhouette score measures how well data points ï¬t within their assigned cluster by\n",
      "comparing intra-cluster cohesion with inter-cluster separation. A higher silhouette score (closer to +1) indicates\n",
      "better-deï¬ned clusters. The optimal number of clusters, kâ = 26, which maximizes the average silhouette score, is\n",
      "marked by a vertical dashed green line.\n",
      "38\n",
      "\n",
      "ð Doc 122: Figure 6: Distribution of articles through KMeans clusters\n",
      "(a) All data (D)\n",
      "012345678910111213141516171819202122232425\n",
      "Cluster\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400Number of Articles\n",
      "0â¿00\n",
      "0â¿01\n",
      "0â¿02\n",
      "0â¿03\n",
      "0â¿04\n",
      "0â¿05\n",
      "Density\n",
      "(b) Training data (Dtr)\n",
      "0123456789101112131415161718192021222324250100200\n",
      "0â¿00\n",
      "0â¿01\n",
      "0â¿02\n",
      "0â¿03\n",
      "0â¿04\n",
      "0â¿05\n",
      "0â¿06\n",
      "0â¿07\n",
      "(c) Validation data (Dval)\n",
      "0123456789101112131415161718192021222324250\n",
      "50\n",
      "100\n",
      "0â¿00\n",
      "0â¿01\n",
      "0â¿02\n",
      "0â¿03\n",
      "0â¿04\n",
      "(d) Test data (Dtest)\n",
      "0123456789101112131415161718192021222324250204060\n",
      "0â¿00\n",
      "0â¿01\n",
      "0â¿02\n",
      "0â¿03\n",
      "0â¿04\n",
      "Note: This ï¬gure presents the distribution of articles across the kâ = 26 clusters, where the centroids were determined\n",
      "by applying the KMeans algorithm to the article embeddings from the training data. Panel (a) shows the distribution\n",
      "for the entire dataset (D), while Panels (b), (c), and (d) illustrate the distributions for the training (Dtr), validation\n",
      "\n",
      "ð Doc 123: for the entire dataset (D), while Panels (b), (c), and (d) illustrate the distributions for the training (Dtr), validation\n",
      "(Dval), and test (Dtest) datasets, respectively. The diï¬erences in distribution across splits suggest some temporal\n",
      "instability in the clustering results.\n",
      "39\n",
      "\n",
      "ð Doc 124: Figure 7: Distribution of articles through LLM clusters\n",
      "(a) All data (D)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Cluster\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000Number of Articles\n",
      "0â¿000\n",
      "0â¿025\n",
      "0â¿050\n",
      "0â¿075\n",
      "0â¿100\n",
      "0â¿125\n",
      "0â¿150\n",
      "0â¿175\n",
      "Density\n",
      "(b) Training data (Dtr)\n",
      "012345678910111214161718190200400\n",
      "0â¿000\n",
      "0â¿025\n",
      "0â¿050\n",
      "0â¿075\n",
      "0â¿100\n",
      "0â¿125\n",
      "0â¿150\n",
      "0â¿175\n",
      "(c) Validation data (Dval)\n",
      "01234567891011121314161718190100200300\n",
      "0â¿00\n",
      "0â¿02\n",
      "0â¿04\n",
      "0â¿06\n",
      "0â¿08\n",
      "0â¿10\n",
      "0â¿12\n",
      "0â¿14\n",
      "0â¿16\n",
      "(d) Test data (Dtest)\n",
      "012345678910111214161718190\n",
      "100\n",
      "200\n",
      "0â¿00\n",
      "0â¿02\n",
      "0â¿04\n",
      "0â¿06\n",
      "0â¿08\n",
      "0â¿10\n",
      "0â¿12\n",
      "0â¿14\n",
      "Note: This ï¬gure presents the distribution of news articles across clusters derived using an LLM-based approach.\n",
      "The upper plot shows the distribution for the entire dataset (D), while the lower plots display the distributions for\n",
      "the training (Dtr), validation (Dval), and test (Dtest) datasets. Clusters 8, 9, 10, and 11, which capture ï¬nancial\n",
      "\n",
      "ð Doc 125: the training (Dtr), validation (Dval), and test (Dtest) datasets. Clusters 8, 9, 10, and 11, which capture ï¬nancial\n",
      "events or shocks, dominate the distribution, with cluster 8 (ï¬nancial, minor, positive) representing approximately\n",
      "one-third of the dataset. This cluster includes articles related to ï¬nancial reports with mildly positive outcomes,\n",
      "potentially oï¬ering insight for long trading signals. Unlike KMeans clustering with embeddings, this LLM-based\n",
      "clustering shows stable distributions across data splits, highlighting the robustness of this method over time.\n",
      "40\n",
      "\n",
      "ð Doc 126: 41\n",
      "\n",
      "ð Doc 127: Figure 8: Comparison of Cumulative Gross Returns across Clustering Approaches\n",
      "(a) Panel A: Cumulative Gross Returns of PKMeans\n",
      "Jul2020 Sep Nov Jan2021 Mar May Jul Sep\n",
      "Time (trading days)\n",
      "0â¿95\n",
      "1â¿00\n",
      "1â¿05\n",
      "1â¿10\n",
      "1â¿15\n",
      "1â¿20\n",
      "1â¿25\n",
      "1â¿30\n",
      "Cumulative Returns\n",
      "Train Validation Test\n",
      "GreedyStable\n",
      "(b) Panel B: Cumulative Gross Returns of PLLM\n",
      "Jul2020 Sep Nov Jan2021 Mar May Jul Sep\n",
      "Time (trading days)\n",
      "1â¿000\n",
      "1â¿025\n",
      "1â¿050\n",
      "1â¿075\n",
      "1â¿100\n",
      "1â¿125\n",
      "1â¿150\n",
      "1â¿175\n",
      "Cumulative Returns\n",
      "Train Validation Test\n",
      "GreedyStable\n",
      "Note: This ï¬gure presents the cumulative gross returns of trading strategies based on KMeans clustering (Panel A) and LLM\n",
      "clustering (Panel B) across diï¬erent data splits. For both approaches, the holding period of the beta-neutral strategies is set\n",
      "to L = 4 trading days. The number of traded clusters diï¬ers between approaches: Î¸ = â0.5kâ = 13 for KMeans (kâ = 26\n",
      "clusters) and Î¸ = â0.5kâ = 10 for LLM (k = 20 clusters). The selection criteria for these parameters is based on maximizing\n",
      "\n",
      "ð Doc 128: clusters) and Î¸ = â0.5kâ = 10 for LLM (k = 20 clusters). The selection criteria for these parameters is based on maximizing\n",
      "the Sharpe Ratios of the train and validation samples. The Test split is higlhighted with a yellow background.\n",
      "42\n",
      "\n",
      "ð Doc 129: Figure 9: Sensitivity of SRPtest\n",
      "to the holding window length (L)\n",
      "(a) KMeans: Distribution of SRPtest\n",
      "(L)\n",
      "Â°7â¿5Â°5â¿0Â°2â¿50â¿02â¿55â¿07â¿5Sharpe Ratio (Test)0â¿000â¿050â¿100â¿150â¿200â¿25Density\n",
      "GreedyStable\n",
      "(b) KMeans: Series of SRPtest\n",
      "(L)\n",
      "2 5 7 1012151720Holding period length (L)\n",
      "Â°4Â°2024Sharpe Ratio (Test)\n",
      "GreedyStable\n",
      "(c) LLM: Distribution of SRPtest\n",
      "(L)\n",
      "Â°4 Â°2 0 2 4 6 8Sharpe Ratio (Test)0â¿000â¿050â¿100â¿150â¿200â¿25Density\n",
      "GreedyStable\n",
      "(d) LLM: Series of SRPtest\n",
      "(L)\n",
      "2 5 7 1012151720Holding period length (L)Â°1012345Sharpe Ratio (Test)\n",
      "GreedyStable\n",
      "Note: This ï¬gure examines the sensitivity of the Sharpe Ratios (SRPtest\n",
      ") of the test portfolio to changes in the\n",
      "holding window length (L), with Î¸ ï¬xed at â0.5kâ. Panels (a) and (b) display the distribution and time series\n",
      "of SRPtest\n",
      "(L) for KMeans clustering, respectively, while Panels (c) and (d) present the same for the LLM-based\n",
      "clustering. The left-hand panels show the skewness of the distributions: KMeans clustering results in a left-skewed\n",
      "\n",
      "ð Doc 130: clustering. The left-hand panels show the skewness of the distributions: KMeans clustering results in a left-skewed\n",
      "distribution of Sharpe Ratios, whereas the LLM-based approach yields a right-skewed distribution, indicating higher\n",
      "proï¬tability. The right-hand panels highlight that KMeans clustering only produces positive Sharpe Ratios for very\n",
      "short holding periods, whereas the LLM-based clustering shows more consistent positive performance across a wider\n",
      "range of L values, though with some variability.\n",
      "43\n",
      "\n",
      "ð Doc 131: Figure 10: Sensitivity of SRPtest\n",
      "to the upper bound on the number of traded clusters on each side (Î¸)\n",
      "(a) KMeans: Distribution of SRPtest\n",
      "(Î¸)\n",
      "Â°6 Â°4 Â°2 0 2 4Sharpe Ratio (Test)0â¿00â¿10â¿20â¿30â¿40â¿50â¿60â¿7Density\n",
      "GreedyStable\n",
      "(b) KMeans: Series of SRPtest\n",
      "(Î¸)\n",
      "1234567891011121314Number of traded clusters (Âµ)\n",
      "Â°3Â°2Â°1012Sharpe Ratio (Test)\n",
      "GreedyStable\n",
      "(c) LLM: Distribution of SRPtest\n",
      "(Î¸)\n",
      "Â°2 0 2 4 6Sharpe Ratio (Test)0â¿000â¿250â¿500â¿751â¿001â¿251â¿501â¿752â¿00Density\n",
      "GreedyStable\n",
      "(d) LLM: Series of SRPtest\n",
      "(Î¸)\n",
      "1234567891011121314Number of traded clusters (Âµ)\n",
      "012345Sharpe Ratio (Test)\n",
      "GreedyStable\n",
      "Note: This ï¬gure displays the sensitivity of the Sharpe Ratios (SRPtest\n",
      ") to variations in the upper bound on the\n",
      "number of traded clusters (Î¸), with L ï¬xed at 4. Panels (a) and (b) show the distribution and series of SRPtest\n",
      "(Î¸)\n",
      "for KMeans clustering, respectively, while Panels (c) and (d) illustrate the same for LLM-based clustering. For\n",
      "\n",
      "ð Doc 132: (Î¸)\n",
      "for KMeans clustering, respectively, while Panels (c) and (d) illustrate the same for LLM-based clustering. For\n",
      "KMeans, the results are mixed: the Stable algorithm generates positive Sharpe Ratios for low Î¸ values, whereas the\n",
      "Greedy algorithm performs better with high Î¸ values, indicating sensitivity and instability. In contrast, the LLM-\n",
      "based clustering shows a more consistent pattern, with a concentration of positive Sharpe Ratios across a broader\n",
      "range of Î¸ values, suggesting greater robustness and stability in the trading strategy.\n",
      "44\n",
      "\n",
      "ð Doc 133: A. Appendix\n",
      "A.1 KMeans Algorithm\n",
      "Algorithm 1. KMeans Clustering Algorithm\n",
      "1: Input: Embedding vectors {e1, e2, . . . , eN }, number of clusters k\n",
      "2: Output: Cluster assignments {D1, D2, . . . , Dk}, centroids {c1, c2, . . . , ck}\n",
      "3: Initialize centroids {c1, c2, . . . , ck} randomly\n",
      "4: repeat\n",
      "5: Assignment Step:\n",
      "6: for each vector ei do\n",
      "7: Assign ei to the nearest centroid:\n",
      "g = arg minââ{1,...,k}\n",
      "ó°ei â câó°2\n",
      "2\n",
      "8: Update cluster assignments: Dg â Dg âª {i}\n",
      "9: end for\n",
      "10: Update Step:\n",
      "11: for each cluster Dg do\n",
      "12: Recalculate centroid cg:\n",
      "cg = 1\n",
      "|Dg|\n",
      "ó°\n",
      "iâDg\n",
      "ei\n",
      "13: end for\n",
      "14: until cluster assignments no longer change\n",
      "15: Return cluster assignments {D1, D2, . . . , Dk} and centroids {c1, c2, . . . , ck}\n",
      "45\n",
      "\n",
      "ð Doc 134: A.2 Hyperparameter Choice Justiï¬cation\n",
      "Our hyperparameters are L and Î¸. Recall that L denotes the number of trading days over which we\n",
      "hold the positions in the beta-neutral strategy, while Î¸ represents the upper bound on each side (long and\n",
      "short) for the amount of clusters we select for the trading strategy. The speciï¬c choice of hyperparameters\n",
      "we made for the results presented in the paper were:\n",
      "L = 4\n",
      "Î¸ = â0.5kâ\n",
      "where k represents the number of clusters (26 for KMeans clustering, and 20 for LLM clustering). This\n",
      "choice is not arbitrary nor opportunistic. Instead, it results from the maximization of the Sharpe Ratio\n",
      "of the portfolio in the train and validation samples for both KMeans and LLM clustering. This choice\n",
      "procedure is completely based on in-sample criteria and it prevents lookahead bias. The justiï¬cation for\n",
      "such choices is made below.\n",
      "A.2.1 KMeans Clustering\n",
      "In Figure A1 we can see that a choice of L = 4 in the training and validation splits generates the most\n",
      "\n",
      "ð Doc 135: A.2.1 KMeans Clustering\n",
      "In Figure A1 we can see that a choice of L = 4 in the training and validation splits generates the most\n",
      "stable Sharpe Ratio. Namely, In the train set (Figure A1a), it makes more sense to choose low values of\n",
      "L (less than 4) to maximize the SR. However, in the validation set (Figure A1b), it makes more sense\n",
      "to choose higher values of L. The choice of L = 4 represents a balanced compromise, providing a stable\n",
      "Sharpe Ratio proï¬le across both splits, ensuring consistent in-sample performance.\n",
      "[Insert Figure A1 about here]\n",
      "On the other hand, the choice of Î¸ = â0.5 Â· 26â = 13 is a choice that pursues stability in the Sharpe\n",
      "Ratio of the train and validation portfolios. As we can see from Figure A2, the Sharpe Ratios tend to\n",
      "converge to the highest and most stable value when we choose the highest possible value of Î¸.\n",
      "[Insert Figure A2 about here]\n",
      "A.2.2 LLM Clustering\n",
      "Following a similar logic as below, the choice of L = 4 sets a consensus between the maximization of\n",
      "\n",
      "ð Doc 136: A.2.2 LLM Clustering\n",
      "Following a similar logic as below, the choice of L = 4 sets a consensus between the maximization of\n",
      "SRPtr\n",
      "and SRPval\n",
      ". That is, maximizing SRPtr\n",
      "requires lower holding period lengths (the maximizer is\n",
      "L = 4), while maximizing SRPval\n",
      "requires increasing the window length. Among this contradiction, from\n",
      "Figure A3 it follows that L = 4 stands as a perfect choice to balance the maximization requirements in\n",
      "both samples, generating a stable choice for the holding period window length.\n",
      "[Insert Figure A3 about here]\n",
      "46\n",
      "\n",
      "ð Doc 137: Finally, the same conclusion as in KMeans applies here. By selecting Î¸ = â0.5 Â· 20â = 10, we get\n",
      "a stable Sharpe Ratio. Even though we observe that SRPtr\n",
      "(L) falls momentarily at Î¸ = 10 for the\n",
      "Greedy algorithm, it still constitutes a good choice. Conversely, at Î¸ = 10 the greedy algorithm sees a\n",
      "jump in SRPval\n",
      "(L) (see Figure A4). All in all, we can easily conclude that Î¸ = â0.5kâ arises as a good\n",
      "hyperpamrameter choice also for LLM clustering.\n",
      "[Insert Figure A4 about here]\n",
      "47\n",
      "\n",
      "ð Doc 138: A.3 Cluster-Average Sharpe Ratios\n",
      "The distribution of cluster-average Sharpe Ratios across diï¬erent clusters reveals distinct patterns\n",
      "between KMeans and LLM-based clustering approaches, as illustrated in Figure A5\n",
      "Panel A presents the results for KMeans clustering, where we observe remarkably consistent distribu-\n",
      "tional patterns across all three data splits. The distributions are approximately symmetric around zero,\n",
      "with the majority of Sharpe ratios falling within the [â5, 5] range. The training set exhibits the high-\n",
      "est density peak (approximately 0.17), followed closely by the test set, while the validation set shows a\n",
      "slightly lower peak density of about 0.125. Notable in the validation set are small secondary peaks at the\n",
      "tails (around Â±15), suggesting the presence of a few clusters with extreme performance characteristics.\n",
      "This consistency across splits suggests that the KMeans clustering approach produces stable performance\n",
      "groupings.\n",
      "\n",
      "ð Doc 139: This consistency across splits suggests that the KMeans clustering approach produces stable performance\n",
      "groupings.\n",
      "Panel B displays the results for LLM-based clustering, revealing more heterogeneous distributions\n",
      "across the splits. The validation set demonstrates a pronounced peak near zero with a maximum density\n",
      "of 0.2, indicating strong concentration of performance in this region. In contrast, the training set exhibits\n",
      "a markedly diï¬erent pattern, with a ï¬atter, more dispersed distribution extending from â20 to +20,\n",
      "suggesting greater performance variability across clusters. The test set presents an intermediate case,\n",
      "with moderate concentration around zero but maintaining signiï¬cant mass in the positive region. This\n",
      "heterogeneity across splits might indicate that the LLM-based clustering captures more nuanced and\n",
      "potentially time-varying patterns in the underlying data.\n",
      "The contrasting patterns between the two clustering approaches suggest diï¬erent strengths: KMeans\n",
      "\n",
      "ð Doc 140: The contrasting patterns between the two clustering approaches suggest diï¬erent strengths: KMeans\n",
      "provides more stable and consistent performance groupings, while LLM-based clustering potentially cap-\n",
      "tures more complex relationships, albeit with greater variability across diï¬erent data splits.\n",
      "[Insert Figure A5 about here]\n",
      "48\n",
      "\n",
      "ð Doc 141: A.4 Optimal Cluster Selection Algorithms\n",
      "Algorithm 2. Greedy Selection | Top average Sharpe Ratio in Validation Set\n",
      "1: Input: Set of clusters G = {1, 2, . . . , kâ}, Sharpe Ratios in the validation sample {SR(i,j)\n",
      "L }(i,j)âBval ,\n",
      "maximum number of traded clusters Î¸ â N (usually, Î¸ â kâ)\n",
      "2: Output: Set of long-traded clusters G+\n",
      "Î¸ and set of short-traded clusters Gâ\n",
      "Î¸\n",
      "Step #1: Compute Cluster Average Sharpe Ratios in Validation Set\n",
      "3: for each g â G do\n",
      "4: Compute average Sharpe Ratio SRval\n",
      "g â 1\n",
      "|Bvalg |\n",
      "ó°\n",
      "(i,j)âBvalg SR(i,j)\n",
      "L\n",
      "5: end for\n",
      "Step #2: Identify Positive and Negative Sharpe Ratio Clusters\n",
      "6: Deï¬ne Gval\n",
      "SR+ â {g â G | SRval\n",
      "g > 0}\n",
      "7: Deï¬ne Gval\n",
      "SRâ â {g â G | SRval\n",
      "g < 0}\n",
      "Step #3: Rank Clusters by Average Sharpe Ratio in the Validation Set\n",
      "8: for each g â G do\n",
      "9: Rank the average Sharpe Ratio Rval\n",
      "g â ó°\n",
      "hâG 1\n",
      "ó°\n",
      "SRval\n",
      "h â¥ SRval\n",
      "g\n",
      "ó°\n",
      "10: end for\n",
      "Step #4: Select Top Î¸ Clusters\n",
      "11: Deï¬ne Î¸+ â min(Î¸, |Gval\n",
      "SR+ |) ; G+\n",
      "Î¸ â {g â G | 1 â¤ Rval\n",
      "g â¤ Î¸+}\n",
      "12: Deï¬ne Î¸â â min(Î¸, |Gval\n",
      "\n",
      "ð Doc 142: g\n",
      "ó°\n",
      "10: end for\n",
      "Step #4: Select Top Î¸ Clusters\n",
      "11: Deï¬ne Î¸+ â min(Î¸, |Gval\n",
      "SR+ |) ; G+\n",
      "Î¸ â {g â G | 1 â¤ Rval\n",
      "g â¤ Î¸+}\n",
      "12: Deï¬ne Î¸â â min(Î¸, |Gval\n",
      "SRâ|) ; Gâ\n",
      "Î¸ â {g â G | kâ â Î¸â < Rval\n",
      "g â¤ kâ}\n",
      "13: Return Long-traded clusters G+\n",
      "Î¸ , Short-traded clusters Gâ\n",
      "Î¸\n",
      "49\n",
      "\n",
      "ð Doc 143: Algorithm 3. Rank Stability | Minimal Rank Diï¬erence between Train & Validation Sets\n",
      "1: Input: Set of clusters G = {1, 2, . . . , kâ}, Sharpe Ratios in the training and validation sample\n",
      "{SR(i,j)\n",
      "L }(i,j)âBtr and {SR(i,j)\n",
      "L }(i,j)âBval , maximum number of traded clusters Î¸\n",
      "2: Output: Set of long-traded clusters G+\n",
      "Î¸ and set of short-traded clusters Gâ\n",
      "Î¸\n",
      "Step #1: Compute Cluster Average Sharpe Ratios in Training & Validation Set\n",
      "3: for each g â G do\n",
      "4: Compute average Sharpe Ratio in Btr : SRtr\n",
      "g â 1\n",
      "|Btrg |\n",
      "ó°\n",
      "(i,j)âBtrg SR(i,j)\n",
      "L\n",
      "5: Compute average Sharpe Ratio in Bval : SRval\n",
      "g â 1\n",
      "|Bvalg |\n",
      "ó°\n",
      "(i,j)âBvalg SR(i,j)\n",
      "L\n",
      "6: end for\n",
      "Step #2: Rank Clusters\n",
      "7: for each g â G do\n",
      "8: Rank the average Sharpe Ratios in Btr : Rtr\n",
      "g â ó°\n",
      "hâG 1\n",
      "ó°\n",
      "SRtr\n",
      "h â¥ SRtr\n",
      "g\n",
      "ó°\n",
      "9: Rank the average Sharpe Ratios in Bval : Rval\n",
      "g â ó°\n",
      "hâG 1\n",
      "ó°\n",
      "SRval\n",
      "h â¥ SRval\n",
      "g\n",
      "ó°\n",
      "10: end for\n",
      "Step #3: Calculate Rank Diï¬erences\n",
      "11: for each g â G do\n",
      "12: Calculate rank diï¬erence Î´g â |Rtr\n",
      "g â Rval\n",
      "g |\n",
      "13: end for\n",
      "\n",
      "ð Doc 144: h â¥ SRval\n",
      "g\n",
      "ó°\n",
      "10: end for\n",
      "Step #3: Calculate Rank Diï¬erences\n",
      "11: for each g â G do\n",
      "12: Calculate rank diï¬erence Î´g â |Rtr\n",
      "g â Rval\n",
      "g |\n",
      "13: end for\n",
      "Step #4: Select Top Î¸ Clusters with Smallest Rank Diï¬erences\n",
      "14: for each g â G do\n",
      "15: Rank the rank diï¬erence : R(Î´g) â ó°\n",
      "hâG 1 (Î´g â¥ Î´h)\n",
      "16: end for\n",
      "17: Select top 2Î¸ clusters with smallest Î´g: GÎ¸ = {g â G | 1 â¤ R(Î´g) â¤ 2Î¸}\n",
      "# Step 5: Determine Long and Short Positions\n",
      "18: Deï¬ne G+\n",
      "Î¸ = {g â GÎ¸ | SRtr\n",
      "g > 0 and SRval\n",
      "g > 0}\n",
      "19: Deï¬ne Gâ\n",
      "Î¸ = {g â GÎ¸ | SRtr\n",
      "g < 0 and SRval\n",
      "g < 0}\n",
      "20: Return Long-traded clusters G+\n",
      "Î¸ , Short-traded clusters Gâ\n",
      "Î¸\n",
      "A.5 Sample of articles for each cluster\n",
      "50\n",
      "\n",
      "ð Doc 145: Table A1: KMeans clustering. Proposed name for the clusters and sample of 3 articles for each cluster.\n",
      "# Title Articles\n",
      "0 Miscellaneous (Colonial, Acciona, Amadeus, Grifols,\n",
      "Endesa, IAG, Bankinter...)\n",
      "â¢ Colonial forecasts rental income of EUR338m in 2020\n",
      "â¢ Accionaâs asset sales will allow it to grow in renewables\n",
      "â¢ Sabadell recommends selling Amadeus shares due to worse sales forecast.\n",
      "1 Quarterly & Semi-Annual Earnings Reports\n",
      "â¢ EnagÃ¡ s 1H net proï¬t falls 9.8% due to lower income and extraordinary items.\n",
      "â¢ Iberdrola: Net proï¬t of EUR1.025m in Q1\n",
      "â¢ Santander almost quintuples Q1 proï¬t due to absence of Covid provisions.\n",
      "2 BBVA & Sabadell: Financial Performance & Strategic\n",
      "Movements\n",
      "â¢ Interest rate hike in Turkey favors BBVAâs net interest margin\n",
      "â¢ Sabadell reorganizes business in Spain following the arrival of the new CEO.\n",
      "â¢ Fitch downgrades Banco Sabadellâs rating one notch to low grade.\n",
      "3 TelefÃ³ nica & Cellnex: Telecommunications Tower Sales\n",
      "& Market Dynamics\n",
      "\n",
      "ð Doc 146: â¢ Fitch downgrades Banco Sabadellâs rating one notch to low grade.\n",
      "3 TelefÃ³ nica & Cellnex: Telecommunications Tower Sales\n",
      "& Market Dynamics\n",
      "â¢ TelefÃ³ nica shares soar after selling towers of its subsidiary in Europe and Latin America.\n",
      "â¢ TelefÃ³ nica hires Goldman Sachs to sell its British tower business\n",
      "â¢ Dutch Competition Authority authorizes Cellnex to integrate 3,150 Deutsche Telekom towers.\n",
      "4 CaixaBank: Mergers and Strategic Moves in the\n",
      "Banking Sector\n",
      "â¢ CaixaBank and Bankia approve their merger project\n",
      "â¢ CaixaBank closes its ï¬rst issuance of green bonds in pounds for 500 million\n",
      "â¢ CaixaBank-Bankia merger could generate EUR500m in savings\n",
      "5 TelefÃ³ nica, Indra, & MÃ¡ sMÃ³ vil: Regulatory and\n",
      "Strategic Moves in Telecom\n",
      "â¢ Indra to partner with TelefÃ³ nica in the deployment of ï¬ber optics in Germany.\n",
      "â¢ TelefÃ³ nica launches a buyback oï¬er for its hybrid bonds of EUR1.000m.\n",
      "â¢ EU refers Liberty Global and TelefÃ³ nica agreement to UK regulator\n",
      "6 Siemens Gamesa: Supply Agreements, Proï¬tability\n",
      "\n",
      "ð Doc 147: â¢ EU refers Liberty Global and TelefÃ³ nica agreement to UK regulator\n",
      "6 Siemens Gamesa: Supply Agreements, Proï¬tability\n",
      "Targets in Renewable Energy\n",
      "â¢ Siemens Gamesa will supply turbines to Elawanâs 150 MW wind farm in Spain.\n",
      "â¢ Siemens Gamesa lowers its proï¬tability target for 2021.\n",
      "â¢ Siemens Gamesa will supply 160 MW for the largest wind farm in the Philippines.\n",
      "7 Cellnex: Strategic Acquisitions and Financial Moves in\n",
      "Telecom Infrastructure\n",
      "â¢ Cellnex launches a EUR1.850m debt issue\n",
      "â¢ Cellnex agrees to buy 10,500 telecommunications towers in France for EUR5.200m\n",
      "â¢ Benetton family sells 2.5% of Cellnex to Singapore sovereign fund\n",
      "8 Acciona, Endesa, EnagÃ¡ s & Naturgy: Strategic Moves\n",
      "& Regulatory Developments in the Energy Sector\n",
      "â¢ Naturgy and EnagÃ¡ s study project to produce green hydrogen in Asturias\n",
      "â¢ Break of ties between Algeria and Morocco may damage gas ï¬ow to Spain\n",
      "â¢ Acciona: Energy business IPO on track for 1H\n",
      "9 Repsol: Strategic Moves and Challenges in the Energy\n",
      "Sector\n",
      "\n",
      "ð Doc 148: â¢ Acciona: Energy business IPO on track for 1H\n",
      "9 Repsol: Strategic Moves and Challenges in the Energy\n",
      "Sector\n",
      "â¢ Repsol to produce green hydrogen at Petronor reï¬nery in 2022\n",
      "â¢ Repsol and Talgo to jointly promote the creation of renewable hydrogen trains\n",
      "â¢ Repsol gains access to a portfolio of renewable assets in Chile through a joint venture\n",
      "10 Ferrovial, Acciona: Strategic Expansions and Financial\n",
      "Maneuvers in Infrastructure\n",
      "â¢ Ferrovial closes the sale of Broadspectrum to Ventia for EUR291m\n",
      "â¢ Acciona awarded the construction of 2 roads in Poland for EUR642m\n",
      "â¢ Renfe awards on-board services contract to Ferrovial for EUR272m\n",
      "11 Solaria: Strategic Moves and Market Challenges in\n",
      "Renewable Energy\n",
      "â¢ Solaria invests EUR220m in Europeâs largest photovoltaic park.\n",
      "â¢ Solaria will supply energy to Shell and Axpo with Europeâs largest photovoltaic plant\n",
      "â¢ Goldman Sachs downgrades Solaria recommendation after stock rise.\n",
      "12 Iberdrola: Strategic Collaborations and Renewable\n",
      "Energy Developments\n",
      "\n",
      "ð Doc 149: â¢ Goldman Sachs downgrades Solaria recommendation after stock rise.\n",
      "12 Iberdrola: Strategic Collaborations and Renewable\n",
      "Energy Developments\n",
      "â¢ Iberdrola will build a self-consumption plant for Lactalis factory in Spain.\n",
      "â¢ Iberdrola and Mapfre launch a renewable energy co-investment vehicle in Spain.\n",
      "â¢ Iberdrola partners with Mitsubishi to decarbonize the industry.\n",
      "51\n",
      "\n",
      "ð Doc 150: 13 IAG: Financial Performance\n",
      "â¢ IAG Q3 results worse than expected\n",
      "â¢ IAG burns cash faster than anticipated\n",
      "â¢ IAG stock may be pricing in a second capital increase\n",
      "14 Santander & CaixaBank: Financial Moves and\n",
      "Sustainability Initiatives\n",
      "â¢ CaixaBank mobilizes EUR12.000m in sustainable ï¬nancing in the ï¬rst 9 months of 2020.\n",
      "â¢ EIB and Banco Santander will inject EUR587m into Portuguese SMEs.\n",
      "â¢ Banco Santander, leader in renewable project ï¬nancing in 2020.\n",
      "15 ACS & Acciona: Strategic Movements and\n",
      "Infrastructure Projects\n",
      "â¢ ACS and Acciona win contracts for new Australian airport worth EUR164m.\n",
      "â¢ Acciona awarded 3 contracts to operate wastewater treatment plants in Sardinia for EUR210m.\n",
      "â¢ ACS expects net proï¬t to grow by around 30% in 2021\n",
      "16 TelefÃ³ nica: Financial Performance and Strategic Moves\n",
      "â¢ Reduction in TelefÃ³ nicaâs debt will improve analystsâperception\n",
      "â¢ TelefÃ³ nicaâs proï¬t more than doubles in Q1 due to lower ï¬nancial expenses.\n",
      "\n",
      "ð Doc 151: â¢ Reduction in TelefÃ³ nicaâs debt will improve analystsâperception\n",
      "â¢ TelefÃ³ nicaâs proï¬t more than doubles in Q1 due to lower ï¬nancial expenses.\n",
      "â¢ TelefÃ³ nica, AmÃ© rica MÃ³ vil and TIM buy the mobile network of Brazilâs Oi.\n",
      "17 MeliÃ¡ and Spanish Tourism Sector: Challenges Amidst\n",
      "the Pandemic\n",
      "â¢ MeliÃ¡ : Spanish hotel sector faces another uncertain summer with cautious optimism.\n",
      "â¢ MeliÃ¡ claims EUR116m from the Spanish government for pandemic-related damages.\n",
      "â¢ MeliÃ¡ : Local Covid-19 lockdowns will continue to aï¬ect MeliÃ¡ .\n",
      "18 Takeover Bids for Naturgy and MÃ¡ sMÃ³ vil\n",
      "â¢ Australian fund IFM launches EUR5.000m bid for 22.69% of Naturgy.\n",
      "â¢ Polygon fund asks CNMV to review and alter the bid for MÃ¡ sMÃ³ vil.\n",
      "â¢ IFM accepts Spanish government conditions in partial bid for Naturgy.\n",
      "19 Naturgy: Financial Performance\n",
      "â¢ Naturgy presents \"weak\" 2020 results\n",
      "â¢ Naturgy may revise its strategic plan upwards due to gas prices.\n",
      "â¢ Bank of America sees upside potential for Naturgy based on fundamentals.\n",
      "\n",
      "ð Doc 152: â¢ Naturgy may revise its strategic plan upwards due to gas prices.\n",
      "â¢ Bank of America sees upside potential for Naturgy based on fundamentals.\n",
      "20 PharmaMar, Grifols: Regulatory Approvals and Market\n",
      "Moves in the Pharmaceutical Sector\n",
      "â¢ EU court annuls European Commissionâs refusal to market PharmaMar drug.\n",
      "â¢ Grifols starts issuing EUR2.000m bonds to buy Biotest.\n",
      "â¢ PharmaMar announces approval of lurbinectedin for lung cancer in Australia.\n",
      "21 Repsol: Financial Performance\n",
      "â¢ Repsol: Net loss of EUR3.289m in 2020.\n",
      "â¢ Repsol reports a loss of EUR711m in Q4 due to exploration and production provisions\n",
      "â¢ Repsol posts a loss of EUR94m in Q3 due to provisions and lower reï¬ning margins.\n",
      "22 Aena: Financial Performance\n",
      "â¢ JPMorgan raises Aenaâs target price to EUR155 from EUR135.\n",
      "â¢ Aena risks a revenue cut of up to EUR2.000m due to rents.\n",
      "â¢ Aena loses EUR170.7m in 1H as passenger traï¬c plummets due to the pandemic.\n",
      "23 EnagÃ¡ s, Endesa, Iberdrola, Red ElÃ© ctrica: Regulatory\n",
      "\n",
      "ð Doc 153: â¢ Aena loses EUR170.7m in 1H as passenger traï¬c plummets due to the pandemic.\n",
      "23 EnagÃ¡ s, Endesa, Iberdrola, Red ElÃ© ctrica: Regulatory\n",
      "and Market Challenges in the Energy Sector\n",
      "â¢ Spanish electric utilities will remain under pressure in the stock market\n",
      "â¢ Spanish government measures are bad news for the electric sector.\n",
      "â¢ Spainâs electricity price closes February with a 52% drop vs January\n",
      "24 BBVA, CaixaBank, Banco Sabadell: Layoï¬s and\n",
      "Restructuring\n",
      "â¢ CaixaBank proposes to unions a redundancy plan aï¬ecting 8,291 employees.\n",
      "â¢ Banco Santander closes its redundancy plan with 3,572 voluntary exits and 19 dismissals\n",
      "â¢ Sabadell prepares an adjustment plan aï¬ecting 2,000 employees\n",
      "25 Inditex, Acerinox: Market Performance and Strategic\n",
      "Developments in the Post-Covid Context\n",
      "â¢ Inditex reopens 94% of its stores worldwide after Covid-19 pandemic.\n",
      "â¢ Sale of Nippon Steel in Acerinox is negative, but expected.\n",
      "â¢ Inditex stock already prices in a strong business recovery.\n",
      "52\n",
      "\n",
      "ð Doc 154: Table A2: LLM clustering. Sample of 3 articles for each cluster.\n",
      "# Title Articles\n",
      "0 Demand, Minor, Positive\n",
      "â¢ MeliÃ¡ âs recovery will be fast, but it will not be completed until 2023\n",
      "â¢ Tourism sector aid in Spain will have a limited impact on listed companies\n",
      "â¢ Spanish airports will recover pre-pandemic traï¬c by the end of 2025\n",
      "1 Demand, Minor, Negative\n",
      "â¢ Tallgrass will contribute fewer dividends to EnagÃ¡ s -JPMorgan Cazenove\n",
      "â¢ Aenaâs stock decline is due to sector visibility -Sabadell\n",
      "â¢ ObservaTUR believes Spainâs economic situation will worsen and calls for more measures\n",
      "2 Demand, Major, Positive\n",
      "â¢ Solaria invests EUR220m in Europeâs largest photovoltaic park\n",
      "â¢ Acciona will build Sao Paulo metro line for EUR2.3 billion\n",
      "â¢ Inditex returns to proï¬t in H1 and continues to recover from the pandemic\n",
      "3 Demand, Major, Negative\n",
      "â¢ Passenger traï¬c at Aena airports falls 79.9% year-on-year in September\n",
      "â¢ UPDATE: Naturgyâs net proï¬t falls 45.6% in 9m due to Covid-19 impact\n",
      "\n",
      "ð Doc 155: â¢ Passenger traï¬c at Aena airports falls 79.9% year-on-year in September\n",
      "â¢ UPDATE: Naturgyâs net proï¬t falls 45.6% in 9m due to Covid-19 impact\n",
      "â¢ Possible capital increase by IAG already priced in\n",
      "4 Supply, Minor, Positive\n",
      "â¢ Repsol returns to proï¬t in Q2 due to crude price increase\n",
      "â¢ Naturgy receives LNG supply contract for ships for 2 years in Spain\n",
      "â¢ Acciona EnergÃ­ a starts up 238 MW photovoltaic complex in Chile\n",
      "5 Supply, Minor, Negative\n",
      "â¢ EnagÃ¡ s operating results worse than expected -Bankinter\n",
      "â¢ IFM rules out extending acceptance period for Naturgy takeover bid and changing\n",
      "conditions\n",
      "â¢ Changes in Siemens Gamesaâs onshore wind business will take time\n",
      "6 Supply, Major, Positive\n",
      "â¢ Capital Energy wins renewable auction in Spain\n",
      "â¢ Repsol expects to start exploiting its huge gas reserve in Brazil in 2026\n",
      "â¢ Repsol will invest EUR657m to expand its industrial complex in Sines, Portugal\n",
      "7 Supply, Major, Negative\n",
      "â¢ Iberdrola halts $1.2 billion investment in Mexico\n",
      "\n",
      "ð Doc 156: 7 Supply, Major, Negative\n",
      "â¢ Iberdrola halts $1.2 billion investment in Mexico\n",
      "â¢ 85% of Acciona workers at Nissan agree to contract termination\n",
      "â¢ CaixaBank reduces workforce adjustment by 500 employees to 7,791 -Source\n",
      "8 Financial, Minor, Positive\n",
      "â¢ Norwegian fund Norges Bank takes 1.14% stake in Naturgy amid IFM takeover bid\n",
      "â¢ Sabadell closes green bond issue for EUR500m -Source\n",
      "â¢ CaixaBank-Bankia merger goals are credible -Deutsche Bank\n",
      "9 Financial, Minor, Negative\n",
      "â¢ UPDATE2: Bankiaâs proï¬t falls 57.6% in 2020 due to provisions for pandemic impact\n",
      "â¢ Iberdrola bond spreads will not be aï¬ected by GalÃ¡ nâs indictment for now\n",
      "â¢ Court maintains precautionary suspension of rent payments to Aena\n",
      "10 Financial, Major, Positive\n",
      "â¢ UPDATE: Endesaâs net proï¬t soars in 2020 due to lower impairment charges\n",
      "â¢ TelefÃ³ nica will reduce debt by EUR5bn after closing Virgin Media and O2 merger\n",
      "â¢ Fluidra buys US company S.R. Smith for $240m\n",
      "53\n",
      "\n",
      "ð Doc 157: 11 Financial, Major, Negative\n",
      "â¢ UPDATE3: Banco Santander reports EUR8.771bn loss in 2020 due to Covid charges\n",
      "â¢ Bankinter downgrades Grifols recommendation to neutral from buy\n",
      "â¢ BBVA reduces layoï¬s to 3,361 and proposes early retirement at 52 with 65% salary\n",
      "12 Technology, Minor, Positive\n",
      "â¢ Siemens Gamesa to supply turbines for 298MW wind farm in the US\n",
      "â¢ Repsol and TÃ Ä¾cnicas Reunidas team up to develop decarbonization technologies\n",
      "â¢ European Commission funds Repsol and EnagÃ¡ s renewable hydrogen project\n",
      "13 Technology, Minor, Negative â¢ Cellnex and REE apply for EU funds to develop rural mobile networks\n",
      "14 Technology, Major, Positive\n",
      "â¢ TelefÃ³ nica and Allianz partner to deploy ï¬ber in Germany\n",
      "â¢ Iberdrola partners with Cosmo to develop 600 MW of oï¬shore wind in Japan\n",
      "â¢ TelefÃ³ nica estimates 5G network will require over EUR6bn in Spain\n",
      "15 Technology, Major, Negative\n",
      "16 Policy, Minor, Positive\n",
      "â¢ EnagÃ¡ s promotes 34 hydrogen and 21 biomethane proposals to recovery funds\n",
      "\n",
      "ð Doc 158: 15 Technology, Major, Negative\n",
      "16 Policy, Minor, Positive\n",
      "â¢ EnagÃ¡ s promotes 34 hydrogen and 21 biomethane proposals to recovery funds\n",
      "â¢ Iberdrola president sees need to reform taxation to make renewables competitive\n",
      "â¢ New electricity tariï¬ in Spain aims to change consumer habits -Experts\n",
      "17 Policy, Minor, Negative\n",
      "â¢ Spanish government measures hurt Iberdrola -IG\n",
      "â¢ Spanish government plans law to reduce CO2 price impact on electricity bills -Source\n",
      "â¢ Iberdrola CEO criticizes electricity reform in Spain for \"unexpected charges\"\n",
      "18 Policy, Major, Positive\n",
      "â¢ Endesa is Spainâs future green leader, but trades at a discount\n",
      "â¢ TCI fund supports ACSâs interest in ASPI and will reject Italyâs oï¬er\n",
      "â¢ Cellnex acquisition in France reassures investors -Berenberg\n",
      "19 Policy, Major, Negative\n",
      "â¢ Sabadell does not expect improvement in partial takeover bid price for Naturgy\n",
      "â¢ Renta 4 downgrades Naturgy to underweight after government measures\n",
      "â¢ Bankinter warns of uncertainties over Iberdrola stock\n",
      "54\n",
      "\n",
      "ð Doc 159: A.6 Function Calling with Llama-3\n",
      "Algorithm 4. Function Calling Workï¬ow for Llama-3\n",
      "Require: D: Dataset of news articles\n",
      "Ensure: Structured JSON output for each article\n",
      "1: Initialize Llama-3 model via GroqCloud API\n",
      "2: for each article i â D do â² Iterate over each article in the dataset\n",
      "3: Message: System â² Deï¬ne the role and task for the LLM\n",
      "âYou are a function calling LLM that analyzes business news in Spanish. For every article,\n",
      "identify the ï¬rms that are directly aï¬ected by the news and classify the shocks in type,\n",
      "magnitude and directionâ\n",
      "4: Message: User â² User provides the article text as input\n",
      "Content: prompt Pi containing the text of article i\n",
      "5: Tool: news_parser â² Deï¬ne the news_parser function\n",
      "Parameters: {firms: array of objects}, where each object contains:\n",
      "â¢ firm: string (âeach one ï¬rm in firms â)\n",
      "â¢ ticker: string (âstock market tickerâ)\n",
      "â¢ shock_type: enum {demand, supply, ï¬nancial, policy, technology}\n",
      "â¢ shock_magnitude: enum {minor, major}\n",
      "\n",
      "ð Doc 160: â¢ ticker: string (âstock market tickerâ)\n",
      "â¢ shock_type: enum {demand, supply, ï¬nancial, policy, technology}\n",
      "â¢ shock_magnitude: enum {minor, major}\n",
      "â¢ shock_direction: enum {positive, negative}\n",
      "6: Send initial messages and tools to Llama-3 â² Initiate interaction with the LLM\n",
      "7: Retrieve response from Llama-3 â² Get the initial response from the LLM\n",
      "8: if Function call is requested by Llama-3 then â² Check if the LLM needs to call a function\n",
      "9: Execute news_parser function with provided arguments â² Run the function\n",
      "10: Append function response to the conversation â² Include function output in the dialogue\n",
      "11: Send updated messages to Llama-3 â² Continue the conversation with new information\n",
      "12: Retrieve ï¬nal response from Llama-3 â² Get the ï¬nal output from the LLM\n",
      "13: end if\n",
      "14: Extract and store structured JSON output â² Save the processed data\n",
      "15: end for\n",
      "A.7 Why not using a diï¬erent benchmark?\n",
      "In evaluating our novel Large Language Model (LLM) methodology for classifying news-implied ï¬rm-\n",
      "\n",
      "ð Doc 161: 15: end for\n",
      "A.7 Why not using a diï¬erent benchmark?\n",
      "In evaluating our novel Large Language Model (LLM) methodology for classifying news-implied ï¬rm-\n",
      "speciï¬c shocks, it is imperative to establish a robust and relevant benchmark. Our chosen benchmark\n",
      "involves transforming news articles into high-dimensional vector embeddings followed by clustering these\n",
      "55\n",
      "\n",
      "ð Doc 162: embeddings using the KMeans algorithm. This section delineates the rationale behind selecting KMeans\n",
      "clustering of vector embeddings over other potential benchmarks such as sentiment analysis and topic\n",
      "modeling.\n",
      "Why not Sentiment Analysis as a benchmark?\n",
      "Sentiment analysis is a widely recognized technique in natural language processing that aims to de-\n",
      "termine the emotional tone conveyed in a text, typically categorizing content as positive, negative, or\n",
      "neutral. While sentiment analysis provides a straightforward approach to gauging the general tone of\n",
      "news articles, it falls short in several critical aspects when juxtaposed with our objectives.\n",
      "First, sentiment analysis is not suï¬ciently granular. Our LLM methodology classiï¬es news articles into\n",
      "20 distinct categories of economic shocks while sentiment analysis classiï¬es articles in a coarse manner,\n",
      "typically into positive, negative, or neutral categories, which is inadequate for benchmarking a detailed\n",
      "classiï¬cation model.\n",
      "\n",
      "ð Doc 163: typically into positive, negative, or neutral categories, which is inadequate for benchmarking a detailed\n",
      "classiï¬cation model.\n",
      "Second, sentiment analysis predominantly focuses on the linguistic and emotional aspects of the text,\n",
      "which do not necessarily correlate with the economic impact on ï¬rms. For instance, a neutral-toned\n",
      "article could describe a signiï¬cant economic event, while a positive sentiment might not always translate\n",
      "to favorable economic outcomes. Consequently, the sentiment does not provide direct insights into the\n",
      "economic consequences, making it an economically irrelevant benchmark for our purposes.\n",
      "Third, sentiment analysis algorithms are often sensitive to linguistic subtleties, leading to inconsistent\n",
      "results across diï¬erent languages and contexts. For example, sarcasm or idiomatic expressions can distort\n",
      "sentiment scores, undermining the reliability of sentiment analysis as a benchmark. This variability poses\n",
      "\n",
      "ð Doc 164: sentiment scores, undermining the reliability of sentiment analysis as a benchmark. This variability poses\n",
      "a challenge for standardization, especially in a multilingual context. For instance, the sentiment derived\n",
      "from analyzing the text in English may signiï¬cantly diï¬er from the sentiment in Spanish.\n",
      "Fourth, sentiment analysis is not robust in the sense that diï¬erent sentiment analysis tools yield\n",
      "divergent assessments of the same text. As shown below, we observe considerable diï¬erences in the\n",
      "identiï¬ed sentiment when applying multiple sentiment analysis providers to a speciï¬c article. This lack\n",
      "of consistency undermines the reliability of sentiment analysis as a benchmark, making it unsuitable for\n",
      "our purposes.\n",
      "Sentiment analysis is highly sensitive to the speciï¬c tool or model employed. Here, we demon-\n",
      "strate this by analyzing a piece of business news using various popular sentiment analysis\n",
      "tools: TextBlob, text2data, VADER, and FinBERT. The methods vary signiï¬cantly in both\n",
      "\n",
      "ð Doc 165: tools: TextBlob, text2data, VADER, and FinBERT. The methods vary signiï¬cantly in both\n",
      "their approach to sentiment determination and the output they provide, as illustrated below.8\n",
      "8Note that applying Loughran-Macdonald is not recommended in for short texts as it yields sparse results. For example,\n",
      "in the example we are considering, it outputs a category distribution that only loads on âStrong Modalâ, which is not a really\n",
      "useful analysis.\n",
      "LM_Scores = {âNegativeâ: 0, âPositiveâ: 0, âUncertaintyâ: 0, âLitigiousâ: 0, âStrong_Modalâ: 2,\n",
      "56\n",
      "\n",
      "ð Doc 166: Example 3: A news article about TelefÃ³ nica and Cellnex | Sentiment: TextBlob\n",
      "Cellnex will face more competition in Europe Score: 0.50\n",
      "TelefÃ³nicaâs (TEF.MC) subsidiary, Telxius Telecom, has agreed to sell its telecommuni-\n",
      "cations tower division in Europe and Latin America to American Tower (AMT), which\n",
      "will expand the latterâs presence in Europe and increase competition for the Spanish\n",
      "wireless telecommunications group Cellnex Telecom (CLNX.MC), according to Equita\n",
      "Sim. Score: 0.00 The transaction \"represents the entry of a new independent tower\n",
      "operator into the Spanish market and potentially more competition for future growth in\n",
      "the European market as well,\" says the brokerage ï¬rm. Score: 0.06\n",
      "Overall Score: 0.085\n",
      "Note: TextBlob is a general-purpose sentiment analysis tool that relies on a pre-built lexicon\n",
      "to assess the polarity of the text. It computes a sentiment score ranging from -1 to 1, where\n",
      "\n",
      "ð Doc 167: to assess the polarity of the text. It computes a sentiment score ranging from -1 to 1, where\n",
      "-1 signiï¬es a negative sentiment, 1 indicates a positive sentiment, and 0 represents a neutral\n",
      "sentiment. The methodology behind TextBlob focuses on tokenizing the input into words and\n",
      "phrases, which are compared against its built-in polarity dictionary.\n",
      "Example 4: A news article about TelefÃ³ nica and Cellnex | Sentiment: text2data\n",
      "Cellnex will face more competition in Europe Score: 0.145\n",
      "TelefÃ³nicaâs (TEF.MC) subsidiary, Telxius Telecom, has agreed to sell its telecommuni-\n",
      "cations tower division in Europe and Latin America to American Tower (AMT), which\n",
      "will expand the latterâs presence in Europe and increase competition for the Spanish\n",
      "wireless telecommunications group Cellnex Telecom (CLNX.MC), according to Equita\n",
      "Sim. Score: -0.512 The transaction \"represents the entry of a new independent\n",
      "tower operator into the Spanish market and potentially more competition for future\n",
      "\n",
      "ð Doc 168: tower operator into the Spanish market and potentially more competition for future\n",
      "growth in the European market as well,\" says the brokerage ï¬rm. Score: -0.560\n",
      "Overall Score: -0.61\n",
      "âWeak_Modalâ: 0, âConstrainingâ: 0, âComplexityâ: 0}\n",
      "57\n",
      "\n",
      "ð Doc 169: Note: text2data employs scientiï¬c deep learning NLP methods to analyze sentiment. Every\n",
      "sentence is split into smaller chunks and represented as a tree structure, capturing the syntactic\n",
      "relationships between words and phrases. To determine the ï¬nal sentiment score, text2data\n",
      "uses probabilistic methods based on a pre-trained data model, providing an output score between\n",
      "-1 and 1, where -1 is negative and 1 is positive.\n",
      "Example 5: A news article about TelefÃ³ nica and Cellnex | Sentiment: VADER\n",
      "Cellnex will face more competition in Europe Score: 0.00\n",
      "TelefÃ³nicaâs (TEF.MC) subsidiary, Telxius Telecom, has agreed to sell its telecommuni-\n",
      "cations tower division in Europe and Latin America to American Tower (AMT), which\n",
      "will expand the latterâs presence in Europe and increase competition for the Spanish\n",
      "wireless telecommunications group Cellnex Telecom (CLNX.MC), according to Equita\n",
      "Sim. Score: 0.69 The transaction \"represents the entry of a new independent tower\n",
      "\n",
      "ð Doc 170: Sim. Score: 0.69 The transaction \"represents the entry of a new independent tower\n",
      "operator into the Spanish market and potentially more competition for future growth in\n",
      "the European market as well,\" says the brokerage ï¬rm. Score: 0.57\n",
      "Overall Score: 0.81\n",
      "Note: VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based\n",
      "sentiment analysis tool uses a combination of lexical features (i.e., words) that are generally\n",
      "classiï¬ed as having positive, negative, or neutral valence. VADER produces four sentiment met-\n",
      "rics: positive, negative, neutral, and a compound score. The compound score is a normalized,\n",
      "weighted composite score that ranges from -1 to 1, indicating the overall sentiment of the text.\n",
      "In this example, we provide the compound measure sentence by sentence and for the whole\n",
      "text.\n",
      "58\n",
      "\n",
      "ð Doc 171: Example 6: A news article about TelefÃ³ nica and Cellnex | Sentiment: FinBERT\n",
      "Cellnex will face more competition in Europe Negative, 0.75\n",
      "TelefÃ³nicaâs (TEF.MC) subsidiary, Telxius Telecom, has agreed to sell its telecommuni-\n",
      "cations tower division in Europe and Latin America to American Tower (AMT), which\n",
      "will expand the latterâs presence in Europe and increase competition for the Spanish\n",
      "wireless telecommunications group Cellnex Telecom (CLNX.MC), according to Equita\n",
      "Sim. Neutral, 0.98 The transaction \"represents the entry of a new independent\n",
      "tower operator into the Spanish market and potentially more competition for future\n",
      "growth in the European market as well,\" says the brokerage ï¬rm. Negative, 0.81\n",
      "Overall Negative, 0.94\n",
      "Note: FinBERT is a domain-speciï¬c transformer-based model trained on ï¬nancial texts. Unlike\n",
      "the previous models, FinBERT provides both a sentiment classiï¬cation (Positive, Negative,\n",
      "\n",
      "ð Doc 172: the previous models, FinBERT provides both a sentiment classiï¬cation (Positive, Negative,\n",
      "Neutral) and a conï¬dence score ranging from 0 to 1, representing the modelâs certainty about\n",
      "the sentiment classiï¬cation.\n",
      "Why not Topic Modeling as a benchmark?\n",
      "Topic modeling, particularly techniques like Latent Dirichlet Allocation (LDA), decomposes text into\n",
      "a set of latent topics based on word co-occurrences. Topic modelling oï¬er a more granular approach\n",
      "compared to sentiment analysis and could potentially oï¬er a valid benchmark for our purpose. However,\n",
      "we argue that transforming news articles into vector embeddings and subsequently clustering them using\n",
      "KMeans oï¬ers a more balanced approach than topic modeling.\n",
      "Topic models rely on bag-of-words representations, which disregard the order and context of words.\n",
      "This limitation hampers the modelâs ability to capture complex semantic relationships and contextual\n",
      "\n",
      "ð Doc 173: This limitation hampers the modelâs ability to capture complex semantic relationships and contextual\n",
      "nuances essential for accurately identifying economic shocks. Consequently, topic models may overlook\n",
      "subtle but economically signiï¬cant information present in the text. On the other hand, vector embeddings\n",
      "encapsulate rich semantic information by capturing the relationships between words in a continuous vector\n",
      "space. Unlike topic models, which are conï¬ned to word co-occurrences, embedding models, particularly\n",
      "transformer-based, generate context-dependent representations, allowing for a nuanced understanding of\n",
      "polysemy and context. This means that the same word can have diï¬erent embeddings depending on the\n",
      "context of the sentence, such as âAppleâ in âApple is a leading tech companyâ versus âApple is a type of\n",
      "fruitâ.\n",
      "59\n",
      "\n",
      "ð Doc 174: An important advantage of vector embeddings is that they scale eï¬ciently with large corpora and can\n",
      "be generated at various granularities, including word, sentence, or document levels. This scalability makes\n",
      "embeddings highly adaptable for diverse downstream tasks such as clustering, classiï¬cation, and similarity\n",
      "detection. In contrast, topic models often require extensive manual tuning and become computationally\n",
      "expensive with larger datasets, limiting their practicality for extensive analyses. This makes embeddings\n",
      "a superior choice for grouping news articles and analyzing their economic implications, as compared to\n",
      "the relatively rigid and broad classiï¬cations produced by topic models.\n",
      "It is true, however, that topic models excel at grouping articles based on shared themes, oï¬ering\n",
      "a straightforward way to identify and interpret these themes by examining the common content of the\n",
      "\n",
      "ð Doc 175: a straightforward way to identify and interpret these themes by examining the common content of the\n",
      "grouped articles. This interpretability is a key advantage of topic models, as it allows for clear labeling\n",
      "of themes. In contrast, vector embeddings lack inherent interpretability at the dimension level. The\n",
      "individual dimensions of an embedding do not have an intuitive meaning, making it challenging to directly\n",
      "understand the relationships they capture. However, this limitation can be mitigated by clustering the\n",
      "embeddings to then apply a similar interpretive process as with topic models: analyzing the articles\n",
      "within each cluster to infer the common patterns. As demonstrated in our analysis, these clusters often\n",
      "correspond to ï¬rm-speciï¬c or industry-speciï¬c topics, oï¬ering valuable insights into economic relationships\n",
      "and forming a valuable benchmark for our LLMâs classiï¬cation of ï¬rm-speciï¬c shocks.\n",
      "\n",
      "ð Doc 176: and forming a valuable benchmark for our LLMâs classiï¬cation of ï¬rm-speciï¬c shocks.\n",
      "Lastly, using embeddings as a benchmark is particularly compelling because they represent the foun-\n",
      "dational layer of an LLM. The ï¬rst step an LLMâs processing pipeline is to transform the text that it is\n",
      "fed into high-dimensional embeddings for further processing. By benchmarking against embeddings, we\n",
      "ensure a direct and relevant comparison between the foundational representations used by LLMs and our\n",
      "specialized classiï¬cation methodology. This comparison highlights the added value of the LLMâs capac-\n",
      "ity to convert these semantic representations (i.e: the vector embeddings) into economically meaningful\n",
      "classiï¬cations. (i.e: our news-implied ï¬rm-speciï¬c shock classiï¬cations).\n",
      "In summary, KMeans clustering of vector embeddings oï¬ers a robust and economically relevant bench-\n",
      "mark for our LLM-based methodology. It provides a rich semantic representation, context-dependent\n",
      "\n",
      "ð Doc 177: mark for our LLM-based methodology. It provides a rich semantic representation, context-dependent\n",
      "ï¬exibility, and scalability that surpass sentiment analysis and topic modeling. Additionally, its alignment\n",
      "with the underlying architecture of LLMs ensures a meaningful comparison. As demonstrated in our\n",
      "analysis, the clusters derived through this approach are predominantly ï¬rm or industry-speciï¬c, thereby\n",
      "oï¬ering a suitable and superior benchmark against which to measure the eï¬ectiveness of our granular\n",
      "classiï¬cation of news-implied ï¬rm-speciï¬c shocks.\n",
      "A.8 Trading Intensity\n",
      "The extraordinary performance of our proposed LLM-based methodology warrants a careful examina-\n",
      "tion of its implementation costs and practical viability. While our primary objective has been to develop\n",
      "a framework that better captures the economic content of news articles and their subsequent market\n",
      "60\n",
      "\n",
      "ð Doc 178: impact, the practical implementation of such strategies necessarily involves trading frictions that could\n",
      "aï¬ect their real-world eï¬cacy. In this section, we analyze the trading intensity patterns of both method-\n",
      "ologies to provide a more complete assessment of their relative merits and to understand how transaction\n",
      "costs might inï¬uence their comparative advantages. We begin by examining the temporal evolution of\n",
      "open positions for both approaches, which provides insights into their underlying trading dynamics and\n",
      "stability characteristics. This analysis is followed by detailed trading intensity metrics and concludes with\n",
      "a reassessment of portfolio statistics after accounting for transaction costs.\n",
      "[Insert Figure A6 about here]\n",
      "The temporal evolution of open positions, as illustrated in Figure A6, reveals fundamental diï¬er-\n",
      "ences in the stability and reliability of trading signals generated by KMeans versus LLM-based clustering\n",
      "\n",
      "ð Doc 179: ences in the stability and reliability of trading signals generated by KMeans versus LLM-based clustering\n",
      "approaches. The KMeans implementation exhibits pronounced volatility in position management, partic-\n",
      "ularly evident in the Greedy algorithmâs behavior, which shows extreme ï¬uctuations ranging from 6 to\n",
      "105 positions. This erratic pattern suggests that KMeans-detected clusters are highly sensitive to market\n",
      "noise and potentially capture transient correlations rather than fundamental relationships. The substan-\n",
      "tial divergence between Greedy and Stable algorithms under KMeans further underscores the methodâs\n",
      "instability, as even minor variations in cluster selection criteria lead to dramatically diï¬erent trading de-\n",
      "cisions. In stark contrast, the LLM-based approach demonstrates remarkably more coherent and stable\n",
      "position management. Both Greedy and Stable algorithms maintain more closely aligned position counts,\n",
      "\n",
      "ð Doc 180: position management. Both Greedy and Stable algorithms maintain more closely aligned position counts,\n",
      "typically ranging between 20 and 75 positions, with highly correlated temporal movements. This conver-\n",
      "gence in behavior between algorithms suggests that LLM-identiï¬ed clusters capture more fundamental\n",
      "and persistent market relationships. Particularly telling is the test period performance, where KMeans\n",
      "exhibits increased position volatility and extreme spikes, while the LLM approach maintains consistent\n",
      "position patterns across both algorithms. This stability in the out-of-sample period provides strong ev-\n",
      "idence that LLM-derived signals, grounded in economic analysis of ï¬rm-speciï¬c shocks, generalize more\n",
      "eï¬ectively to unseen data.\n",
      "[Insert Table A3 about here]\n",
      "The trading intensity metrics, detailed in Table A3, provide quantitative validation of the structural\n",
      "diï¬erences between KMeans and LLM clustering approaches. Under KMeans, the dramatic disparity\n",
      "\n",
      "ð Doc 181: diï¬erences between KMeans and LLM clustering approaches. Under KMeans, the dramatic disparity\n",
      "between Greedy and Stable algorithms (averaging 40.1 versus 10.77 positions, with standard deviations of\n",
      "18.59 and 6.41 respectively) reï¬ects the methodâs fundamental instability. More concerning is the Stable\n",
      "algorithmâs exceptionally high Changes/Position ratio (3.228 versus 0.798 for Greedy), indicating frequent\n",
      "position adjustments necessitated by the transient nature of KMeans-identiï¬ed clusters. The LLM imple-\n",
      "mentation demonstrates substantially more balanced and stable metrics across both algorithms. Average\n",
      "position counts converge (31.8 for Greedy, 26.61 for Stable) with more moderate standard deviations\n",
      "61\n",
      "\n",
      "ð Doc 182: (14.84 and 12.16), suggesting that both aggressive and conservative cluster selection approaches identify\n",
      "similar, fundamentally-driven trading opportunities. The more balanced Changes/Position ratios (1.234\n",
      "and 1.473) and consistent turnover rates (approximately 39% for both algorithms) indicate that LLM-\n",
      "identiï¬ed clusters require less frequent rebalancing, supporting the hypothesis that they capture more\n",
      "persistent market relationships.\n",
      "[Insert Table A4 about here]\n",
      "Finally, the introduction of trading costs impacts the performance metrics of both clustering ap-\n",
      "proaches (see Table A4), though with notably diï¬erent implications for their practical viability. The\n",
      "KMeans-based strategy exhibits visible performance degradation, particularly evident in the test period\n",
      "where both algorithms generate losses (Greedy: -4.1%, Stable: -6.8% average annual returns). This\n",
      "deterioration is accompanied by elevated risk metrics, with the Stable algorithm showing particularly\n",
      "\n",
      "ð Doc 183: deterioration is accompanied by elevated risk metrics, with the Stable algorithm showing particularly\n",
      "concerning characteristics including high standard deviation (14.2%) and extreme kurtosis (14.74) in the\n",
      "test period, suggesting frequent occurrence of extreme returns. In contrast, the LLM-based approach\n",
      "demonstrates superior resilience to trading costs, maintaining more stable performance characteristics\n",
      "across all periods. Most notably, in the test period, the strategy maintains its positive performance\n",
      "(Greedy: 19.0%, Stable: 24.7% annual returns) with substantially lower risk metrics (standard deviations\n",
      "of 6.2% and 7.0% respectively). The LLM approachâs more moderate VaR and CVaR measures compared\n",
      "to KMeans further underscore its superior risk management characteristics under transaction costs. This\n",
      "stark contrast in net performance can be attributed to the fundamentally diï¬erent nature of the signals\n",
      "\n",
      "ð Doc 184: stark contrast in net performance can be attributed to the fundamentally diï¬erent nature of the signals\n",
      "generated by each approach. While KMeansâstatistically-driven clusters require frequent rebalancing that\n",
      "ampliï¬es transaction costs, the LLMâs economically-motivated clusters appear to identify more persistent\n",
      "price patterns that remain proï¬table even after accounting for trading frictions. However, it is worth\n",
      "noting that neither approach was explicitly optimized for transaction cost eï¬ciency, suggesting potential\n",
      "for further improvement through cost-aware portfolio construction. These results highlight that while our\n",
      "LLM-based news parser successfully captures predictable market reactions to news articles, practitioners\n",
      "implementing such strategies would beneï¬t from incorporating transaction costs into their optimization\n",
      "framework.\n",
      "62\n",
      "\n",
      "ð Doc 185: Table A3: Trading Intensity Analysis: Model Comparison\n",
      "(a) Panel A: KMeans\n",
      "Split Algorithm # Open Positions Trading Activity (%) Trading Costs (%)\n",
      "Avg. Std. Max Min Turnover Changes/Pos. Cost Active\n",
      "All Greedy 40.1 18.59 105 6 32.03 0.798 0.0320 100.0\n",
      "Stable 10.77 6.41 30 0 34.75 3.228 0.0347 99.1\n",
      "Train Greedy 36.4 19.33 88 7 30.59 0.840 0.0306 100.0\n",
      "Stable 9.89 5.93 27 0 33.73 3.412 0.0337 98.2\n",
      "Validation Greedy 48.4 10.00 80 30 31.39 0.649 0.0314 100.0\n",
      "Stable 12.34 6.05 30 1 33.42 2.708 0.0334 100.0\n",
      "Test Greedy 38.8 21.74 105 6 35.86 0.925 0.0359 100.0\n",
      "Stable 10.84 7.47 28 1 39.30 3.626 0.0393 100.0\n",
      "(b) Panel B: LLM\n",
      "Split Algorithm # Open Positions Trading Activity (%) Trading Costs (%)\n",
      "Avg. Std. Max Min Turnover Changes/Pos. Cost Active\n",
      "All Greedy 31.8 14.84 75 4 39.21 1.234 0.0392 100.0\n",
      "Stable 26.61 12.16 56 3 39.18 1.473 0.0392 100.0\n",
      "Train Greedy 29.9 16.34 75 4 40.42 1.351 0.0404 100.0\n",
      "Stable 25.54 12.90 56 3 40.45 1.584 0.0404 100.0\n",
      "\n",
      "ð Doc 186: Train Greedy 29.9 16.34 75 4 40.42 1.351 0.0404 100.0\n",
      "Stable 25.54 12.90 56 3 40.45 1.584 0.0404 100.0\n",
      "Validation Greedy 37.0 7.69 58 24 38.43 1.039 0.0384 100.0\n",
      "Stable 31.38 6.82 50 17 37.95 1.209 0.0379 100.0\n",
      "Test Greedy 29.7 16.24 75 6 37.56 1.264 0.0376 100.0\n",
      "Stable 23.43 13.71 54 3 37.85 1.615 0.0378 100.0\n",
      "Note: This table presents trading intensity metrics for both Greedy and Stable algorithms across diï¬erent data splits\n",
      "for two diï¬erent models: KMeans (Panel A) and LLM (Panel B). The metrics are computed at a daily frequency.\n",
      "The â# Open Positionsâ columns report position-related statistics: âAvg.â shows the mean number of concurrent\n",
      "open positions per day, âStd.â represents their standard deviation, while âMaxâand âMinâindicate the maximum and\n",
      "minimum number of positions held simultaneously. Under âTrading Activity (%)â, âTurnoverâ is calculated as the\n",
      "sum of absolute changes in position sizes divided by the total portfolio size, expressed as a percentage; formally,\n",
      "\n",
      "ð Doc 187: sum of absolute changes in position sizes divided by the total portfolio size, expressed as a percentage; formally,\n",
      "T urnovert = 100 Ã (ó°\n",
      "i |wi,t â wi,tâ1|)/(ó°\n",
      "i |wi,t|), where wi,t represents the position size in asset i at time t.\n",
      "âChanges/Pos.â represents the average number of modiï¬cations per position per day, computed as the daily turnover\n",
      "divided by the average number of positions, providing insight into how actively individual positions are managed. The\n",
      "âTrading Costs (%)â section reports âCostâ as the average daily implementation shortfall (computed as the product\n",
      "of daily turnover and a transaction cost parameter of 10 basis points) expressed in percentage terms, while âActiveâ\n",
      "shows the percentage of trading days with at least one open position. All metrics are ï¬rst computed daily and then\n",
      "averaged over their respective periods, except for Max and Min positions which represent the absolute extremes over\n",
      "each period.\n",
      "63\n",
      "\n",
      "ð Doc 188: 64\n",
      "\n",
      "ð Doc 189: Table A4: Portfolio Statistics Comparison: KMeans vs LLM Clustering (net of Trading Costs)\n",
      "(a) Panel A: Statistics of PKMeans\n",
      "Split Algo. Cum.\n",
      "Ret.\n",
      "Avg.\n",
      "Ret.\n",
      "St.\n",
      "Dev.\n",
      "Sharpe\n",
      "Ra-\n",
      "tio\n",
      "Sortino\n",
      "Ra-\n",
      "tio\n",
      "Max.\n",
      "DD\n",
      "Calmar\n",
      "Ratio\n",
      "Skew. Exc.\n",
      "Kurt.\n",
      "VaR\n",
      "95%\n",
      "CVaR\n",
      "95%\n",
      "All Greedy 0.963 -2.9 9.6 -0.3 -0.3 -9.5 -0.3 -0.46 4.00 -13.7 -23.4\n",
      "Stable 1.329 24.4 16.8 1.3 1.5 -8.3 2.9 0.18 5.08 -23.0 -36.8\n",
      "Train Greedy 0.911 -13.2 11.6 -1.2 -1.1 -9.5 -1.4 -0.52 2.72 -18.7 -28.9\n",
      "Stable 1.182 28.9 19.7 1.3 1.4 -8.3 3.5 -0.23 3.24 -30.6 -44.0\n",
      "Validation Greedy 1.058 17.1 7.3 2.2 2.2 -4.0 4.3 -0.48 1.10 -10.2 -16.6\n",
      "Stable 1.115 35.7 13.3 2.3 2.6 -4.2 8.6 -0.23 1.85 -19.3 -29.0\n",
      "Test Greedy 0.988 -4.1 6.8 -0.6 -0.8 -5.3 -0.8 1.76 5.10 -8.2 -10.4\n",
      "Stable 0.979 -6.8 14.2 -0.5 -0.6 -5.6 -1.2 2.49 14.74 -19.4 -27.4\n",
      "(b) Panel B: Statistics of PLLM\n",
      "Split Algo. Cum.\n",
      "Ret.\n",
      "Avg.\n",
      "Ret.\n",
      "St.\n",
      "Dev.\n",
      "Sharpe\n",
      "Ra-\n",
      "tio\n",
      "Sortino\n",
      "Ra-\n",
      "tio\n",
      "Max.\n",
      "DD\n",
      "Calmar\n",
      "Ratio\n",
      "Skew. Exc.\n",
      "Kurt.\n",
      "VaR\n",
      "95%\n",
      "CVaR\n",
      "95%\n",
      "\n",
      "ð Doc 190: Split Algo. Cum.\n",
      "Ret.\n",
      "Avg.\n",
      "Ret.\n",
      "St.\n",
      "Dev.\n",
      "Sharpe\n",
      "Ra-\n",
      "tio\n",
      "Sortino\n",
      "Ra-\n",
      "tio\n",
      "Max.\n",
      "DD\n",
      "Calmar\n",
      "Ratio\n",
      "Skew. Exc.\n",
      "Kurt.\n",
      "VaR\n",
      "95%\n",
      "CVaR\n",
      "95%\n",
      "All Greedy 1.152 11.5 9.6 1.1 1.4 -7.6 1.5 1.47 9.93 -14.4 -19.6\n",
      "Stable 1.200 15.0 8.6 1.6 1.9 -7.2 2.1 0.29 2.23 -12.1 -17.4\n",
      "Train Greedy 1.040 6.2 11.4 0.5 0.7 -7.6 0.8 1.65 8.97 -16.2 -21.6\n",
      "Stable 1.101 15.9 9.9 1.5 1.7 -7.2 2.2 0.18 1.68 -14.1 -20.3\n",
      "Validation Greedy 1.054 16.2 8.2 1.8 2.3 -3.3 4.9 0.16 1.31 -10.9 -17.2\n",
      "Stable 1.013 3.8 7.0 0.5 0.6 -2.2 1.7 0.22 1.31 -11.7 -15.3\n",
      "Test Greedy 1.054 19.0 6.2 2.8 3.5 -1.6 11.9 1.35 7.85 -7.5 -10.6\n",
      "Stable 1.069 24.7 7.0 3.1 4.7 -1.3 18.6 0.86 1.99 -10.1 -11.6\n",
      "Note: Portfolio statistics of trading strategies based on clusters obtained from KMeans (Panel A) and LLM (Panel\n",
      "B) approaches. The statistics provided include performance metrics (Cumulative Return, Average Return (%)), risk\n",
      "measures (Standard Deviation (%), Maximum Drawdown (%), Value at Risk (%), Conditional Value at Risk (%)),\n",
      "\n",
      "ð Doc 191: measures (Standard Deviation (%), Maximum Drawdown (%), Value at Risk (%), Conditional Value at Risk (%)),\n",
      "risk-adjusted performance ratios (Sharpe Ratio, Sortino Ratio, Calmar Ratio), and return distribution characteristics\n",
      "(Skewness, Excess Kurtosis). These statistics are provided for both cluster-selection algorithms: Greedy and Stable.\n",
      "Except for the Cumulative Return, all returns are annualized. The Sharpe Ratio is computed using the daily returns,\n",
      "assuming 252 trading days in a year. The Sortino Ratio is calculated using the daily downside returns. The\n",
      "Maximum Drawdown is the maximum loss from a peak to a trough. The Calmar Ratio is the ratio of the annualized\n",
      "return to the maximum drawdown. Skewness measures the asymmetry of the return distribution, while Kurtosis\n",
      "quantiï¬es the tailsâ thickness. The Value at Risk (VaR) and Conditional Value at Risk (CVaR) are calculated\n",
      "at a 95% conï¬dence level. All returns are calculated net of transaction costs. We implement a transaction cost\n",
      "\n",
      "ð Doc 192: at a 95% conï¬dence level. All returns are calculated net of transaction costs. We implement a transaction cost\n",
      "estimate of 10 basis points per trade,. The Greedy algorithm longs (shorts) clusters that maximize (minimize) the\n",
      "cluster-average-SR in the validation sample subject to a positivity (negativity) constraint, while the Stable algorithm\n",
      "longs (shorts) clusters that minimize the rank diï¬erence between the training and validation rankings of the cluster-\n",
      "average-SRâs subject to a positivity (negativity) constraint, which is now imposed on both sample splits. In both\n",
      "algorithms, the cardinality of each leg is upper-bounded by a hyperparameter Î¸. The holding period of the beta-\n",
      "neutral positions is set to L = 4 trading days for both approaches. The number of traded clusters is Î¸ = 0.5k = 13\n",
      "for KMeans (kâ = 26 clusters) and Î¸ = 0.5k = 10 for LLM (kâ = 20 clusters). The selection criteria for these\n",
      "\n",
      "ð Doc 193: for KMeans (kâ = 26 clusters) and Î¸ = 0.5k = 10 for LLM (kâ = 20 clusters). The selection criteria for these\n",
      "hyperparameters (L, Î¸) is based on maximizing the Sharpe Ratios of the train and validation samples.\n",
      "65\n",
      "\n",
      "ð Doc 194: Figure A1: Sharpe Ratios in the train and validation splits as a function of L (KMeans)\n",
      "2468101214161820Holding period length (L)Â°1â¿0Â°0â¿50â¿00â¿51â¿01â¿52â¿02â¿53â¿0Sharpe Ratio (Train)\n",
      "GreedyStable\n",
      "(a) Plot of SRPtr\n",
      "(L) over a grid of L\n",
      "2 4 6 8101214161820Holding period length (L)\n",
      "012345Sharpe Ratio (Validation)\n",
      "GreedyStable\n",
      "(b) Plot of SRPval\n",
      "(L) over a grid of L\n",
      "Note: This ï¬gure shows the Sharpe Ratios (SR) as a function of the holding period length (L) for the KMeans\n",
      "clustering method in the training (Panel a) and validation (Panel b) splits. In Panel (a), the Sharpe Ratios in\n",
      "the training set indicate that lower values of L (less than 4) maximize performance. Conversely, in Panel (b), the\n",
      "validation set shows higher Sharpe Ratios for longer holding periods. The choice of L = 4 represents a balanced\n",
      "compromise, providing a stable Sharpe Ratio proï¬le across both splits, ensuring consistent in-sample performance\n",
      "without introducing lookahead bias.\n",
      "66\n",
      "\n",
      "ð Doc 195: Figure A2: Sharpe Ratios in the train and validation splits as a function of Î¸ (KMeans)\n",
      "1234567891011121314Number of traded clusters (Âµ)\n",
      "Â°0â¿50â¿00â¿51â¿01â¿52â¿0Sharpe Ratio (Train)\n",
      "GreedyStable\n",
      "(a) Plot of SRPtr\n",
      "(Î¸) over a grid of Î¸\n",
      "1234567891011121314Number of traded clusters (Âµ)\n",
      "Â°1012345Sharpe Ratio (Validation)\n",
      "GreedyStable\n",
      "(b) Plot of SRPval\n",
      "(Î¸) over a grid of Î¸\n",
      "Note: This ï¬gure illustrates the Sharpe Ratios (SR) as a function of Î¸, the upper bound on the number of traded\n",
      "clusters, for the KMeans clustering method in the training (Panel a) and validation (Panel b) splits. In Panel\n",
      "(a), the Sharpe Ratios in the training set show a trend of increasing stability and maximizing performance as Î¸\n",
      "approaches its upper limit. Similarly, Panel (b) displays a consistent pattern in the validation set, where higher\n",
      "values of Î¸ lead to convergence at the highest and most stable Sharpe Ratios. The choice of Î¸ = 13 (i.e: â0.5 Â· 26â)\n",
      "\n",
      "ð Doc 196: values of Î¸ lead to convergence at the highest and most stable Sharpe Ratios. The choice of Î¸ = 13 (i.e: â0.5 Â· 26â)\n",
      "reï¬ects this observed stability and optimization, providing a balanced and robust selection for the portfolio strategy.\n",
      "67\n",
      "\n",
      "ð Doc 197: Figure A3: Sharpe Ratios in the train and validation splits as a function of hyperparameters (LLM)\n",
      "2468101214161820Holding period length (L)Â°1â¿0Â°0â¿50â¿00â¿51â¿01â¿52â¿02â¿5Sharpe Ratio (Train)\n",
      "GreedyStable\n",
      "(a) Plot of SRPtr\n",
      "(L) over a grid of L\n",
      "2468101214161820Holding period length (L)0â¿00â¿51â¿01â¿52â¿02â¿53â¿03â¿5Sharpe Ratio (Validation)\n",
      "GreedyStable\n",
      "(b) Plot of SRPval\n",
      "(L) over a grid of L\n",
      "Note: This ï¬gure shows the Sharpe Ratios (SR) as a function of the holding period length (L) for the LLM clustering\n",
      "method, across the training (Panel a) and validation (Panel b) splits. In Panel (a), the Sharpe Ratios in the training\n",
      "set reach their maximum at L = 4, suggesting shorter holding periods are more eï¬ective for maximizing performance.\n",
      "Conversely, Panel (b) illustrates that longer holding periods yield higher Sharpe Ratios in the validation set. The\n",
      "choice of L = 4 serves as a compromise, balancing the trade-oï¬ between maximizing SR in both splits and providing\n",
      "\n",
      "ð Doc 198: choice of L = 4 serves as a compromise, balancing the trade-oï¬ between maximizing SR in both splits and providing\n",
      "a stable and consistent holding period length for the strategy.\n",
      "68\n",
      "\n",
      "ð Doc 199: Figure A4: Sharpe Ratios in the train and validation splits as a function of Î¸ (LLM)\n",
      "1234567891011121314Number of traded clusters (Âµ)\n",
      "Â°0â¿50â¿00â¿51â¿01â¿52â¿02â¿5Sharpe Ratio (Train)\n",
      "GreedyStable\n",
      "(a) Plot of SRPtr\n",
      "(Î¸) over a grid of Î¸\n",
      "1234567891011121314Number of traded clusters (Âµ)0â¿00â¿51â¿01â¿52â¿02â¿53â¿0Sharpe Ratio (Validation)\n",
      "GreedyStable\n",
      "(b) Plot of SRPval\n",
      "(Î¸) over a grid of Î¸\n",
      "Note: This ï¬gure illustrates the Sharpe Ratios (SR) as a function of Î¸, the upper bound on the number of traded\n",
      "clusters, for the LLM clustering method in the training (Panel a) and validation (Panel b) splits. In Panel (a),\n",
      "the Sharpe Ratios for the training set indicate a temporary dip at Î¸ = 10 for the Greedy algorithm, yet this value\n",
      "still provides a relatively stable outcome. In contrast, Panel (b) shows that Î¸ = 10 leads to a noticeable increase in\n",
      "Sharpe Ratios for the validation set, particularly beneï¬ting the Greedy algorithm. The choice of Î¸ = â0.5kâ = 10\n",
      "\n",
      "ð Doc 200: Sharpe Ratios for the validation set, particularly beneï¬ting the Greedy algorithm. The choice of Î¸ = â0.5kâ = 10\n",
      "strikes a balance, conï¬rming it as an eï¬ective hyperparameter selection for achieving stability in both the training\n",
      "and validation splits with LLM clustering.\n",
      "69\n",
      "\n",
      "ð Doc 201: Figure A5: Distribution of Cluster-Average Sharpe Ratios (SRg) by Split\n",
      "(a) Panel A: KMeans Clustering\n",
      "Â°15Â°10Â°5 0 5 10 15 20Cluster-Average Sharpe Ratio (SRg)0â¿0000â¿0250â¿0500â¿0750â¿1000â¿1250â¿1500â¿1750â¿200Density\n",
      "SplitTrainValidationTest\n",
      "(b) Panel B: LLM Clustering\n",
      "Â°20Â°15Â°10Â°5 0 5 101520Cluster-Average Sharpe Ratio (SRg)0â¿0000â¿0250â¿0500â¿0750â¿1000â¿1250â¿1500â¿1750â¿200Density\n",
      "SplitTrainValidationTest\n",
      "Note: This ï¬gure presents the distribution of cluster-average Sharpe Ratios (SRg) across training, validation, and test data\n",
      "splits for both KMeans clustering (Panel A) and LLM clustering (Panel B). Each Sharpe Ratio is computed as the average\n",
      "of beta-neutral positions associated with articles in a given cluster. The KMeans approach (Panel A) shows distributions\n",
      "centered around 0 in the validation set, with some outliers exhibiting unusually high or low Sharpe Ratios. The training and\n",
      "\n",
      "ð Doc 202: centered around 0 in the validation set, with some outliers exhibiting unusually high or low Sharpe Ratios. The training and\n",
      "test set distributions are slightly right-skewed, suggesting better performance in certain clusters, with no signiï¬cant outliers.\n",
      "In contrast, the LLM clustering (Panel B) exhibits left-skewed distributions across all splits, indicating a higher frequency\n",
      "of lower Sharpe Ratios. The training data shows fat tails, suggesting extreme values, while the validation data has lighter\n",
      "tails. The test data distribution is more bell-shaped, with Sharpe Ratios concentrated between 5 and 15, indicating stronger\n",
      "performance in some clusters.\n",
      "70\n",
      "\n",
      "ð Doc 203: Figure A6: Evolution of Open Positions: KMeans vs LLM Clustering\n",
      "(a) Panel A: KMeans Clustering\n",
      "Jul2020 Sep Nov Jan2021Mar May Jul Sep\n",
      "Time (trading days)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100# Open Positions\n",
      "Train ValidationTestGreedyStable\n",
      "(b) Panel B: LLM Clustering\n",
      "Jul2020 Sep Nov Jan2021Mar May Jul Sep\n",
      "Time (trading days)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100# Open Positions\n",
      "Train ValidationTest GreedyStable\n",
      "Note: This ï¬gure shows the daily evolution of the number of open positions for both Greedy (blue) and Stable (green) algorithms\n",
      "across diï¬erent data splits (Train, Validation, Test) using KMeans clustering (Panel A) and LLM clustering (Panel B).\n",
      "The time period spans from July 2020 to September 2021. Vertical dashed lines separate the diï¬erent data splits. The\n",
      "Greedy algorithm selects clusters that maximize (minimize) the cluster-average-SR for long (short) positions, while the Stable\n",
      "algorithm minimizes the rank diï¬erence between training and validation rankings. The number of traded clusters is Î¸ =\n",
      "\n",
      "ð Doc 204: algorithm minimizes the rank diï¬erence between training and validation rankings. The number of traded clusters is Î¸ =\n",
      "0.5k = 13 for KMeans (kâ = 26 clusters) and Î¸ = 0.5k = 10 for LLM (kâ = 20 clusters).71\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs): \n",
    "    print(f\"\\nð Doc {i}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d16046b2",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16296a7",
   "metadata": {},
   "source": [
    "Notice that after splitting, we now have many more document objects than we started with. This is because each original page of the PDF has been split into multiple smaller chunks. \n",
    "\n",
    "This splitting is critical for several reasons:\n",
    "1. It helps fit content within the context window of LLMs\n",
    "2. It enables more precise retrieval of relevant information\n",
    "3. It allows for more efficient storage in vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88711cb4",
   "metadata": {},
   "source": [
    "> Important to note: When using `split_documents()`, the LangChain splitters automatically preserve the metadata from the original documents and attach it to each new chunk. This ensures that we maintain information about where each chunk came from, which is crucial for proper attribution and context when retrieving information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f720db",
   "metadata": {},
   "source": [
    "## Token splitting\n",
    "\n",
    "So far, we've been splitting based on character count. However, there's another important approach: splitting on token count.\n",
    "\n",
    "This is particularly useful because LLMs process text as tokens, not characters, and they have context windows defined by token limits (e.g., 4096 tokens, 8192 tokens, etc.). Splitting by token count gives us a more accurate measure of how much text an LLM can process at once.\n",
    "\n",
    "A token is roughly 4 characters on average in English, but this varies widely. Common words might be a single token, while rare words might be split into multiple tokens. By using a `TokenTextSplitter`, we can ensure our chunks respect the actual token boundaries that an LLM would use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da0bcc05",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0116b01b-2b0b-47a7-a107-bbc497029713",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07a95e78",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text1 = \"foo bar bazzyfoo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8eec0912",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', ' bar', ' b', 'az', 'zy', 'foo']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352f549",
   "metadata": {},
   "source": [
    "This demonstrates how tokenization works differently from character splitting. The string \"foo bar bazzyfoo\" is split into tokens like [\"foo\", \" bar\", \" b\", \"az\", \"zy\", \"foo\"]. Notice how some words remain whole, while others (like \"bazzyfoo\") get broken into multiple tokens.\n",
    "\n",
    "This highlights an important point: tokenization doesn't always respect word boundaries. The way text is tokenized depends on the tokenizer's vocabulary and training, and can sometimes break words in unexpected places.\n",
    "\n",
    "Now let's try applying token splitting to our PDF documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffa29d43",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e53e203a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the PDF: 4409\n"
     ]
    }
   ],
   "source": [
    "tokens = text_splitter.split_documents(pages)\n",
    "print(f\"Total number of tokens in the PDF: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f520336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð§© Token 0: Predicting Market Reactions to News:\n",
      "\n",
      "ð§© Token 1: An LLM-Based Approach Using Spanish Business Articles\n",
      "ð§© Token 2: \n",
      "Jesus Villota â\n",
      "Abstract\n",
      "Mark\n",
      "ð§© Token 3: ets do not always eï¬ciently\n",
      "ð§© Token 4:  incorporate news, particularly when information is complex or\n",
      "\n",
      "ð§© Token 5: ambiguous. Traditional text analysis methods fail to capture\n",
      "ð§© Token 6:  the economic structure of information\n",
      "and its ï¿½\n",
      "ð§© Token 7: ï¿½rm-speciï¬c implications\n",
      "ð§© Token 8: . We propose a novel methodology that guides LLMs\n",
      "ð§© Token 9:  to systematically\n",
      "identify and classify ï¬\n",
      "ð§© Token 10: rm-speciï¬c economic shocks\n",
      "ð§© Token 11:  in news articles according to their type, magnitude,\n",
      "ð§© Token 12: \n",
      "and direction. This economically-informed classi\n",
      "ð§© Token 13: ï¬cation allows for a more nuanced understanding\n",
      "ð§© Token 14:  of\n",
      "how markets process complex information. Using a\n",
      "ð§© Token 15:  simple trading strategy, we demonstrate that our\n",
      "LL\n",
      "ð§© Token 16: M-based classiï¬cation sign\n",
      "ð§© Token 17: iï¬cantly outperforms a benchmark\n",
      "ð§© Token 18:  based on clustering vector embeddings,\n",
      "\n",
      "ð§© Token 19: generating consistent proï¬ts out-\n",
      "ð§© Token 20: of-sample while maintaining transparent and durable trading signals\n",
      "ð§© Token 21: .\n",
      "The results suggest that LLMs, when\n",
      "ð§© Token 22:  properly guided by economic frameworks, can eï¿½\n",
      "ð§© Token 23: ï¿½ectively identify\n",
      "persistent patterns in how\n",
      "ð§© Token 24:  markets react to diï¬erent types of\n",
      "ð§© Token 25:  ï¬rm-speciï¬\n",
      "ð§© Token 26: c news. Our ï¬ndings contribute\n",
      "ð§© Token 27: \n",
      "to understanding market eï¬ciency\n",
      "ð§© Token 28:  and information processing, while oï¬ering\n",
      "ð§© Token 29:  a promising new tool for\n",
      "analyzing ï¿½\n",
      "ð§© Token 30: ï¿½nancial narratives.\n",
      "JEL Codes:\n",
      "ð§© Token 31:  G12, G14, C45, C\n",
      "ð§© Token 32: 58, C63, D83\n",
      "Keywords\n",
      "ð§© Token 33: : Large Language Models, Business News, Stock Market\n",
      "ð§© Token 34:  Reaction, Market Eï¬ciency\n",
      "\n",
      "ð§© Token 35: â CEMFI, Calle Casado\n",
      "ð§© Token 36:  del Alisal, 5, 28014, Madrid\n",
      "ð§© Token 37: , Spain. I am deeply grateful to Enrique Sent\n",
      "ð§© Token 38: ana, Manuel Arellano, Dante\n",
      "Am\n",
      "ð§© Token 39: engual, Rafael Repullo, Javier Su\n",
      "ð§© Token 40: Ã¡rez, David MartÃ­nez-M\n",
      "ð§© Token 41: iera, Julio Crego, and Francisco PeÃ±\n",
      "ð§© Token 42: aranda for their comments. I am\n",
      "especially\n",
      "ð§© Token 43:  indebted to NicolÃ¡s Forteza and MatÃ­\n",
      "ð§© Token 44: as Covarrubias, whose invaluable guidance and\n",
      "ð§© Token 45:  insights have greatly enriched this\n",
      "work. I also\n",
      "ð§© Token 46:  thank discussants for their helpful comments, as well\n",
      "ð§© Token 47:  as the participants of the Banking & Finance Seminar\n",
      "ð§© Token 48:  (CEMFI), the\n",
      "Generative AI\n",
      "ð§© Token 49:  in Finance Conference (Concordia University),\n",
      "ð§© Token 50:  the Mirian AndrÃ©s Seminar (University\n",
      "ð§© Token 51:  of La Rioja), the 3rd Contemporary\n",
      "\n",
      "ð§© Token 52: Issues in Financial Markets and Banking (Notting\n",
      "ð§© Token 53: ham Trent University), the Sao Paulo School of Advanced\n",
      "ð§© Token 54:  Science in High Dimensional\n",
      "Models (FG\n",
      "ð§© Token 55: V) and the Barcelona Summer Forum: Machine Learning\n",
      "ð§© Token 56:  in Economics (BSE). Finally, I grate\n",
      "ð§© Token 57: fully acknowledge ï¬nancial\n",
      "support from\n",
      "ð§© Token 58:  Banco de EspaÃ±a (BdE\n",
      "ð§© Token 59: ).\n",
      "ð§© Token 60: 1. Introduction\n",
      "In ï¬nancial\n",
      "ð§© Token 61:  markets, news play a pivotal role in shaping stock\n",
      "ð§© Token 62:  prices. Every day, market participants\n",
      "respond to\n",
      "ð§© Token 63:  a broad spectrum of news ranging from ï¬\n",
      "ð§© Token 64: rm-speciï¬c announcements,\n",
      "ð§© Token 65:  such as earnings releases,\n",
      "to macroeconomic events\n",
      "ð§© Token 66: , such as central bank interest rate announcements, or\n",
      "ð§© Token 67:  geopolitical developments,\n",
      "like international trade conï¿½\n",
      "ð§© Token 68: ï¿½icts or political elections. The Eï¿½\n",
      "ð§© Token 69: ï¿½cient Market Hypothesis (EMH),\n",
      "ð§© Token 70:  formalized\n",
      "by [1] posits that\n",
      "ð§© Token 71:  markets eï¬ciently incorporate new information\n",
      "ð§© Token 72:  almost instantaneously. Both theoret-\n",
      "ical perspectives\n",
      "ð§© Token 73:  and empirical observations indicate that markets do not always exhibit\n",
      "ð§© Token 74:  such eï¬ciency,\n",
      "particularly\n",
      "ð§© Token 75:  when the information is complex or ambiguous. This discrepancy\n",
      "ð§© Token 76:  between theory and reality\n",
      "suggests signiï¿½\n",
      "ð§© Token 77: ï¿½ï¿½cant room for improvement in understanding how\n",
      "ð§© Token 78:  news is processed by market participants\n",
      "and how it\n",
      "ð§© Token 79:  inï¬uences asset prices. A\n",
      "ð§© Token 80:  substantial body of literature has tried to predict market reactions\n",
      "ð§© Token 81: \n",
      "to news, yet some important gaps persist.\n",
      "ð§© Token 82:  Our review of the literature reveals three critical limitations in\n",
      "ð§© Token 83: \n",
      "current approaches to analyzing ï¬nancial\n",
      "ð§© Token 84:  news: a lack of economic focus in textual analysis\n",
      "ð§© Token 85:  methodology,\n",
      "insuï¬cient attention\n",
      "ð§© Token 86:  to ï¬rm-speciï¿½\n",
      "ð§© Token 87: ï¿½c eï¬ects, and\n",
      "ð§© Token 88:  over-reliance on headlines.\n",
      "First,\n",
      "ð§© Token 89:  we examine the lack of economic focus in current methodological\n",
      "ð§© Token 90:  approaches to analyzing\n",
      "ï¬nancial news\n",
      "ð§© Token 91: . This limitation is evident across three main streams of\n",
      "ð§© Token 92:  literature.\n",
      "Sentiment Analysis. Traditional approaches frequently\n",
      "ð§© Token 93:  rely on sentiment analysis, reduc-\n",
      "ing\n",
      "ð§© Token 94:  the richness of news content to binary classiï¿½\n",
      "ð§© Token 95: ï¿½ï¿½cations of positive or negative sentiment.\n",
      "ð§© Token 96:  The\n",
      "seminal work of [2]\n",
      "ð§© Token 97:  demonstrated the predictive power of media sentiment in ï¿½\n",
      "ð§© Token 98: ï¿½nancial markets,\n",
      "showing that negative\n",
      "ð§© Token 99:  media coverage leads to downward pressure on market prices,\n",
      "ð§© Token 100:  followed\n",
      "by a reversion to fundamentals. This\n",
      "ð§© Token 101:  ï¬nding sparked signiï¿½\n",
      "ð§© Token 102: ï¿½cant interest in sentiment-based\n",
      "appro\n",
      "ð§© Token 103: aches, with [3] extending the analysis to\n",
      "ð§© Token 104:  ï¬rm-speciï¬\n",
      "ð§© Token 105: c news and revealing that negative\n",
      "word content not\n",
      "ð§© Token 106:  only forecasts poor ï¬rm earnings but also\n",
      "ð§© Token 107:  indicates a temporary underreac-\n",
      "tion in\n",
      "ð§© Token 108:  stock prices. Despite these early successes, the methodology\n",
      "ð§© Token 109:  of sentiment analysis has\n",
      "faced important challenges. [\n",
      "ð§© Token 110: 4] highlighted a fundamental issue: general-purpose\n",
      "ð§© Token 111:  dictionar-\n",
      "ies often misclassify words\n",
      "ð§© Token 112:  in ï¬nancial contexts, leading them\n",
      "ð§© Token 113:  to develop specialized ï¬nancial\n",
      "word\n",
      "ð§© Token 114:  lists. Building on this insight, [5]\n",
      "ð§© Token 115:  demonstrated that the weighting scheme applied to\n",
      "these\n",
      "ð§© Token 116:  words is as crucial as the word lists themselves,\n",
      "ð§© Token 117:  introducing a more nuanced approach\n",
      "to content analysis.\n",
      "ð§© Token 118:  The emergence of social media and machine learning has driven\n",
      "ð§© Token 119:  fur-\n",
      "ther methodological innovations in sentiment analysis.\n",
      "ð§© Token 120:  [6] leveraged Twitter data to predict\n",
      "\n",
      "ð§© Token 121: DJIA movements, while [7] revealed that\n",
      "ð§© Token 122:  sentimentâs predictive power is particularly pro-\n",
      "ð§© Token 123: \n",
      "nounced during recessions, suggesting time-\n",
      "ð§© Token 124: varying importance of news sentiment. Recent\n",
      "\n",
      "ð§© Token 125: advances in machine learning have pushed the boundaries further\n",
      "ð§© Token 126: , with [8] developing a so-\n",
      "\n",
      "ð§© Token 127: phisticated supervised learning framework speciï¿½\n",
      "ð§© Token 128: ï¿½cally designed for return prediction. The\n",
      "\n",
      "ð§© Token 129: advent of transformer-based models has enabled even\n",
      "ð§© Token 130:  more sophisticated approaches, with [9]\n",
      "and\n",
      "ð§© Token 131:  [10] applying BERT-based architectures to\n",
      "ð§© Token 132:  ï¬nancial sentiment analysis. However,\n",
      "ð§© Token 133:  despite\n",
      "1\n",
      "ð§© Token 134: their widespread adoption and continued methodological reï¬\n",
      "ð§© Token 135: nements, sentiment analysis ap-\n",
      "proaches\n",
      "ð§© Token 136:  remain fundamentally limited. They often miss the intricacy\n",
      "ð§© Token 137:  inherent in news by\n",
      "focusing on linguistic patterns\n",
      "ð§© Token 138:  rather than economically relevant considerations.\n",
      "Topic modeling.\n",
      "ð§© Token 139:  Beyond sentiment analysis, researchers have also explored topic modeling\n",
      "ð§© Token 140: \n",
      "as an alternative approach to categorize text into\n",
      "ð§© Token 141:  broader themes. The pioneering work of\n",
      "[11\n",
      "ð§© Token 142: ] demonstrated that computational linguistics methods could reveal important\n",
      "ð§© Token 143:  patterns in\n",
      "market reactions to news, ï¿½\n",
      "ð§© Token 144: ï¿½nding that stock prices do not immediately and\n",
      "ð§© Token 145:  consistently reï¬ect\n",
      "news, with\n",
      "ð§© Token 146:  eï¬ects varying signiï¿½\n",
      "ð§© Token 147: ï¿½ï¿½cantly across diï¬erent\n",
      "ð§© Token 148:  types of stories and market conditions.\n",
      "Topic modeling\n",
      "ð§© Token 149:  approaches have since been applied across ï¬n\n",
      "ð§© Token 150: ancial research domains. [12]\n",
      "used these\n",
      "ð§© Token 151:  techniques to analyze Federal Reserve communications, while [13\n",
      "ð§© Token 152: ] developed a\n",
      "topic model analyzing over 800,\n",
      "ð§© Token 153: 000 Wall Street Journal articles to track news attention to\n",
      "ð§© Token 154: \n",
      "diï¬erent economic themes. [\n",
      "ð§© Token 155: 14] further integrated topic modeling with asset pricing models\n",
      "ð§© Token 156: \n",
      "to derive systematic risk factors from news. However\n",
      "ð§© Token 157: , these models are limited in adapting\n",
      "to new\n",
      "ð§© Token 158:  and evolving information and lack the speciï¿½\n",
      "ð§© Token 159: ï¿½city needed to assess the precise impact\n",
      "of\n",
      "ð§© Token 160:  news on individual ï¬rms or sectors\n",
      "ð§© Token 161: . While topic models can identify broad themes, they\n",
      "ð§© Token 162: \n",
      "struggle to capture the changing context of ï¿½\n",
      "ð§© Token 163: ï¿½ï¿½nancial news, particularly when new narratives\n",
      "ð§© Token 164: \n",
      "emerge, such as unexpected geopolitical events or\n",
      "ð§© Token 165:  technological disruptions.\n",
      "Vector-based models. Vector\n",
      "ð§© Token 166: -based models have emerged as an alternative approach to\n",
      "ð§© Token 167:  ad-\n",
      "dress the limitations of both sentiment analysis\n",
      "ð§© Token 168:  and topic modeling. The foundational models\n",
      "in this\n",
      "ð§© Token 169:  domain, Word2Vec and GloVe\n",
      "ð§© Token 170: , established the paradigm of mapping words to contin-\n",
      "ð§© Token 171: \n",
      "uous vector spaces based on their co-occ\n",
      "ð§© Token 172: urrence patterns, enabling mathematical operations\n",
      "on words and\n",
      "ð§© Token 173:  capturing semantic relationships. [15] pioneered their application\n",
      "ð§© Token 174:  in ï¬nance by\n",
      "developing time\n",
      "ð§© Token 175: -varying measures of product similarity from ï¿½\n",
      "ð§© Token 176: ï¿½ï¿½rmsâ10-K descriptions\n",
      "ð§© Token 177: , demon-\n",
      "strating how vector representations could\n",
      "ð§© Token 178:  capture nuanced competitive relationships that tra-\n",
      "ditional\n",
      "ð§© Token 179:  industry classiï¬cations miss.\n",
      "ð§© Token 180:  The advent of transformer architectures marked a signif-\n",
      "ð§© Token 181: \n",
      "icant advancement, leading to more sophisticated models\n",
      "ð§© Token 182:  such as BERT, RoBERTa, or\n",
      "ð§© Token 183:  GPT.\n",
      "These models process text through multiple\n",
      "ð§© Token 184:  attention layers, generating context-aware embed-\n",
      "\n",
      "ð§© Token 185: dings by considering relationships between all words simultaneously.\n",
      "ð§© Token 186:  [16] demonstrated their\n",
      "superior performance in\n",
      "ð§© Token 187:  predicting stock movements following ï¬nancial news\n",
      "ð§© Token 188:  events, while\n",
      "[17] leveraged B\n",
      "ð§© Token 189: ERT to develop a novel measure of bond â\n",
      "ð§© Token 190: greennessâ, revealing how subtle\n",
      "text\n",
      "ð§© Token 191: ual diï¬erences in bond documentation translate\n",
      "ð§© Token 192:  into measurable price eï¬ects.\n",
      "ð§© Token 193:  Recent ap-\n",
      "plications have further expanded the\n",
      "ð§© Token 194:  scope of these methods. [18] analyzed ï¿½\n",
      "ð§© Token 195: ï¿½ï¿½nance sentiment\n",
      "across multiple countries and\n",
      "ð§© Token 196:  centuries, while [19] integrated sentiment analysis from\n",
      "ð§© Token 197:  GPT and\n",
      "BERT into traditional asset pricing\n",
      "ð§© Token 198:  models. [20] introduced âasset\n",
      "ð§© Token 199:  embeddingsâ, showing how\n",
      "these\n",
      "ð§© Token 200:  techniques can uncover latent ï¬rm characteristics from\n",
      "ð§© Token 201:  investorsâholdings data. How-\n",
      "\n",
      "ð§© Token 202: ever, even when ï¬ne-tun\n",
      "ð§© Token 203: ed with domain-speciï¬c\n",
      "ð§© Token 204:  training data (e.g: FinBERT\n",
      "ð§© Token 205: ), these methods\n",
      "2\n",
      "ð§© Token 206: cannot inherently incorporate economic structure, which limits their\n",
      "ð§© Token 207:  ability to comprehend\n",
      "the economic implications of news articles\n",
      "ð§© Token 208: .\n",
      "Having examined the limitations of current methodological approaches\n",
      "ð§© Token 209: , we now turn to a second crit-\n",
      "\n",
      "ð§© Token 210: ical gap in the literature: there is an ins\n",
      "ð§© Token 211: uï¬cient focus on ï¬\n",
      "ð§© Token 212: rm-speciï¬c analysis in\n",
      "ð§© Token 213:  existing research. Many\n",
      "studies examine the impact\n",
      "ð§© Token 214:  of news on broader market indices such as the S\n",
      "ð§© Token 215: &P500 or DJIA, rather than\n",
      "\n",
      "ð§© Token 216: on individual ï¬rms. For example\n",
      "ð§© Token 217: , [21] and [22] analyzed comprehensive\n",
      "ð§© Token 218:  news coverage to understand ag-\n",
      "gregate market\n",
      "ð§© Token 219:  movements, while more recent work has leveraged increasingly\n",
      "ð§© Token 220:  sophisticated data sources.\n",
      "[6] developed novel\n",
      "ð§© Token 221:  mood tracking tools for Twitter messages to predict DJIA\n",
      "ð§© Token 222:  movements, and [7] ex-\n",
      "am\n",
      "ð§© Token 223: ined a century of New York Times ï¬\n",
      "ð§© Token 224: nancial columns to study market-wide returns during\n",
      "ð§© Token 225:  recessions.\n",
      "[23], [24]\n",
      "ð§© Token 226:  and [25] constructed innovative news-based indices\n",
      "ð§© Token 227:  that have enhanced our understanding of\n",
      "market-wide\n",
      "ð§© Token 228:  uncertainty and volatility. While these and other similar studies\n",
      "ð§© Token 229:  provide valuable insights\n",
      "into market-wide reactions,\n",
      "ð§© Token 230:  they fall short in elucidating how speci\n",
      "ð§© Token 231: ï¬c ï¬rms are\n",
      "ð§© Token 232:  aï¬ected by news events.\n",
      "\n",
      "ð§© Token 233: Firm-speciï¬c impacts\n",
      "ð§© Token 234:  are often masked when aggregated at the index level\n",
      "ð§© Token 235: , leading to a loss of criti-\n",
      "\n",
      "ð§© Token 236: cal information about how particular entities are inï¿½\n",
      "ð§© Token 237: ï¿½uenced by speciï¬c\n",
      "ð§© Token 238:  news. For example, during the\n",
      "COVID\n",
      "ð§© Token 239: -19 pandemic, market indices masked substantial heterogeneity\n",
      "ð§© Token 240:  in ï¬rm-level responses with some\n",
      "ð§© Token 241: \n",
      "sectors like technology and healthcare experiencing positive returns\n",
      "ð§© Token 242: , while others, such as hospitality,\n",
      "travel\n",
      "ð§© Token 243: , and retail, experiencing signiï¬\n",
      "ð§© Token 244: cant negative impacts due to widespread lockdowns and\n",
      "ð§© Token 245:  reduced\n",
      "consumer spending. Such diï¬\n",
      "ð§© Token 246: erences are often obscured when focusing solely on market indices\n",
      "ð§© Token 247: . Tools\n",
      "like Named Entity Recognition (NER\n",
      "ð§© Token 248: ), which could help identify ï¬rms\n",
      "ð§© Token 249:  impacted by particular events,\n",
      "remain underutil\n",
      "ð§© Token 250: ized in ï¬nancial research, further\n",
      "ð§© Token 251:  contributing to the lack of ï¬rm-\n",
      "ð§© Token 252: level granularity.\n",
      "The third and ï¿½\n",
      "ð§© Token 253: ï¿½nal critical issue is the over-rel\n",
      "ð§© Token 254: iance on headlines as the basis for news analysis.\n",
      "ð§© Token 255: \n",
      "Headlines are often used due to their availability\n",
      "ð§© Token 256:  and the simplicity of extracting sentiment from them,\n",
      "\n",
      "ð§© Token 257: making them convenient but insuï¬cient\n",
      "ð§© Token 258:  for comprehensive analysis. [26] provided early evidence\n",
      "ð§© Token 259:  of\n",
      "this limitation, showing distinct market reactions to\n",
      "ð§© Token 260:  headline news versus no-news events, particularly in\n",
      "ð§© Token 261: \n",
      "terms of drift after bad news and reversals\n",
      "ð§© Token 262:  after extreme price movements. As natural language processing\n",
      "\n",
      "ð§© Token 263: techniques evolved, researchers continued to focus primarily on\n",
      "ð§© Token 264:  headlines: [27] and [10] applied\n",
      "ð§© Token 265:  increasingly\n",
      "sophisticated deep learning and B\n",
      "ð§© Token 266: ERT models to headline analysis, while recent work by\n",
      "ð§© Token 267:  [28] and [29] has\n",
      "ext\n",
      "ð§© Token 268: ended this approach using large language models to extract contextual\n",
      "ð§© Token 269: ized representations from news\n",
      "headlines. While these\n",
      "ð§© Token 270:  studies have advanced our understanding of market reactions to news\n",
      "ð§© Token 271: , headlines\n",
      "are designed to capture attention rather than\n",
      "ð§© Token 272:  provide comprehensive information. Consequently, relying\n",
      "solely\n",
      "ð§© Token 273:  on headlines can lead to overly simplistic analyses that fail\n",
      "ð§© Token 274:  to capture critical contextual details\n",
      "necessary for accurately predicting\n",
      "ð§© Token 275:  market reactions.\n",
      "This paper seeks to address these\n",
      "ð§© Token 276:  three limitations by leveraging Large Language Models (LLMs\n",
      "ð§© Token 277: ) to\n",
      "facilitate an economically-struct\n",
      "ð§© Token 278: ured, granular and ï¬rm-\n",
      "ð§© Token 279: speciï¬c analysis of complete news\n",
      "ð§© Token 280:  articles. LLMs\n",
      "are particularly suited for economic\n",
      "ð§© Token 281:  interpretation due to their extensive training on human-generated\n",
      "ð§© Token 282: \n",
      "3\n",
      "ð§© Token 283: text, including ï¬nancial and economic\n",
      "ð§© Token 284:  discourse. This exposure enables them to âunder\n",
      "ð§© Token 285: standâ economic\n",
      "concepts, cause-\n",
      "ð§© Token 286: and-eï¬ect relationships, and\n",
      "ð§© Token 287:  market mechanisms in ways that mirror human economic\n",
      "reason\n",
      "ð§© Token 288: ing. Unlike purely statistical approaches, LLMs can\n",
      "ð§© Token 289:  recognize economic patterns and implications\n",
      "that would be evident\n",
      "ð§© Token 290:  to market participants, making them powerful tools for ï¿½\n",
      "ð§© Token 291: ï¿½ï¿½nancial analysis. For\n",
      "example,\n",
      "ð§© Token 292:  LLMs could simulate human analysis of news articles,\n",
      "ð§© Token 293:  understanding the economic shocks that\n",
      "a news article describes\n",
      "ð§© Token 294:  upon a speciï¬c ï¿½\n",
      "ð§© Token 295: ï¿½rm âsuch as supply chain disruptions aï¿½\n",
      "ð§© Token 296: ï¿½ï¿½ecting manufacturing,\n",
      "shifts in\n",
      "ð§© Token 297:  consumer demand impacting retail, or policy changes inï¿½\n",
      "ð§© Token 298: ï¿½ï¿½uencing energy sectorsâ and quantifying\n",
      "ð§© Token 299: \n",
      "both the magnitude and direction of these impacts on\n",
      "ð§© Token 300:  speciï¬c ï¬r\n",
      "ð§© Token 301: ms. In this study, we leverage LLMs\n",
      "ð§© Token 302: \n",
      "to parse a dataset of Spanish business news articles\n",
      "ð§© Token 303:  from DowJones Newswires, spanning June 2020\n",
      "ð§© Token 304:  to\n",
      "September 2021, a particularly unstable period marked\n",
      "ð§© Token 305:  by economic disruptions due to the COVID-\n",
      "\n",
      "ð§© Token 306: 19 pandemic. This period was purposefully chosen for\n",
      "ð§© Token 307:  its inherent complexity and market instability.\n",
      "Testing our\n",
      "ð§© Token 308:  methodology during such a challenging period allows us to rig\n",
      "ð§© Token 309: orously evaluate its robustness\n",
      "and eï¿½\n",
      "ð§© Token 310: ï¿½ectiveness. While many methodologies can perform\n",
      "ð§© Token 311:  adequately during stable market conditions,\n",
      "their true capabilities\n",
      "ð§© Token 312:  are revealed when faced with unprecedented market dynamics and rapid\n",
      "ð§© Token 313:  economic\n",
      "changes.\n",
      "Our methodology consists of de\n",
      "ð§© Token 314: ï¬ning a schema with which we guide\n",
      "ð§© Token 315:  an LLM to detect ï¬rm-\n",
      "ð§© Token 316: speciï¬c\n",
      "shocks from\n",
      "ð§© Token 317:  business news and to further classify them by their type\n",
      "ð§© Token 318:  (demand, supply, technological,\n",
      "policy,\n",
      "ð§© Token 319:  ï¬nancial), magnitude (minor\n",
      "ð§© Token 320: , major) and direction (positive, negative).\n",
      "ð§© Token 321:  Through their ability\n",
      "to categorize and comprehend the\n",
      "ð§© Token 322:  economic implications of news, LLMs generate insights that\n",
      "ð§© Token 323:  surpass\n",
      "traditional methodologies, revealing the underlying mechanisms\n",
      "ð§© Token 324:  driving market behavior. This allows\n",
      "for a more\n",
      "ð§© Token 325:  detailed assessment of how speciï¬c\n",
      "ð§© Token 326:  pieces of information inï¬uence particular\n",
      "ð§© Token 327:  ï¬rms, providing\n",
      "a richer\n",
      "ð§© Token 328:  and more precise picture of market dynamics. As our\n",
      "ð§© Token 329:  benchmark, we employ a vector-based\n",
      "appro\n",
      "ð§© Token 330: ach that represents each news article as a high-\n",
      "ð§© Token 331: dimensional embedding vector using a sentence\n",
      "transformer\n",
      "ð§© Token 332: . This benchmark choice serves two key purposes. First\n",
      "ð§© Token 333: , it oï¬ers greater granularity\n",
      "ð§© Token 334:  and\n",
      "sophistication compared to traditional methods\n",
      "ð§© Token 335:  like sentiment analysis and topic modeling. Second, it\n",
      "ð§© Token 336: \n",
      "provides theoretical consistency with our LLM-\n",
      "ð§© Token 337: based approach, as vector embeddings constitute the\n",
      "ð§© Token 338:  ï¬rst\n",
      "layer of an LL\n",
      "ð§© Token 339: Mâs architecture. This parallel allows us\n",
      "ð§© Token 340:  to eï¬ectively compare the predictive\n",
      "ð§© Token 341:  power of\n",
      "the LLMâs initial\n",
      "ð§© Token 342:  representation (vector embeddings) with its ï¿½\n",
      "ð§© Token 343: ï¿½ï¿½nal output (economically structured news\n",
      "ð§© Token 344: \n",
      "classiï¬cation). Through this\n",
      "ð§© Token 345:  comparison, we can assess whether incorporating economic structure in\n",
      "ð§© Token 346:  the\n",
      "LLM processing step enhances our ability to\n",
      "ð§© Token 347:  predict market reactions to news.\n",
      "To evaluate the\n",
      "ð§© Token 348:  timing ability of our proposed methodology, we develop a\n",
      "ð§© Token 349:  trading strategy that builds\n",
      "on the traditional portfolio sorting\n",
      "ð§© Token 350:  approach. While conventional strategies sort stocks based on ï¿½\n",
      "ð§© Token 351: ï¿½ï¿½rm\n",
      "characteristics, we instead sort\n",
      "ð§© Token 352:  based on news clusters. For the benchmark (vector\n",
      "ð§© Token 353:  embeddings), we\n",
      "employ KMeans\n",
      "ð§© Token 354:  clustering, while our LLM methodology clusters articles\n",
      "ð§© Token 355:  by shock categories. We identify\n",
      "the best and\n",
      "ð§© Token 356:  worst-performing clusters by analyzing the stock price responses\n",
      "ð§© Token 357:  of aï¬ected ï¬r\n",
      "ð§© Token 358: ms, then\n",
      "construct a long-short portfolio\n",
      "ð§© Token 359:  strategy that takes long positions in the best-performing\n",
      "ð§© Token 360:  clusters and\n",
      "4\n",
      "ð§© Token 361: short positions in the worst-performing ones. The\n",
      "ð§© Token 362:  proï¬tability of this strategy serves\n",
      "ð§© Token 363:  as a measure of\n",
      "each clustering methodologyï¿½\n",
      "ð§© Token 364: ï¿½s ability to identify economically meaningful news patterns that\n",
      "ð§© Token 365:  translate\n",
      "into improved market timing abilities. Our ï¿½\n",
      "ð§© Token 366: ï¿½ï¿½ndings reveal that while the vector-\n",
      "ð§© Token 367: based model successfully\n",
      "identiï¬es\n",
      "ð§© Token 368:  ï¬rm- and industry-speci\n",
      "ð§© Token 369: ï¬c clusters, its trading signals lack\n",
      "ð§© Token 370:  persistence. The modelâs reliance\n",
      "on\n",
      "ð§© Token 371:  historical ï¬rm and industry performance patterns generates\n",
      "ð§© Token 372:  ephemeral signals that do not translate\n",
      "well\n",
      "ð§© Token 373:  to future market conditions. In contrast, our LL\n",
      "ð§© Token 374: M-based methodology produces clusters based on\n",
      "econom\n",
      "ð§© Token 375: ically meaningful shock classiï¬cations\n",
      "ð§© Token 376: , resulting in more persistent trading signals. The superior\n",
      "ð§© Token 377: \n",
      "out-of-sample performance of our LL\n",
      "ð§© Token 378: M-based trading strategy demonstrates its enhanced capability to\n",
      "ð§© Token 379: \n",
      "capture and interpret market reactions to news,\n",
      "ð§© Token 380:  underscoring the advantages of incorporating economic\n",
      "structure\n",
      "ð§© Token 381:  into news analysis.\n",
      "The objective of this paper\n",
      "ð§© Token 382:  is not to parse the largest dataset available or to\n",
      "ð§© Token 383:  develop a realistic trading\n",
      "strategy with commercial application\n",
      "ð§© Token 384: . Rather, it aims to introduce a novel methodology\n",
      "ð§© Token 385:  for analyzing news\n",
      "articles in a granular and\n",
      "ð§© Token 386:  ï¬rm-speciï¬\n",
      "ð§© Token 387: c manner, demonstrating its utility through a reduced dataset\n",
      "ð§© Token 388: . By\n",
      "focusing on a smaller, high\n",
      "ð§© Token 389: -quality dataset, the study emphasizes methodological rigor\n",
      "ð§© Token 390:  and interpretability.\n",
      "The ï¬nd\n",
      "ð§© Token 391: ings are intended to contribute to a more nuanced understanding\n",
      "ð§© Token 392:  of how market participants\n",
      "process news, using a\n",
      "ð§© Token 393:  simple trading strategy to illustrate the potential of this approach\n",
      "ð§© Token 394:  in capturing the\n",
      "complexities of information processing in\n",
      "ð§© Token 395:  ï¬nancial markets. This methodological contribution\n",
      "ð§© Token 396:  lays the\n",
      "groundwork for future research that could\n",
      "ð§© Token 397:  extend these techniques to larger datasets and more complex\n",
      "\n",
      "ð§© Token 398: trading applications, ultimately enhancing our ability to understand\n",
      "ð§© Token 399:  and predict market behavior in\n",
      "response to news.\n",
      "ð§© Token 400: \n",
      "The remainder of this paper is organized as follows\n",
      "ð§© Token 401: : Section 2 presents the dataset and preprocessing\n",
      "\n",
      "ð§© Token 402: steps. Section 3 provides a mathematical framework for analyzing\n",
      "ð§© Token 403:  news articles. In Section 4, we focus\n",
      "\n",
      "ð§© Token 404: on clustering news articles â ï¬r\n",
      "ð§© Token 405: st presenting the benchmark framework using KMeans clust\n",
      "ð§© Token 406: ering of vector\n",
      "embeddings, followed by\n",
      "ð§© Token 407:  our novel LLM-based methodology. Section 5\n",
      "ð§© Token 408:  details the construction of a simple\n",
      "trading strategy\n",
      "ð§© Token 409: , including market-beta-neutral positions for each\n",
      "ð§© Token 410:  ï¬rm-article pair, extraction of\n",
      "ð§© Token 411:  cluster-\n",
      "average Sharpe Ratios, and\n",
      "ð§© Token 412:  selection of optimal clusters based on two proposed algorithms.\n",
      "ð§© Token 413:  In Section 6,\n",
      "we perform robustness checks\n",
      "ð§© Token 414:  by examining the sensitivity of our results to hyperparam\n",
      "ð§© Token 415: eter variations.\n",
      "Finally, Section 7 concludes and\n",
      "ð§© Token 416:  discusses the implications of our ï¬ndings\n",
      "ð§© Token 417: \n",
      "2. Data\n",
      "This paper employs a dataset\n",
      "ð§© Token 418:  of Spanish business news articles sourced from Dow Jones New\n",
      "ð§© Token 419: swires,\n",
      "covering the period from June\n",
      "ð§© Token 420:  24, 2020, to September 30, 2021.\n",
      "ð§© Token 421:  The selection of this timeframe is\n",
      "deliberate\n",
      "ð§© Token 422: , driven by two key considerations. First, given\n",
      "ð§© Token 423:  the substantial computational demands of\n",
      "LLM-based\n",
      "ð§© Token 424:  analysis, we strategically focus on a smaller, carefully\n",
      "ð§© Token 425:  curated dataset. This deliberate scope\n",
      "reduction allows\n",
      "ð§© Token 426:  us to thoroughly demonstrate our novel methodologyâs\n",
      "ð§© Token 427:  eï¬ectiveness in decoding market-\n",
      "ð§© Token 428: \n",
      "5\n",
      "ð§© Token 429: news relationships while keeping computational costs manageable. Second,\n",
      "ð§© Token 430:  we speciï¬cally chose the\n",
      "ð§© Token 431:  Covid-\n",
      "19 era to test our methodology\n",
      "ð§© Token 432: âs extrapolative capabilities during periods of\n",
      "ð§© Token 433:  signiï¬cant market instability\n",
      "\n",
      "ð§© Token 434: and volatility. While existing textual algorithms typically perform well\n",
      "ð§© Token 435:  in stable market conditions, they\n",
      "often struggle to\n",
      "ð§© Token 436:  generalize eï¬ectively during periods\n",
      "ð§© Token 437:  of heightened uncertainty. By focusing on this\n",
      "vol\n",
      "ð§© Token 438: atile period, we can better assess our methodologyï¿½\n",
      "ð§© Token 439: ï¿½s robustness and its ability to maintain predictive\n",
      "ð§© Token 440: \n",
      "power under challenging market conditions.\n",
      "The dataset\n",
      "ð§© Token 441:  consists of high-quality articles that have been ï¿½\n",
      "ð§© Token 442: ï¿½ï¿½ltered to include only those mentioning\n",
      "\n",
      "ð§© Token 443: Spanish publicly traded ï¬rms listed on\n",
      "ð§© Token 444:  the IBEX-35 index. These 35 companies\n",
      "ð§© Token 445:  represent the largest ï¬rms\n",
      "in\n",
      "ð§© Token 446:  Spain by market capitalization and are typically the most\n",
      "ð§© Token 447:  liquid and actively traded Spanish stocks.\n",
      "Moreover,\n",
      "ð§© Token 448:  these companies tend to receive the most consistent media coverage\n",
      "ð§© Token 449: , making them ideal for the\n",
      "scope of our\n",
      "ð§© Token 450:  analysis.\n",
      "The use of Dow Jones Newsw\n",
      "ð§© Token 451: ires as our news source is also intentional. Dow\n",
      "ð§© Token 452:  Jones has a standard\n",
      "practice of including the stock\n",
      "ð§© Token 453:  market ticker of ï¬rms directly\n",
      "ð§© Token 454:  aï¬ected by the article in parentheses\n",
      "ð§© Token 455: , while\n",
      "excluding ï¬rms mentioned\n",
      "ð§© Token 456:  for secondary purposes from ticker speciï¿½\n",
      "ð§© Token 457: ï¿½cation. This feature signiï¬\n",
      "ð§© Token 458: cantly facil-\n",
      "itates the extraction of\n",
      "ð§© Token 459:  named entities (i.e., Named Entity Recogn\n",
      "ð§© Token 460: ition, or NER). The tickers used\n",
      "ð§© Token 461:  by Dow\n",
      "Jones align with those from Yahoo Finance\n",
      "ð§© Token 462: , enabling seamless integration between our NER algorithm and\n",
      "ð§© Token 463: \n",
      "subsequent ï¬rm-speci\n",
      "ð§© Token 464: ï¬c trading operations via the Yahoo Finance\n",
      "ð§© Token 465:  API. We employ a pattern recognition\n",
      "algorithm\n",
      "ð§© Token 466:  through the regex library in Python to identify speci\n",
      "ð§© Token 467: ï¬c mentions of publicly traded companies\n",
      "\n",
      "ð§© Token 468: in the Spanish stock exchange. The algorithm searches for\n",
      "ð§© Token 469:  patterns of the form â(<WORD\n",
      "ð§© Token 470: >.MC)â for any\n",
      "<W\n",
      "ð§© Token 471: ORD>. For instance, consider the following example article\n",
      "ð§© Token 472:  (translated into English for convenience):\n",
      "Example\n",
      "ð§© Token 473:  1: An article about ACS and Acciona (\n",
      "ð§© Token 474: translated into English)\n",
      "ACS and Acc\n",
      "ð§© Token 475: iona Secure Contracts for New Australian Airport\n",
      "A consortium\n",
      "ð§© Token 476:  of Actividades de ConstrucciÃ³n y\n",
      "ð§© Token 477:  Servicios SA (ACS.MC)\n",
      "ð§© Token 478:  and Acciona\n",
      "SA (ANA.MC)\n",
      "ð§© Token 479:  has won a contract to build the operations area of\n",
      "ð§© Token 480:  the Western Sydney\n",
      "International Airport (Nancy-\n",
      "ð§© Token 481: Bird Walton) and carry out paving works, amount\n",
      "ð§© Token 482: ing to\n",
      "AUD265 million (EUR164\n",
      "ð§© Token 483:  million) for the Australian subsidiary CIMIC Group\n",
      "ð§© Token 484:  Ltd\n",
      "(CIM.AU). CIM\n",
      "ð§© Token 485: IC will carry out the work through its subsidiary CP\n",
      "ð§© Token 486: B Contractors, as\n",
      "stated in a press\n",
      "ð§© Token 487:  release. This is the third project awarded by Western\n",
      "ð§© Token 488:  Sydney Airport\n",
      "to the joint venture after being selected\n",
      "ð§© Token 489:  to carry out earthworks. Construction will take\n",
      "\n",
      "ð§© Token 490: two years, and the Western Sydney airport is expected\n",
      "ð§© Token 491:  to open in 2026.\n",
      "Our NER\n",
      "ð§© Token 492:  algorithm applied to Example 1 successfully identiï¿½\n",
      "ð§© Token 493: ï¿½es the Spanish ï¬rms ACS\n",
      "ð§© Token 494: .MC (Actividades\n",
      "de Constru\n",
      "ð§© Token 495: cciÃ³ n y Servicios SA) and\n",
      "ð§© Token 496:  ANA.MC (Acciona SA) while\n",
      "ð§© Token 497:  disregarding the Australian CIM.AU\n",
      "(\n",
      "ð§© Token 498: CIMIC Groups Ltd). To further ensure the\n",
      "ð§© Token 499:  reliability of ï¬rm identiï¿½\n",
      "ð§© Token 500: ï¿½cation, we validate the extracted\n",
      "entities\n",
      "ð§© Token 501:  using a Large Language Model (LLM). In\n",
      "ð§© Token 502:  particular, we feed the articles to the LLM\n",
      "ð§© Token 503: , which parses\n",
      "6\n",
      "ð§© Token 504: them according to a predeï¬ned\n",
      "ð§© Token 505:  schema. As we will see later, the ï¿½\n",
      "ð§© Token 506: ï¿½ï¿½rst task in this schema is to\n",
      "ð§© Token 507:  identify\n",
      "the listed Spanish ï¬rms\n",
      "ð§© Token 508:  directly aï¬ected by the events described\n",
      "ð§© Token 509:  in the article. Finally, the identiï¿½\n",
      "ð§© Token 510: ï¿½ï¿½ed\n",
      "ï¬rms are\n",
      "ð§© Token 511:  ï¬ltered against a dynamic list of\n",
      "ð§© Token 512:  IBEX-35 members. Due to the high\n",
      "ð§© Token 513:  quality of the dataset, the\n",
      "correlation between\n",
      "ð§© Token 514:  entities identiï¬ed by the LL\n",
      "ð§© Token 515: M and those extracted via pattern recognition is almost\n",
      "\n",
      "ð§© Token 516: exact.\n",
      "For subsequent analysis, we partition\n",
      "ð§© Token 517:  the dataset into three splits: Train, Validation\n",
      "ð§© Token 518: , and Test. Each\n",
      "split serves a distinct\n",
      "ð§© Token 519:  purpose that will be explained in detail as we progress\n",
      "ð§© Token 520:  through the paper. Summary\n",
      "statistics for each\n",
      "ð§© Token 521:  data split are provided in Table 1.\n",
      "[\n",
      "ð§© Token 522: Insert Table 1 about here]\n",
      "The most frequently\n",
      "ð§© Token 523:  used words in the whole dataset are depicted in Figure\n",
      "ð§© Token 524:  1 by means of a WordCloud.\n",
      "As\n",
      "ð§© Token 525:  shown, the most prominent words include âemp\n",
      "ð§© Token 526: resaâ (ï¬rm), ï¿½\n",
      "ð§© Token 527: ï¿½compaÃ±Ã­aâ (company),\n",
      "ð§© Token 528:  and âespaÃ±aâ\n",
      "(\n",
      "ð§© Token 529: Spain), reinforcing that the dataset primarily comprises Spanish business\n",
      "ð§© Token 530:  news, with a prevalence of\n",
      "technical terms such\n",
      "ð§© Token 531:  as âbeneï¬cio\n",
      "ð§© Token 532:  netoâ (net proï¬\n",
      "ð§© Token 533: t), âprecio objetivo\n",
      "ð§© Token 534: â (target price), âproy\n",
      "ð§© Token 535: ectoâ (project),\n",
      "and ï¿½\n",
      "ð§© Token 536: ï¿½operaciÃ³nâ (operation).\n",
      "\n",
      "ð§© Token 537: [Insert Figure 1 about here]\n",
      "The distribution\n",
      "ð§© Token 538:  of the number of articles published per day is illustrated\n",
      "ð§© Token 539:  in Figure 2a, showing\n",
      "that the most\n",
      "ð§© Token 540:  frequent publication rate is between 5 and 10 articles per\n",
      "ð§© Token 541:  day, though some days exhibit\n",
      "unusually high\n",
      "ð§© Token 542:  publication counts. Figure 2b shows the distribution of\n",
      "ð§© Token 543:  the number of words per article,\n",
      "with the\n",
      "ð§© Token 544:  majority of articles containing between 70 and 280 words.\n",
      "ð§© Token 545:  This indicates that the articles are\n",
      "relatively succinct\n",
      "ð§© Token 546: , providing direct information. However, the long right\n",
      "ð§© Token 547:  tail points to instances of more\n",
      "comprehens\n",
      "ð§© Token 548: ive coverage.\n",
      "[Insert Figure 2 about here\n",
      "ð§© Token 549: ]\n",
      "The time series of the number of articles\n",
      "ð§© Token 550:  published per day throughout the sample period is shown in\n",
      "ð§© Token 551: \n",
      "Figure 3. The series exhibits considerable variability,\n",
      "ð§© Token 552:  with frequent ï¬uctuations from fewer than\n",
      "ð§© Token 553:  5 articles\n",
      "per day to sudden spikes exceeding 20\n",
      "ð§© Token 554:  articles. The 30-day moving average smooths\n",
      "ð§© Token 555:  the series, conï¬rming\n",
      "\n",
      "ð§© Token 556: the previous observation that, on average, between 5\n",
      "ð§© Token 557:  and 10 articles are published daily.\n",
      "[Insert\n",
      "ð§© Token 558:  Figure 3 about here]\n",
      "Data Availability. The\n",
      "ð§© Token 559:  dataset used in this study contains conï¬\n",
      "ð§© Token 560: dential information provided under\n",
      "agreements with the\n",
      "ð§© Token 561:  Bank of Spain and Dow Jones Newswires,\n",
      "ð§© Token 562:  and cannot be shared publicly or with\n",
      "third parties\n",
      "ð§© Token 563: . Interested readers may access the same data from\n",
      "ð§© Token 564:  Dow Jones Newswires for a fee.\n",
      "\n",
      "ð§© Token 565: 7\n",
      "ð§© Token 566: 3. Mathematical Treatment of News Articles\n",
      "Our\n",
      "ð§© Token 567:  dataset consists of N = 2, 613 Spanish\n",
      "ð§© Token 568:  business news articles sourced from DowJones and spanning\n",
      "\n",
      "ð§© Token 569: the period from 2020/06/24 to 2021\n",
      "ð§© Token 570: /09/30. We denote as D the\n",
      "ð§© Token 571:  set of all articles in our sample. These\n",
      "\n",
      "ð§© Token 572: articles have been speciï¬cally\n",
      "ð§© Token 573:  ï¬ltered to reference ï¬\n",
      "ð§© Token 574: rms listed on the IBEX-35.\n",
      "ð§© Token 575:  Let FIBEX35 denote the\n",
      "universe\n",
      "ð§© Token 576:  of such ï¬rms. Each article\n",
      "ð§© Token 577:  i â D is a textual document detailing an\n",
      "ð§© Token 578:  event that directly pertains\n",
      "to a subset of\n",
      "ð§© Token 579:  ï¬rms Fi â F\n",
      "ð§© Token 580: IBEX35. The publication date and time of\n",
      "ð§© Token 581:  each article are represented as\n",
      "ãdi\n",
      "\n",
      "ð§© Token 582: 0, ti\n",
      "0ã, where di\n",
      "ð§© Token 583: \n",
      "0 captures the date (YYYY-MM\n",
      "ð§© Token 584: -DD) and ti\n",
      "0 captures the time\n",
      "ð§© Token 585:  (HH:MM) of publication.\n",
      "Therefore\n",
      "ð§© Token 586:  we observe the moment at which Fi receives the ï¿½\n",
      "ð§© Token 587: ï¿½treatmentâ of public news dissemination.\n",
      "\n",
      "ð§© Token 588: Eï¬ective treatment day\n",
      "We are\n",
      "ð§© Token 589:  interested in examining the impact of each news article i\n",
      "ð§© Token 590:  â D on the stock price of the ï¿½\n",
      "ð§© Token 591: ï¿½ï¿½rms\n",
      "directly aï¿½\n",
      "ð§© Token 592: ï¿½ected by it (i.e., all\n",
      "ð§© Token 593:  j â Fi). Since publication datetime may\n",
      "ð§© Token 594:  not coincide with trading hours,\n",
      "we deï¿½\n",
      "ð§© Token 595: ï¿½ï¿½ne an eï¬ective treatment\n",
      "ð§© Token 596:  date, denoted Ëdi\n",
      "0.\n",
      "ð§© Token 597:  This maps the news article publication datetime to the\n",
      "ð§© Token 598: \n",
      "nearest trading date where the stock price can\n",
      "ð§© Token 599:  reï¬ect the news impact.\n",
      "\n",
      "ð§© Token 600: Let d denote the set of all dates in our\n",
      "ð§© Token 601:  sample timeline and let Ëd â\n",
      "ð§© Token 602:  d denote the subset of Spanish\n",
      "trading days\n",
      "ð§© Token 603: . We deï¬ne a function ï¿½\n",
      "ð§© Token 604: ï¿½ : d â Ëd that ï¿½\n",
      "ð§© Token 605: ï¿½nds the next trading date after a given\n",
      "ð§© Token 606:  date:\n",
      "Î(d) := min\n",
      "ð§© Token 607: { Ëd â Ëd |\n",
      "ð§© Token 608:  Ëd > d}. We set Ë\n",
      "ð§© Token 609: di\n",
      "0 to the publication date if the article\n",
      "ð§© Token 610:  was published on a trading\n",
      "day before market close\n",
      "ð§© Token 611:  (17:30 in Spain), and to the\n",
      "ð§© Token 612:  next trading day otherwise. Formally,\n",
      "Ë\n",
      "ð§© Token 613: di\n",
      "0 :=\n",
      "ó°»\n",
      "\n",
      "ð§© Token 614: ó°¿\n",
      "ó°½\n",
      "\n",
      "ð§© Token 615: di\n",
      "0 if di\n",
      "0 â \n",
      "ð§© Token 616: Ëd â§ ti\n",
      "0 < 17:\n",
      "ð§© Token 617: 30\n",
      "Î(di\n",
      "0) if\n",
      "ð§© Token 618:  di\n",
      "0 ââ Ëd\n",
      "ð§© Token 619:  â¨ ti\n",
      "0 â¥ 17:30\n",
      "\n",
      "ð§© Token 620: .\n",
      "The two possible cases are illustrated in Figure\n",
      "ð§© Token 621:  4.\n",
      "[Insert Figure 4 about here]\n",
      "ð§© Token 622: \n",
      "Data Splitting\n",
      "For robust model development and\n",
      "ð§© Token 623:  evaluation, the dataset is partitioned into three sequential\n",
      "ð§© Token 624:  subsets:\n",
      "training, validation, and test\n",
      "ð§© Token 625: : D := Dtr âª Dval ï¿½\n",
      "ð§© Token 626: ï¿½ Dtest. Deï¬ne N\n",
      "ð§© Token 627: split := |Dsplit| for split â\n",
      "ð§© Token 628:  {tr, val, test},\n",
      "where |\n",
      "ð§© Token 629: Â· | denotes the cardinality of a set.\n",
      "ð§© Token 630:  The training and validation sets collectively comprise 80%\n",
      "\n",
      "ð§© Token 631: of the total dataset (Ntr+Nval\n",
      "ð§© Token 632: \n",
      "N = 0.8) and are instrumental\n",
      "ð§© Token 633:  in constructing and ï¬ne-tuning\n",
      "ð§© Token 634:  the trading\n",
      "strategy. The remaining 20%\n",
      "ð§© Token 635:  (Ntest\n",
      "N = 0.2)\n",
      "ð§© Token 636:  is reserved for out-of-sample testing to\n",
      "ð§© Token 637:  assess the performance\n",
      "and generalizability of the\n",
      "ð§© Token 638:  strategy under unseen conditions.\n",
      "8\n",
      "ð§© Token 639: 4. Clustering News Articles\n",
      "In this\n",
      "ð§© Token 640:  section we present our clustering methodology based on news\n",
      "ð§© Token 641: -implied ï¬rm-speci\n",
      "ð§© Token 642: ï¬c shock classi-\n",
      "ï¿½\n",
      "ð§© Token 643: ï¿½ï¿½cations and we compare it against a\n",
      "ð§© Token 644:  benchmark based on clustering the vector embedding representations\n",
      "ð§© Token 645: \n",
      "of the articles. For ease of exposition,\n",
      "ð§© Token 646:  we will ï¬rst present the benchmark\n",
      "ð§© Token 647:  model.\n",
      "4.1 Benchmark: K\n",
      "ð§© Token 648: Means clustering of vector embeddings\n",
      "\n",
      "ð§© Token 649: 4.1.1 Why this benchmark?\n",
      "\n",
      "ð§© Token 650: In evaluating our novel Large Language Model (LLM\n",
      "ð§© Token 651: ) methodology for classifying news-implied ï¿½\n",
      "ð§© Token 652: ï¿½ï¿½rm-\n",
      "speciï¬\n",
      "ð§© Token 653: c shocks, we selected KMeans clustering\n",
      "ð§© Token 654:  of high-dimensional vector embeddings as the\n",
      "ð§© Token 655:  benchmark\n",
      "over alternatives like sentiment analysis and topic modeling\n",
      "ð§© Token 656: . Sentiment analysis, while straightforward,\n",
      "l\n",
      "ð§© Token 657: acks the necessary granularity, oï¬\n",
      "ð§© Token 658: ering only positive, negative, or neutral classi\n",
      "ð§© Token 659: ï¬cations, which is insuf\n",
      "ð§© Token 660: -\n",
      "ï¬cient to compare with our\n",
      "ð§© Token 661:  granular LLM-based economic shock classi\n",
      "ð§© Token 662: ï¬cation. Additionally, sentiment\n",
      "analysis\n",
      "ð§© Token 663:  focuses on the emotional tone rather than the economic impact\n",
      "ð§© Token 664: , it is prone to inconsistencies\n",
      "due to linguistic\n",
      "ð§© Token 665:  nuances and it can deliver very diï¬\n",
      "ð§© Token 666: erent outcomes depending on the speciï¬\n",
      "ð§© Token 667: c sentiment\n",
      "analysis tool employed.\n",
      "On the\n",
      "ð§© Token 668:  other hand, topic modeling provides more detailed classi\n",
      "ð§© Token 669: ï¬cations than sentiment analysis but\n",
      "\n",
      "ð§© Token 670: relies on bag-of-words representations that\n",
      "ð§© Token 671:  fail to capture complex semantic relationships and contex-\n",
      "ð§© Token 672: \n",
      "tual nuances essential for identifying economic shocks accurately\n",
      "ð§© Token 673: . Vector embeddings, particularly those\n",
      "generated\n",
      "ð§© Token 674:  by transformer-based models, oï¬\n",
      "ð§© Token 675: er enhanced semantic representation by capturing context-\n",
      "dependent\n",
      "ð§© Token 676:  meanings and scaling eï¬ciently with\n",
      "ð§© Token 677:  large datasets, making them more ï¬ex\n",
      "ð§© Token 678: ible and adaptable\n",
      "for clustering and class\n",
      "ð§© Token 679: iï¬cation. Although embeddings\n",
      "ð§© Token 680:  lack inherent interpretability, this issue is ad-\n",
      "ð§© Token 681: \n",
      "dressed by clustering, which allows us\n",
      "ð§© Token 682:  to infer meaningful ï¬rm-speci\n",
      "ð§© Token 683: ï¬c or industry-speciï¿½\n",
      "ð§© Token 684: ï¿½ï¿½c patterns from\n",
      "the grouped articles.\n",
      "ð§© Token 685: \n",
      "Lastly, using embeddings as a benchmark\n",
      "ð§© Token 686:  is particularly compelling because they represent the foun-\n",
      "ð§© Token 687: \n",
      "dational layer of an LLM. Name\n",
      "ð§© Token 688: ly, the ï¬rst step in\n",
      "ð§© Token 689:  an LLMâs processing pipeline is to\n",
      "ð§© Token 690:  transform the text\n",
      "that it is fed into embed\n",
      "ð§© Token 691: dings for further processing. By benchmarking against\n",
      "ð§© Token 692:  embeddings, we ensure\n",
      "a direct and\n",
      "ð§© Token 693:  relevant comparison between the foundational representations used by LLMs\n",
      "ð§© Token 694:  and our spe-\n",
      "cialized classiï¿½\n",
      "ð§© Token 695: ï¿½ï¿½cation methodology. This comparison highlights the added\n",
      "ð§© Token 696:  value of the LLMâs capacity to\n",
      "ð§© Token 697: \n",
      "convert these semantic representations (i.e\n",
      "ð§© Token 698: : the vector embeddings) into economically meaningful\n",
      "ð§© Token 699:  classi-\n",
      "ï¬cations.\n",
      "ð§© Token 700:  (i.e: our news-implied\n",
      "ð§© Token 701:  ï¬rm-speciï¬\n",
      "ð§© Token 702: c shock classiï¬cations).\n",
      "ð§© Token 703:  Consequently, KMeans clustering of\n",
      "vector\n",
      "ð§© Token 704:  embeddings provides a robust, scalable, and\n",
      "ð§© Token 705:  economically pertinent benchmark, superior to senti-\n",
      "\n",
      "ð§© Token 706: ment analysis and topic modeling, for assessing our LL\n",
      "ð§© Token 707: M-based classiï¬cation of\n",
      "ð§© Token 708:  news-implied ï¬rm-spe\n",
      "ð§© Token 709: ciï¬c\n",
      "shocks. A\n",
      "ð§© Token 710:  more detailed discussion can be found in A.7\n",
      "ð§© Token 711: .\n",
      "9\n",
      "ð§© Token 712: 4.1.2 Vector embeddings:\n",
      "ð§© Token 713:  âTransforming text into high-dimensional vectors\n",
      "ð§© Token 714: â\n",
      "Any piece of text can be represented\n",
      "ð§© Token 715:  as a high-dimensional vector embedding by using\n",
      "ð§© Token 716:  a transformer.\n",
      "Transformers are a type of\n",
      "ð§© Token 717:  deep learning architecture introduced by [30] which have\n",
      "ð§© Token 718:  revolutionized natural\n",
      "language processing (NLP).\n",
      "ð§© Token 719:  The core idea behind them is the self-att\n",
      "ð§© Token 720: ention mechanism, which allows the\n",
      "model to weigh\n",
      "ð§© Token 721:  the importance of diï¬erent words in\n",
      "ð§© Token 722:  a sentence when generating a representation for\n",
      "each word\n",
      "ð§© Token 723: . This mechanism enables transformers to capture long-\n",
      "ð§© Token 724: range dependencies and contextual\n",
      "relationships within the text\n",
      "ð§© Token 725:  more eï¬ectively than previous models\n",
      "ð§© Token 726:  like recurrent neural networks (RNNs).\n",
      "\n",
      "ð§© Token 727: A transformer model consists of an encoder (and\n",
      "ð§© Token 728:  potentially, a decoder as well) composed of\n",
      "ð§© Token 729:  multiple\n",
      "layers of self-attention and\n",
      "ð§© Token 730:  feedforward neural networks. In our context, we\n",
      "ð§© Token 731:  primarily use the encoder to\n",
      "convert a\n",
      "ð§© Token 732:  piece of text into a ï¬xed\n",
      "ð§© Token 733: -size vector, known as an embedding.\n",
      "ð§© Token 734:  Since our articles are written\n",
      "in Spanish, we\n",
      "ð§© Token 735:  employ a Multilingual Sentence Transformer, which\n",
      "ð§© Token 736:  has been trained on text from\n",
      "multiple languages.\n",
      "ð§© Token 737: \n",
      "For every news article i â D,\n",
      "ð§© Token 738:  we obtain a representative vector embedding ei ï¿½\n",
      "ð§© Token 739: ï¿½ R512 that provides\n",
      "a numerical representation of\n",
      "ð§© Token 740:  various aspects of the text, such as syntactic\n",
      "ð§© Token 741:  structure, semantic content,\n",
      "and contextual nuances.\n",
      "ð§© Token 742:  While it is challenging to assign a speciï¿½\n",
      "ð§© Token 743: ï¿½ï¿½c human-readable meaning to each of\n",
      "ð§© Token 744: \n",
      "the 512 components, we can interpret the vector\n",
      "ð§© Token 745:  as a whole in various ways:\n",
      "â¢ Sem\n",
      "ð§© Token 746: antic Similarity: Similar articles will have similar embed\n",
      "ð§© Token 747: dings. For instance, if one article\n",
      "\n",
      "ð§© Token 748: discusses a companyâs quarterly earnings\n",
      "ð§© Token 749:  and another article discusses the same companyâs\n",
      "ð§© Token 750:  annual\n",
      "earnings, their embeddings will\n",
      "ð§© Token 751:  be close in the 512-dimensional space.\n",
      "\n",
      "ð§© Token 752: â¢ Topic Clustering: Articles on similar topics\n",
      "ð§© Token 753:  will cluster together. For example, articles about\n",
      "\n",
      "ð§© Token 754: ï¬nancial markets might cluster in one\n",
      "ð§© Token 755:  region of the embedding space, while articles about\n",
      "ð§© Token 756:  mergers\n",
      "and acquisitions cluster in another.\n",
      "\n",
      "ð§© Token 757: â¢ Sentiment Analysis: Diï¬erent\n",
      "ð§© Token 758:  regions of the embedding space can implicitly represent di\n",
      "ð§© Token 759: ï¬erent\n",
      "sentiments. Articles with\n",
      "ð§© Token 760:  positive news might cluster in one area, while those\n",
      "ð§© Token 761:  with negative news\n",
      "cluster in another.\n",
      "\n",
      "ð§© Token 762: 4.1.3 Clustering embedd\n",
      "ð§© Token 763: ings with KMeans\n",
      "With the numerical representation\n",
      "ð§© Token 764:  of each article in the form of embeddings\n",
      "ð§© Token 765:  {ei}iâD, we now\n",
      "ð§© Token 766:  seek to\n",
      "identify groups of similar articles.\n",
      "ð§© Token 767:  Namely, we use the KMeans algorithm\n",
      "ð§© Token 768: , a popular clustering method\n",
      "that assigns a\n",
      "ð§© Token 769:  set of vectors into k clusters GKMeans\n",
      "ð§© Token 770:  := {0, 1, ..., k â\n",
      "ð§© Token 771:  1} to minimize the within-cluster\n",
      "\n",
      "ð§© Token 772: sum of squares (WCSS). The implementation of\n",
      "ð§© Token 773:  this clustering algorithm is methodically presented in\n",
      "\n",
      "ð§© Token 774: Appendix Algorithm 1. Each cluster g ï¿½\n",
      "ð§© Token 775: ï¿½ GKMeans deï¬nes\n",
      "ð§© Token 776:  a centroid cg, which is the average\n",
      "ð§© Token 777:  vector of all\n",
      "10\n",
      "ð§© Token 778: the members of a cluster. In the ï¿½\n",
      "ð§© Token 779: ï¿½rst step, we apply the algorithm to\n",
      "ð§© Token 780:  the training data (Dtr).\n",
      "min{\n",
      "ð§© Token 781: Dtrg },{cg}\n",
      "ï¿½\n",
      "ð§© Token 782: ï¿½ï¿½ï¿½k\n",
      "g=1\n",
      "ï¿½\n",
      "ð§© Token 783: ï¿½ï¿½ï¿½\n",
      "iâDtrg\n",
      "ð§© Token 784:  ó°ei â cgï¿½\n",
      "ð§© Token 785: ï¿½ï¿½ï¿½2\n",
      "2\n",
      "s.t\n",
      "ð§© Token 786: .\n",
      "ó°k\n",
      "g=1\n",
      "ð§© Token 787:  Dtr\n",
      "g = Dtr\n",
      "Dtr\n",
      "ð§© Token 788: \n",
      "g â© Dtr\n",
      "h = ï¿½\n",
      "ð§© Token 789: ï¿½ âg, h â GK\n",
      "ð§© Token 790: Means : g â= h\n",
      ".\n",
      "ð§© Token 791: \n",
      "The optimal number of clusters kâ in\n",
      "ð§© Token 792:  this algorithm is to be set exogenously.\n",
      "ð§© Token 793:  Here, we take it to\n",
      "maximize the\n",
      "ð§© Token 794:  average silhouette score in the training sample over some grid\n",
      "ð§© Token 795:  k of cluster sizes k:\n",
      "kâ\n",
      "ð§© Token 796:  := arg maxkâk\n",
      "1\n",
      "\n",
      "ð§© Token 797: |Dtr|\n",
      "ó°\n",
      "\n",
      "ð§© Token 798: iâDtr\n",
      "sk(ei)\n",
      "ð§© Token 799:  .\n",
      "The silhouette score sk(ei) ï¿½\n",
      "ð§© Token 800: ï¿½ [â1, 1] measures how well\n",
      "ð§© Token 801:  an embedding is clustered by comparing its\n",
      "similar\n",
      "ð§© Token 802: ity to its own cluster (intra-cl\n",
      "ð§© Token 803: uster distance) with its similarity to the nearest other\n",
      "ð§© Token 804:  cluster (inter-\n",
      "cluster distance). A\n",
      "ð§© Token 805:  clustering conï¬guration with a\n",
      "ð§© Token 806:  higher average silhouette score (close to +1)\n",
      "ð§© Token 807:  is\n",
      "considered better because it indicates that clusters\n",
      "ð§© Token 808:  are dense and well-separated. Formally\n",
      "ð§© Token 809: , the silhouette\n",
      "score is deï¬\n",
      "ð§© Token 810: ned as\n",
      "sk(ei) := bk\n",
      "ð§© Token 811: \n",
      "ó°\n",
      "eiï¿½\n",
      "ð§© Token 812: ï¿½\n",
      "â ak\n",
      "ó°\n",
      "\n",
      "ð§© Token 813: eió°\n",
      "max {ak (\n",
      "ð§© Token 814: ei) , bk (ei)} ,\n",
      "\n",
      "ð§© Token 815: where, for i â Dtr\n",
      "g\n",
      "ð§© Token 816:  , the intra-cluster distance is deï¿½\n",
      "ð§© Token 817: ï¿½ï¿½ned as ak(ei) := (\n",
      "ð§© Token 818: |Dtr\n",
      "g | â 1)â\n",
      "ð§© Token 819: 1 ó°\n",
      "mâ\n",
      "ð§© Token 820: Dtrg ,mâ=i \n",
      "ð§© Token 821: ó°ei â emï¿½\n",
      "ð§© Token 822: ï¿½2\n",
      "and it represents the average distance from\n",
      "ð§© Token 823:  an embedding ei to all other embedd\n",
      "ð§© Token 824: ings in the same cluster,\n",
      "while the inter\n",
      "ð§© Token 825: -cluster distance is bk(ei)\n",
      "ð§© Token 826:  := minlâ=g(|D\n",
      "ð§© Token 827: tr\n",
      "l |)â1 ï¿½\n",
      "ð§© Token 828: ï¿½ï¿½\n",
      "mâDtr\n",
      "l\n",
      "ð§© Token 829: \n",
      "ó°ei â emï¿½\n",
      "ð§© Token 830: ï¿½ï¿½2 and it represents the\n",
      "minimum average\n",
      "ð§© Token 831:  distance from an embedding ei to all embed\n",
      "ð§© Token 832: dings in the nearest diï¬erent\n",
      "ð§© Token 833:  cluster.\n",
      "In Figure 5 we plot the average\n",
      "ð§© Token 834:  silhouette score for Dtr computed over a grid k\n",
      "ð§© Token 835:  ranging from 2 to 100.\n",
      "The vertical dashed\n",
      "ð§© Token 836:  green line signals the maximizer of the grid,\n",
      "ð§© Token 837:  which corresponds to a cluster size of\n",
      "kï¿½\n",
      "ð§© Token 838: ï¿½ = 26.\n",
      "[Insert Figure 5 about\n",
      "ð§© Token 839:  here]\n",
      "Given the optimal number of clusters k\n",
      "ð§© Token 840: â, we ï¬t the K\n",
      "ð§© Token 841: Means algorithm on the training embeddings\n",
      "\n",
      "ð§© Token 842: {ei | i â Dtr} to\n",
      "ð§© Token 843:  obtain the centroids {ctr\n",
      "1 ,\n",
      "ð§© Token 844:  ctr\n",
      "2 , . . . , c\n",
      "ð§© Token 845: tr\n",
      "kâ}. Following Algorithm 1\n",
      "ð§© Token 846: . (detailed in A.1):\n",
      "\n",
      "ð§© Token 847: {ctr\n",
      "1 , ctr\n",
      "2 ,\n",
      "ð§© Token 848:  . . . , ctr\n",
      "kâ\n",
      "ð§© Token 849: } = KMeans({e1, e\n",
      "ð§© Token 850: 2, . . . , eNtr },\n",
      "ð§© Token 851:  kâ) .\n",
      "We then ï¿½\n",
      "ð§© Token 852: ï¿½nd the cluster associated to each embedding e\n",
      "ð§© Token 853: i in the validation set {ei | i ï¿½\n",
      "ð§© Token 854: ï¿½ Dval} according\n",
      "to the centro\n",
      "ð§© Token 855: ids resulting from clustering the training data {ctr\n",
      "ð§© Token 856: \n",
      "1 , ..., ctr\n",
      "kï¿½\n",
      "ð§© Token 857: ï¿½}. This allows us to obtain the\n",
      "cl\n",
      "ð§© Token 858: ustering of the news articles in the validation sample\n",
      "ð§© Token 859: \n",
      "Dval\n",
      "g =\n",
      "ï¿½\n",
      "ð§© Token 860: ï¿½\n",
      "i â Dval\n",
      "ï¿½\n",
      "ð§© Token 861: ï¿½ï¿½ó°ó°\n",
      "ð§© Token 862: ó° g = arg minï¿½\n",
      "ð§© Token 863: ï¿½âG\n",
      "ó°ei\n",
      "ð§© Token 864:  â ctr\n",
      "â ï¿½\n",
      "ð§© Token 865: ï¿½ï¿½2\n",
      "2\n",
      "ó°\n",
      "ð§© Token 866: \n",
      "âg â GKMeans\n",
      "ð§© Token 867: .\n",
      "11\n",
      "ð§© Token 868: Similarly, by assigning each embedding ei ï¿½\n",
      "ð§© Token 869: ï¿½ {ei | i â Dtest}\n",
      "ð§© Token 870:  to the nearest centroid ctr\n",
      "g ,\n",
      "ð§© Token 871:  we obtain the\n",
      "clusters in the test set\n",
      "ð§© Token 872: \n",
      "Dtest\n",
      "g =\n",
      "ï¿½\n",
      "ð§© Token 873: ï¿½\n",
      "i â Dtest\n",
      "ï¿½\n",
      "ð§© Token 874: ï¿½ï¿½ó°ó°\n",
      "ð§© Token 875: ó° g = arg minï¿½\n",
      "ð§© Token 876: ï¿½âG\n",
      "ó°ei\n",
      "ð§© Token 877:  â ctr\n",
      "â ï¿½\n",
      "ð§© Token 878: ï¿½ï¿½2\n",
      "2\n",
      "ó°\n",
      "ð§© Token 879: \n",
      "âg â GKMeans\n",
      "ð§© Token 880: .\n",
      "[Insert Figure 6 about here]\n",
      "\n",
      "ð§© Token 881: In Figure 6 we can see that the distribution of\n",
      "ð§© Token 882:  articles in the whole sample (D) is fairly\n",
      "ð§© Token 883:  homogenous\n",
      "across the 26 clusters, with\n",
      "ð§© Token 884:  each cluster containing between 50 and 250 articles on average\n",
      "ð§© Token 885: . The notable\n",
      "exceptions are cluster 3,\n",
      "ð§© Token 886:  which contains only 24 articles, and cluster 4,\n",
      "ð§© Token 887:  which concentrates 428 articles.\n",
      "However, the\n",
      "ð§© Token 888:  distribution proï¬le is not consistent over\n",
      "ð§© Token 889:  data splits, which indicates that this classiï¿½\n",
      "ð§© Token 890: ï¿½ï¿½cation\n",
      "procedure is unstable over\n",
      "ð§© Token 891:  time.\n",
      "Although not directly interpretable, by\n",
      "ð§© Token 892:  looking at the articles pooled in a certain cluster,\n",
      "ð§© Token 893:  we can\n",
      "provide some intuition of what it\n",
      "ð§© Token 894:  represents. In most cases, each cluster contains articles\n",
      "ð§© Token 895:  involving a ï¬rm\n",
      "or set of\n",
      "ð§© Token 896:  ï¬rms in the same sector.\n",
      "ð§© Token 897:  For example, cluster 3 pools articles about Telef\n",
      "ð§© Token 898: Ã³ nica and Cellnex\n",
      "(telecom\n",
      "ð§© Token 899: s), cluster 4 contains articles about Caixa\n",
      "ð§© Token 900: Bank, cluster 9 concentrates articles about Repsol\n",
      "ð§© Token 901: ,\n",
      "cluster 12 about Iberdrol\n",
      "ð§© Token 902: a, cluster 15 gathers articles on Infrastructure (led\n",
      "ð§© Token 903:  by ACS and Acciona) and so\n",
      "on\n",
      "ð§© Token 904: .\n",
      "However, there are some exceptions to this\n",
      "ð§© Token 905:  general rule, for example, cluster 0 is a\n",
      "ð§© Token 906:  âmiscellanousâ\n",
      "cl\n",
      "ð§© Token 907: uster: it covers articles about diï¬\n",
      "ð§© Token 908: erent ï¬rms with no apparent relation\n",
      "ð§© Token 909:  between them. Another example\n",
      "is cluster 1,\n",
      "ð§© Token 910:  which pools articles related to the quarterly or semiann\n",
      "ð§© Token 911: ual publication of results by diï¬erent\n",
      "ð§© Token 912: \n",
      "ï¬rms. In Appendix Table\n",
      "ð§© Token 913:  A1 we provide a sample of 3 articles for\n",
      "ð§© Token 914:  each cluster and propose a name for\n",
      "each one\n",
      "ð§© Token 915:  based on the articles they pool.\n",
      "4.\n",
      "ð§© Token 916: 2 LLM-based approach: âWhat\n",
      "ð§© Token 917:  if an LLM reads the news?â\n",
      "ð§© Token 918: \n",
      "One may wonder whether empowering an LLM to\n",
      "ð§© Token 919:  parse news articles according to a predeï¿½\n",
      "ð§© Token 920: ï¿½ned schema\n",
      "that guides it in elluc\n",
      "ð§© Token 921: idating news-implied ï¬rm\n",
      "ð§© Token 922: -speciï¬c shocks can deliver\n",
      "ð§© Token 923:  better insights on how markets\n",
      "react to new information\n",
      "ð§© Token 924: . In this section we will brieï¿½\n",
      "ð§© Token 925: ï¿½y introduce what Large Language Models are,\n",
      "\n",
      "ð§© Token 926: how they have evolved and then, we will dive\n",
      "ð§© Token 927:  into how we can guide them to produce an economically\n",
      "ð§© Token 928: \n",
      "structured analysis of business news.\n",
      "4\n",
      "ð§© Token 929: .2.1 Large Language Models\n",
      "In natural\n",
      "ð§© Token 930:  language processing (NLP), Large Language Models (\n",
      "ð§© Token 931: LLMs) are designed to âunderstand\n",
      "ð§© Token 932: â\n",
      "and generate human-like text.\n",
      "ð§© Token 933:  These models utilize the transformer architecture, which excels\n",
      "ð§© Token 934:  in modeling\n",
      "complex language tasks by capturing long-\n",
      "ð§© Token 935: range dependencies and contextual relationships.\n",
      "At the heart\n",
      "ð§© Token 936:  of LLMs lies the concept of tokens, which\n",
      "ð§© Token 937:  serve as the elemental units of text. Tokens\n",
      "\n",
      "ð§© Token 938: can be individual words, subword units, or\n",
      "ð§© Token 939:  characters. Let x1:n := {x\n",
      "ð§© Token 940: 1, x2, . . . , x\n",
      "ð§© Token 941: n} represent a sequence of\n",
      "12\n",
      "ð§© Token 942: tokens. The goal of an LLM\n",
      "ð§© Token 943:  is to estimate the probability distribution of the next token\n",
      "ð§© Token 944:  xn+1 conditioned\n",
      "on the previous tokens\n",
      "ð§© Token 945:  x1:n\n",
      "P[xn+\n",
      "ð§© Token 946: 1 | {x1, x2, .\n",
      "ð§© Token 947:  . . , xn}] .\n",
      "An\n",
      "ð§© Token 948:  LLM is a neural network architecture designed to learn\n",
      "ð§© Token 949:  and approximate this conditional probabil-\n",
      "ity distribution\n",
      "ð§© Token 950:  over sequences of tokens with a large number of parameters\n",
      "ð§© Token 951:  Î. Namely, we can formulate\n",
      "\n",
      "ð§© Token 952: an LLM as a parameterized function fï¿½\n",
      "ð§© Token 953: ï¿½ that maps a sequence of tokens {x1\n",
      "ð§© Token 954: , x2, . . . , xn\n",
      "ð§© Token 955: } to a probability\n",
      "distribution over the vocabulary\n",
      "ð§© Token 956: , where the parameters Î are learned from a\n",
      "ð§© Token 957:  large corpus of text training\n",
      "data.\n",
      "f\n",
      "ð§© Token 958: Î : {x1, x2,\n",
      "ð§© Token 959:  . . . , xn} â P[\n",
      "ð§© Token 960: xn+1 | {x1, x\n",
      "ð§© Token 961: 2, . . . , xn} ;\n",
      "ð§© Token 962:  Î]\n",
      "Interacting with an LLM\n",
      "ð§© Token 963:  involves specifying a preï¬x sequence x\n",
      "ð§© Token 964: 1:n, termed the âprompt\n",
      "ð§© Token 965: â, and sampling\n",
      "the subsequent tokens x\n",
      "ð§© Token 966: n+1:z, known as the ï¿½\n",
      "ð§© Token 967: ï¿½completionâ. This process enables users\n",
      "ð§© Token 968:  to guide and control\n",
      "the generation of text according\n",
      "ð§© Token 969:  to desired contexts and constraints.\n",
      "{x1\n",
      "ð§© Token 970: , . . . , xn}ï¿½\n",
      "ð§© Token 971: ï¿½ï¿½ ó°¾ï¿½\n",
      "ð§© Token 972: ï¿½ ó°\n",
      "prompt\n",
      "\n",
      "ð§© Token 973: ââ {xn+1, . .\n",
      "ð§© Token 974:  . , xz}ó°¿ \n",
      "ð§© Token 975: ó°¾ó°½ ï¿½\n",
      "ð§© Token 976: ï¿½ï¿½ï¿½\n",
      "completion\n",
      "4.2\n",
      "ð§© Token 977: .2 Evolution of LLMs\n",
      "The transformer architecture\n",
      "ð§© Token 978: , introduced in the seminal work âAttention\n",
      "ð§© Token 979:  Is All You Needâ ([30]),\n",
      "\n",
      "ð§© Token 980: revolutionized LLM development due to its superior handling\n",
      "ð§© Token 981:  of long-range dependencies and eï¬\n",
      "ð§© Token 982: cient\n",
      "parallelization of computations. Sub\n",
      "ð§© Token 983: sequent advancements include the encoder-only BERT\n",
      "ð§© Token 984:  model ([31]),\n",
      "showcasing the power\n",
      "ð§© Token 985:  of pre-training on large datasets for ï¿½\n",
      "ð§© Token 986: ï¿½ne-tuning on speciï¿½\n",
      "ð§© Token 987: ï¿½c tasks.\n",
      "Conversely, OpenAI\n",
      "ð§© Token 988: âs GPT series ([32]) demonstrated\n",
      "ð§© Token 989:  the potential of decoder-only models for gener\n",
      "ð§© Token 990: -\n",
      "ative tasks. In particular, the release\n",
      "ð§© Token 991:  of GPT-3 marked a signiï¿½\n",
      "ð§© Token 992: ï¿½ï¿½cant leap in LLM capabilities with\n",
      "ð§© Token 993:  its\n",
      "175 billion parameters and remarkable few-shot\n",
      "ð§© Token 994:  learning abilities. This model highlighted the impor-\n",
      "ð§© Token 995: \n",
      "tance of prompt engineering, where carefully crafted\n",
      "ð§© Token 996:  prompts can guide model outputs without extensive\n",
      "ï¿½\n",
      "ð§© Token 997: ï¿½ne-tuning.\n",
      "The trend towards\n",
      "ð§© Token 998:  open-source models like BLOOM ([33\n",
      "ð§© Token 999: ]), Mixtral and Metaâs Ll\n",
      "ð§© Token 1000: ama series ([34])\n",
      "emphasizes accessibility\n",
      "ð§© Token 1001:  and transparency in LLM development. The latest models\n",
      "ð§© Token 1002: , including Ope-\n",
      "nAIâ\n",
      "ð§© Token 1003: s GPT-4.5, GPT\n",
      "ð§© Token 1004: -o1 and o3, Googleâ\n",
      "ð§© Token 1005: s Gemini 2.5 Pro, Anthropicï¿½\n",
      "ð§© Token 1006: ï¿½s Claude 3.7 Sonnet, and\n",
      "ð§© Token 1007:  Metaâs\n",
      "Llama-3\n",
      "ð§© Token 1008:  series continue to push boundaries with improved accuracy, multim\n",
      "ð§© Token 1009: odal capabilities, and larger\n",
      "context windows.\n",
      "ð§© Token 1010: \n",
      "4.2.3 Function Calling with Ll\n",
      "ð§© Token 1011: ama-3\n",
      "In our endeavor we will employ\n",
      "ð§© Token 1012:  Llama-3, developed by Meta AI and\n",
      "ð§© Token 1013:  released on April 18, 20241 . This\n",
      "\n",
      "ð§© Token 1014: model has been pre-trained on approximately 15 trillion\n",
      "ð§© Token 1015:  tokens of text gathered from âpublicly available\n",
      "ð§© Token 1016: \n",
      "1 âIntroducing Meta Llama 3\n",
      "ð§© Token 1017: : The most capable openly available LLM to date\n",
      "ð§© Token 1018: â [April 18, 2024]\n",
      "13\n",
      "ð§© Token 1019: sourcesâ and it comes in two sizes\n",
      "ð§© Token 1020: : 8 billion and 70 billion parameters. In this\n",
      "ð§© Token 1021:  application, we will employ\n",
      "the 70B version\n",
      "ð§© Token 1022: , which we will access through an API via Gro\n",
      "ð§© Token 1023: qCloud.\n",
      "Moreover, we will employ a\n",
      "ð§© Token 1024:  function calling approach to streamline the process of interacting\n",
      "ð§© Token 1025:  with\n",
      "the LLM. This implies prespec\n",
      "ð§© Token 1026: ifying a set of functions to the LLM that\n",
      "ð§© Token 1027:  will then be passed through our\n",
      "dataset\n",
      "ð§© Token 1028:  of news articles to obtain a structured output in JSON\n",
      "ð§© Token 1029:  format. The formal procedure is thoroughly\n",
      "described in\n",
      "ð§© Token 1030:  Appendix Algorithm 4.\n",
      "Each article i ï¿½\n",
      "ð§© Token 1031: ï¿½ D implies a conversation with the LLM.\n",
      "ð§© Token 1032:  The structure of the conversation consists\n",
      "of deï¿½\n",
      "ð§© Token 1033: ï¿½ï¿½ning ï¬rst a ï¿½\n",
      "ð§© Token 1034: ï¿½system messageâ, which provides a general\n",
      "ð§© Token 1035:  context and purpose to the model. In our\n",
      "\n",
      "ð§© Token 1036: case:\n",
      "â You are a function calling LL\n",
      "ð§© Token 1037: M that analyses business news in Spanish.\n",
      "â\n",
      "ð§© Token 1038:  For every article, you must identify the ï¿½\n",
      "ð§© Token 1039: ï¿½rms directly aï¬ected by\n",
      "ð§© Token 1040:  the news. Do not include\n",
      "every ï¿½\n",
      "ð§© Token 1041: ï¿½rm mentioned in the article, only include those\n",
      "ð§© Token 1042:  that are directly aï¬ected by the\n",
      "ð§© Token 1043:  shocks\n",
      "narrated therein.\n",
      "â The\n",
      "ð§© Token 1044:  identiï¬ed ï¬r\n",
      "ð§© Token 1045: ms must be Spanish and should be publicly listed in\n",
      "ð§© Token 1046:  the Spanish exchange\n",
      "(their ticker is of\n",
      "ð§© Token 1047:  the form âTICKER.MCï¿½\n",
      "ð§© Token 1048: ï¿½). Do not include non-Spanish foreign ï¿½\n",
      "ð§© Token 1049: ï¿½ï¿½rms. Do\n",
      "not include Spanish\n",
      "ð§© Token 1050:  ï¬rms that are not publicly traded\n",
      "ð§© Token 1051: .\n",
      "â For each identiï¬\n",
      "ð§© Token 1052: ed ï¬rm, classify the shocks that\n",
      "ð§© Token 1053:  aï¬ect them (type, magnitude\n",
      "ð§© Token 1054: , category). The\n",
      "type of shock can be\n",
      "ð§© Token 1055:  âdemandâ, âsupply\n",
      "ð§© Token 1056: â, âï¬nancial\n",
      "ð§© Token 1057: â, âpolicyâ, or\n",
      "ð§© Token 1058:  âtechnologyâ. The magnitude\n",
      "can\n",
      "ð§© Token 1059:  be âminorâor â\n",
      "ð§© Token 1060: majorâ. The direction can be â\n",
      "ð§© Token 1061: positiveâor ânegativeâ.\n",
      "ð§© Token 1062: \n",
      "â If a ï¬rm is a\n",
      "ð§© Token 1063: ï¬ected neutrally by the news\n",
      "ð§© Token 1064:  article, donât include it in the\n",
      "ð§© Token 1065:  analysis.\n",
      "Then, a news article is fed\n",
      "ð§© Token 1066:  to the LLM. For illustration purposes, we\n",
      "ð§© Token 1067:  will work with Example 2:\n",
      "Example 2:\n",
      "ð§© Token 1068:  An article about Cellnex and TelefÃ³ n\n",
      "ð§© Token 1069: ica (translated into English)\n",
      "Cellnex\n",
      "ð§© Token 1070:  will face more competition in Europe\n",
      "TelefÃ³n\n",
      "ð§© Token 1071: icaâs (TEF.MC)\n",
      "ð§© Token 1072:  subsidiary, Telxius Telecom, has agreed to\n",
      "ð§© Token 1073:  sell its telecommuni-\n",
      "cations tower\n",
      "ð§© Token 1074:  division in Europe and Latin America to American Tower (\n",
      "ð§© Token 1075: AMT), which\n",
      "will expand the latterï¿½\n",
      "ð§© Token 1076: ï¿½s presence in Europe and increase competition for the\n",
      "ð§© Token 1077:  Spanish\n",
      "wireless telecommunications group Cellnex Telecom (\n",
      "ð§© Token 1078: CLNX.MC), according to Equita\n",
      "ð§© Token 1079: \n",
      "Sim. The transaction \"represents the entry\n",
      "ð§© Token 1080:  of a new independent tower operator into\n",
      "the Spanish\n",
      "ð§© Token 1081:  market and potentially more competition for future growth in the\n",
      "ð§© Token 1082:  European\n",
      "market as well,\" says the brokerage ï¿½\n",
      "ð§© Token 1083: ï¿½ï¿½rm.\n",
      "Next, we deï¿½\n",
      "ð§© Token 1084: ï¿½ï¿½ne an umbrella function âï¿½\n",
      "ð§© Token 1085: ï¿½rmsâ, which asks the LL\n",
      "ð§© Token 1086: M to identify the set Fi\n",
      "LLM for\n",
      "ð§© Token 1087:  each\n",
      "i â D. Then, for\n",
      "ð§© Token 1088:  each j â Fi\n",
      "LLM we ask\n",
      "ð§© Token 1089:  the LLM to categorize the type, expected\n",
      "ð§© Token 1090:  magnitude, and\n",
      "expected direction that the shock described\n",
      "ð§© Token 1091:  in the article implies in that particular ï¬\n",
      "ð§© Token 1092: rm j.\n",
      "14\n",
      "ð§© Token 1093: [Insert Table 2 about here]\n",
      "The function\n",
      "ð§© Token 1094:  calling schema is outlined in Table 2. First,\n",
      "ð§© Token 1095:  we need to prompt the LLM, and then\n",
      "ð§© Token 1096:  we\n",
      "need to specify the desired format of its\n",
      "ð§© Token 1097:  response. The âOptionsâ column imposes\n",
      "ð§© Token 1098:  the answer format that\n",
      "the LLM must follow\n",
      "ð§© Token 1099: . For example, in firms, the â\n",
      "ð§© Token 1100: arrayâ option indicates that the answer must be\n",
      "ð§© Token 1101:  an\n",
      "enumeration of ï¬r\n",
      "ð§© Token 1102: ms, while the âstringâ option\n",
      "ð§© Token 1103:  in the subfunctions firm and ticker indicates\n",
      "ð§© Token 1104:  that the\n",
      "answer must be a single name.\n",
      "ð§© Token 1105:  Finally, the shock_ subfunctions ask the\n",
      "ð§© Token 1106:  LLM to choose from a predeï¿½\n",
      "ð§© Token 1107: ï¿½ned\n",
      "set of possible responses.\n",
      "Note\n",
      "ð§© Token 1108:  that the ï¬rms identiï¿½\n",
      "ð§© Token 1109: ï¿½ï¿½ed by the LLM are used to\n",
      "ð§© Token 1110:  validate the ï¬rms identiï¿½\n",
      "ð§© Token 1111: ï¿½ï¿½ed by the pattern\n",
      "recognition algorithm\n",
      "ð§© Token 1112:  (those extracted with regex by exploiting the pattern <\n",
      "ð§© Token 1113: WORD>.MC). As mentioned\n",
      "earlier\n",
      "ð§© Token 1114: , given the high quality of the ï¬\n",
      "ð§© Token 1115: ltered dataset (the ticker of the ï¿½\n",
      "ð§© Token 1116: ï¿½ï¿½rms that are actively involved in\n",
      "\n",
      "ð§© Token 1117: the article are explicitly stated), they are almost identical\n",
      "ð§© Token 1118: . Hence, we indistinctively use Fi to\n",
      "ð§© Token 1119:  simplify\n",
      "notation.\n",
      "The LLM provides two\n",
      "ð§© Token 1120:  outputs: structured data (âStructured Output\n",
      "ð§© Token 1121: â) and a explanatory text de-\n",
      "\n",
      "ð§© Token 1122: scribing its reasoning (âUnstructured\n",
      "ð§© Token 1123:  Ouptutâ). The explanations help us\n",
      "ð§© Token 1124:  verify if the model correctly\n",
      "understands how\n",
      "ð§© Token 1125:  to use the function-calling schema and follow system\n",
      "ð§© Token 1126:  instructions. To assess the LLMâs\n",
      "ð§© Token 1127: \n",
      "understanding, we review a random sample of\n",
      "ð§© Token 1128:  these explanations and look for patterns of misinterpreta-\n",
      "ð§© Token 1129: \n",
      "tion, confusion, or hallucination. If\n",
      "ð§© Token 1130:  we identify such issues, we reï¬\n",
      "ð§© Token 1131: ne the system prompts and function\n",
      "descriptions\n",
      "ð§© Token 1132:  to provide clearer guidance. This iterative prompt re\n",
      "ð§© Token 1133: ï¬nement continues until the LLM\n",
      "ð§© Token 1134:  reli-\n",
      "ably generates correct outputs across multiple\n",
      "ð§© Token 1135:  test scenarios.\n",
      "1) Structured Output:\n",
      "ð§© Token 1136: \n",
      "firm ticker shock_type shock_\n",
      "ð§© Token 1137: magnitude shock_direction\n",
      "Cellnex Telecom\n",
      "ð§© Token 1138:  CLNX.MC supply minor negative\n",
      "Tele\n",
      "ð§© Token 1139: fÃ³ nica TEF.MC ï¿½\n",
      "ð§© Token 1140: ï¿½nancial minor positive\n",
      "2) Unstruct\n",
      "ð§© Token 1141: ured Output (justiï¬cation)\n",
      "ð§© Token 1142: \n",
      "The news about American Towerâs expansion\n",
      "ð§© Token 1143:  in Europe may increase competition for Cellnex,\n",
      "\n",
      "ð§© Token 1144: which is why itâs classiï¿½\n",
      "ð§© Token 1145: ï¿½ï¿½ed as a negative supply shock. On\n",
      "ð§© Token 1146:  the other hand, TelefÃ³nica beneï¿½\n",
      "ð§© Token 1147: ï¿½ï¿½ts\n",
      "from the sale of its tower\n",
      "ð§© Token 1148:  division, which is why itâs class\n",
      "ð§© Token 1149: iï¬ed as a positive ï¿½\n",
      "ð§© Token 1150: ï¿½nancial shock.\n",
      "This procedure is run\n",
      "ð§© Token 1151:  iteratively from beginning (deï¬ning\n",
      "ð§© Token 1152:  system prompt) to end (getting the output)\n",
      "ð§© Token 1153: \n",
      "for every i â D.2\n",
      "\n",
      "ð§© Token 1154: 2 This procedure was run on a MacBook Pro M\n",
      "ð§© Token 1155: 2 with 16GB RAM, 12-core central\n",
      "ð§© Token 1156:  processing units (CPU), 19-core\n",
      "g\n",
      "ð§© Token 1157: raphics processing units (GPU), and 16-core\n",
      "ð§© Token 1158:  Neural Engine.\n",
      "15\n",
      "ð§© Token 1159: 4.2.4 Clustering with the\n",
      "ð§© Token 1160:  LLM\n",
      "Formally, we can deï¿½\n",
      "ð§© Token 1161: ï¿½ï¿½ne the set B := {(i\n",
      "ð§© Token 1162: , j) | i â D â§\n",
      "ð§© Token 1163:  j â Fi} containing all the unique pairs\n",
      "ð§© Token 1164:  of articles\n",
      "and identiï¬ed\n",
      "ð§© Token 1165:  ï¬rms. The LLM assigns\n",
      "ð§© Token 1166:  each pair (i, j) â B\n",
      "ð§© Token 1167:  with a choice from each of the following sets:\n",
      "ð§© Token 1168: \n",
      "âshock typeâ ST := {\n",
      "ð§© Token 1169: demand, supply, ï¬nancial,\n",
      "ð§© Token 1170:  technology, policy}\n",
      "âshock magnitudeï¿½\n",
      "ð§© Token 1171: ï¿½ SM := {minor, major}\n",
      "\n",
      "ð§© Token 1172: âshock directionâ SD := {positive\n",
      "ð§© Token 1173: , negative}\n",
      "The clustering of news articles\n",
      "ð§© Token 1174:  follows naturally by taking the Cartesian product of these\n",
      "ð§© Token 1175:  three sets:\n",
      "GLLM := ST Ã\n",
      "ð§© Token 1176:  SM Ã SD, and the total number of clusters\n",
      "ð§© Token 1177:  is now kLLM = |GLLM\n",
      "ð§© Token 1178:  | = 20. Consequently, a\n",
      "news article\n",
      "ð§© Token 1179:  to which the LLM assigns sT â\n",
      "ð§© Token 1180:  ST , sM â SM , sD\n",
      "ð§© Token 1181:  â SD will belong to cluster (sT\n",
      "ð§© Token 1182:  , sM , sD) â\n",
      "\n",
      "ð§© Token 1183: GLLM . Formally, the set of\n",
      "ð§© Token 1184:  all possible clusters is deï¬ned as\n",
      "ð§© Token 1185: :\n",
      "GLLM := {(sT\n",
      "ð§© Token 1186:  , sM , sD) | sT\n",
      "ð§© Token 1187:  â ST , sM â SM ,\n",
      "ð§© Token 1188:  sD â SD} ,\n",
      "and each\n",
      "ð§© Token 1189:  cluster can then be mapped to a positive integer as\n",
      "ð§© Token 1190:  GLLM â {k â N0\n",
      "ð§© Token 1191:  | 0 â¤ k â¤ 19}. A\n",
      "represent\n",
      "ð§© Token 1192: ative sample of 3 articles from each cluster is provided\n",
      "ð§© Token 1193:  in Appendix Table A2.\n",
      "In Figure 7\n",
      "ð§© Token 1194:  we plot the distribution of news articles through clusters.\n",
      "ð§© Token 1195:  As we can see, most articles are\n",
      "ass\n",
      "ð§© Token 1196: igned to clusters 8, 9, 10, and\n",
      "ð§© Token 1197:  11, which are the clusters referred to ï¿½\n",
      "ð§© Token 1198: ï¿½nancial events or shocks. Such\n",
      "cl\n",
      "ð§© Token 1199: usters are mostly composed of articles about the publication of\n",
      "ð§© Token 1200:  quarterly and semiannual results. More\n",
      "spe\n",
      "ð§© Token 1201: ciï¬cally, cluster 8 (\n",
      "ð§© Token 1202: ï¬nancial, minor, positive)\n",
      "ð§© Token 1203:  concentrates around 1/3 of the sample and\n",
      "ð§© Token 1204:  is associated\n",
      "to the publication of results that mildly\n",
      "ð§© Token 1205:  surpass the expectations of investors, hence, making this\n",
      "ð§© Token 1206:  cluster\n",
      "a good candidate for a long trading signal\n",
      "ð§© Token 1207: .\n",
      "On the other hand, other clusters such\n",
      "ð§© Token 1208:  as 16 (policy, minor, positive) and\n",
      "ð§© Token 1209:  0 (demand, minor, positive)\n",
      "also\n",
      "ð§© Token 1210:  concentrate a big share of news. Note that no\n",
      "ð§© Token 1211:  cluster has been assigned to cluster 13 (technology,\n",
      "ð§© Token 1212: \n",
      "minor, negative). Compared to KMe\n",
      "ð§© Token 1213: ans clustering with embeddings, the distribution\n",
      "ð§© Token 1214:  of articles across\n",
      "these reï¬ned\n",
      "ð§© Token 1215:  clusters is now remarkably stable across diï¬\n",
      "ð§© Token 1216: erent data splits. This consistency indicates\n",
      "that clust\n",
      "ð§© Token 1217: ering based on a thorough analysis of the shocks implied\n",
      "ð§© Token 1218:  by each article for the aï¬ected\n",
      "ð§© Token 1219:  ï¬rms\n",
      "yields a\n",
      "ð§© Token 1220:  robust, time-invariant categorization.\n",
      "ð§© Token 1221:  This is an encouraging ï¬nding for\n",
      "ð§© Token 1222:  subsequent research and\n",
      "applications.\n",
      "[\n",
      "ð§© Token 1223: Insert Figure 7 about here]\n",
      "5. Trading\n",
      "ð§© Token 1224:  Strategy\n",
      "5.1 Beta-neutral positions on\n",
      "ð§© Token 1225:  every (i, j) â B\n",
      "\n",
      "ð§© Token 1226: Since we are interested in the individual eï¿½\n",
      "ð§© Token 1227: ï¿½ect of an article i â D in\n",
      "ð§© Token 1228:  each of the aï¬ected ï¿½\n",
      "ð§© Token 1229: ï¿½rms j â Fi,\n",
      "we\n",
      "ð§© Token 1230:  work with the set B := ï¿½\n",
      "ð§© Token 1231: ï¿½\n",
      "(i, j) | i ï¿½\n",
      "ð§© Token 1232: ï¿½ D â§ j â Fiï¿½\n",
      "ð§© Token 1233: ï¿½ï¿½\n",
      ", where |B| = 34\n",
      "ð§© Token 1234: 10 > |D| = 2613. We\n",
      "ð§© Token 1235:  then ï¬t a\n",
      "16\n",
      "ð§© Token 1236: market model to each unique pair (i, j\n",
      "ð§© Token 1237: ) â B on a lookback window of\n",
      "ð§© Token 1238:  100 days with a buï¬er of\n",
      "ð§© Token 1239:  10 days\n",
      "before the eï¬ective\n",
      "ð§© Token 1240:  treatment date Ëdi\n",
      "0.\n",
      "r\n",
      "ð§© Token 1241: j\n",
      "d = Î±(i,j)\n",
      "ð§© Token 1242:  + Î²(i,j)rM\n",
      "\n",
      "ð§© Token 1243: d + ó°(i,\n",
      "ð§© Token 1244: j)\n",
      "d ,\n",
      "where rj\n",
      "\n",
      "ð§© Token 1245: d denotes the return of ï¬rm j\n",
      "ð§© Token 1246:  at trading day d in excess of the risk-\n",
      "ð§© Token 1247: free asset, which we take to\n",
      "be the\n",
      "ð§© Token 1248:  daily euro short-term rate (eSTR),\n",
      "ð§© Token 1249:  and rM\n",
      "d denotes the excess return of\n",
      "ð§© Token 1250:  the market (IBEX-35).\n",
      "These\n",
      "ð§© Token 1251:  returns are obtained from adjusted close prices, which correct\n",
      "ð§© Token 1252:  the price evolution for corporate\n",
      "actions such as dividends\n",
      "ð§© Token 1253: , stock splits, and new stock issuance. The\n",
      "ð§© Token 1254:  notation overload in the regression\n",
      "coeï¬\n",
      "ð§© Token 1255: cients (Î±(i,j), Î²(\n",
      "ð§© Token 1256: i,j)) emphasizes the fact that Î± and\n",
      "ð§© Token 1257:  Î² are speciï¬c to each\n",
      "ð§© Token 1258:  pair (i, j) â B since\n",
      "ð§© Token 1259:  the\n",
      "market model is computed for each ï¿½\n",
      "ð§© Token 1260: ï¿½rm j â FIBEX-35\n",
      "ð§© Token 1261:  on a lookback window of time which is particular\n",
      "ð§© Token 1262: \n",
      "to each article i â D.\n",
      "\n",
      "ð§© Token 1263: The reason why we ï¬t a market\n",
      "ð§© Token 1264:  model to each (i, j) â\n",
      "ð§© Token 1265:  B is to then apply a market-neutral strategy\n",
      "ð§© Token 1266: \n",
      "as in [26] and [35].\n",
      "ð§© Token 1267:  This is an investment approach designed to minimize or eliminate\n",
      "ð§© Token 1268:  exposure to overall\n",
      "market movements, isolating the\n",
      "ð§© Token 1269:  performance of a speciï¬c ï¿½\n",
      "ð§© Token 1270: ï¿½ï¿½rm. In particular, we employ a\n",
      "ð§© Token 1271:  beta-neutral\n",
      "strategy by buying one unit\n",
      "ð§© Token 1272:  of ï¬rm jâs stock\n",
      "ð§© Token 1273:  and shorting Î²(i,j) units\n",
      "ð§© Token 1274:  of the market index (i.e.: an\n",
      "ð§© Token 1275:  ETF\n",
      "replicating the IBEX-35).\n",
      "ð§© Token 1276:  This hedged position harvests the idiosyncratic\n",
      "ð§© Token 1277:  returns from the market model\n",
      "and it only makes\n",
      "ð§© Token 1278:  sense when ï¬rm jâs\n",
      "ð§© Token 1279:  returns are expected to outperform or underperform\n",
      "ð§© Token 1280:  the market.3\n",
      "The position delivers abnormal returns\n",
      "ð§© Token 1281:  AR(i,j)\n",
      "d at some\n",
      "ð§© Token 1282:  trading day d â¥ Ëdi\n",
      "0 given\n",
      "ð§© Token 1283:  by\n",
      "rj\n",
      "d â Î²(i\n",
      "ð§© Token 1284: ,j)rM\n",
      "d = Î±(\n",
      "ð§© Token 1285: i,j) + ó°\n",
      "ð§© Token 1286: (i,j)\n",
      "d =: AR\n",
      "ð§© Token 1287: (i,j)\n",
      "d .\n",
      "The\n",
      "ð§© Token 1288:  position is taken at the eï¬ective\n",
      "ð§© Token 1289:  treatment date Ëdi\n",
      "0 and is maintained\n",
      "ð§© Token 1290:  over a holding window consisting\n",
      "of L â\n",
      "ð§© Token 1291:  N trading days after Ëdi\n",
      "0,\n",
      "ð§© Token 1292:  where L is set to 4 trading days. The\n",
      "ð§© Token 1293:  justiï¬cation for this choice of\n",
      "ð§© Token 1294:  L\n",
      "results from the maximization of the Shar\n",
      "ð§© Token 1295: pe Ratio of the portfolio in the train and validation\n",
      "ð§© Token 1296:  samples for\n",
      "both KMeans and LLM\n",
      "ð§© Token 1297: -based clustering.4 Finally, we compute\n",
      "ð§© Token 1298:  the Sharpe Ratio of each position SR(i\n",
      "ð§© Token 1299: ,j),\n",
      "which we will subsequently employ to\n",
      "ð§© Token 1300:  optimize cluster selection.\n",
      "5.2 Optimal\n",
      "ð§© Token 1301:  Cluster Selection\n",
      "After taking beta-neutral positions on\n",
      "ð§© Token 1302:  each pair (i, j) â B\n",
      "ð§© Token 1303:  and holding them over L days, we can obtain\n",
      "ð§© Token 1304: \n",
      "a measure of how proï¬table\n",
      "ð§© Token 1305:  the positions are on average for articles that belong to\n",
      "ð§© Token 1306:  the same cluster. For\n",
      "this purpose, let\n",
      "ð§© Token 1307:  Bg denote the set of all article-ï¿½\n",
      "ð§© Token 1308: ï¿½ï¿½rm pairs such that the article belongs to\n",
      "ð§© Token 1309:  some cluster\n",
      "g â G.\n",
      "B\n",
      "ð§© Token 1310: g := {(i, j) | (\n",
      "ð§© Token 1311: i, j) â B â§ i\n",
      "ð§© Token 1312:  â Dg}.\n",
      "3 For expected under\n",
      "ð§© Token 1313: performance of ï¬rm j, reverse the\n",
      "ð§© Token 1314:  beta-neutral positions: sell one unit of ï¿½\n",
      "ð§© Token 1315: ï¿½ï¿½rm j and buy Î²(i,\n",
      "ð§© Token 1316: j) units\n",
      "of the market index. However\n",
      "ð§© Token 1317: , note that this will be handled later by a\n",
      "ð§© Token 1318:  Trading Rule (T R).\n",
      "4 The choice\n",
      "ð§© Token 1319:  of L is justiï¬ed in\n",
      "ð§© Token 1320:  detail in A.2, and the sensitivity of\n",
      "ð§© Token 1321:  the trading strategyâs out-of-\n",
      "ð§© Token 1322: sample performance to\n",
      "diï¬erent values\n",
      "ð§© Token 1323:  of L is examined in Section 6 (â\n",
      "ð§© Token 1324: Robustness Checksâ).\n",
      "17\n",
      "ð§© Token 1325: The average Sharpe Ratio associated to each cluster is\n",
      "ð§© Token 1326: \n",
      "SRg = 1\n",
      "|Bg|\n",
      "ð§© Token 1327: \n",
      "ó°\n",
      "(i,j\n",
      "ð§© Token 1328: )âBg\n",
      "SR(i,\n",
      "ð§© Token 1329: j),\n",
      "and it provides a measure of the\n",
      "ð§© Token 1330:  performance of the beta-neutral positions in each cluster\n",
      "ð§© Token 1331: . The distribution\n",
      "of cluster-average Sharpe\n",
      "ð§© Token 1332:  Ratios across the diï¬erent clusters\n",
      "ð§© Token 1333:  is shown in Appendix Figure A5.\n",
      "We\n",
      "ð§© Token 1334:  then focus on developing two algorithms that optimally leverage\n",
      "ð§© Token 1335:  the cluster information for our\n",
      "trading strategy.\n",
      "ð§© Token 1336:  Our approach draws parallels with traditional portfolio sorting methods,\n",
      "ð§© Token 1337:  where assets\n",
      "are typically arranged into deciles based\n",
      "ð§© Token 1338:  on speciï¬c characteristics, and\n",
      "ð§© Token 1339:  trading positions are established\n",
      "by going long on top\n",
      "ð§© Token 1340:  deciles and short on bottom ones. Similarly,\n",
      "ð§© Token 1341:  our strategy will construct self-ï¬n\n",
      "ð§© Token 1342: ancing\n",
      "portfolios based on clusters rather than individual\n",
      "ð§© Token 1343:  assets: taking long positions in clusters expected to\n",
      "\n",
      "ð§© Token 1344: outperform and short positions in those expected to\n",
      "ð§© Token 1345:  underperform. To identify the optimal clusters for\n",
      "ð§© Token 1346: \n",
      "trading, we propose two distinct algorithmic\n",
      "ð§© Token 1347:  approaches. The ï¬rst approach,\n",
      "ð§© Token 1348:  which we term âgreedyâ,\n",
      "ð§© Token 1349: \n",
      "selects clusters by maximizing the Sharpe Ratio\n",
      "ð§© Token 1350:  within the validation dataset. The second approach,\n",
      "\n",
      "ð§© Token 1351: termed âstableâ, utilizes a\n",
      "ð§© Token 1352:  broader information set by incorporating both training and validation data\n",
      "ð§© Token 1353: ,\n",
      "aiming to identify clusters that maintain consistent\n",
      "ð§© Token 1354:  performance across both splits. In both algorithms, we\n",
      "ð§© Token 1355: \n",
      "impose sign restrictions to ensure that our trading\n",
      "ð§© Token 1356:  positions align with the expected direction of returns.\n",
      "\n",
      "ð§© Token 1357: 5.2.1 Greedy Algorithm\n",
      "\n",
      "ð§© Token 1358: The greedy selection of clusters is done in the validation\n",
      "ð§© Token 1359:  sample Bval := {(i, j)\n",
      "ð§© Token 1360:  â B | i â Dval}\n",
      "ð§© Token 1361:  , from\n",
      "where we compute the cluster-average\n",
      "ð§© Token 1362:  SRval\n",
      "g for each g â G\n",
      "ð§© Token 1363: . Deï¬ne Gval\n",
      "SR\n",
      "ð§© Token 1364: + := {g â G | SRval\n",
      "ð§© Token 1365: \n",
      "g > 0} and\n",
      "Gval\n",
      "\n",
      "ð§© Token 1366: SRâ := {g â G | SR\n",
      "ð§© Token 1367: val\n",
      "g < 0} as the sets of\n",
      "ð§© Token 1368:  clusters with positive and negative Sharpe Ratios in\n",
      "ð§© Token 1369:  the\n",
      "validation sample. Obviously, we will\n",
      "ð§© Token 1370:  be interested in taking long positions when reading an article\n",
      "ð§© Token 1371:  that\n",
      "is clustered in some g â G\n",
      "ð§© Token 1372: val\n",
      "SR+ , and short positions in clusters\n",
      "ð§© Token 1373:  g â Gval\n",
      "SR+ . However\n",
      "ð§© Token 1374: , our trading strategy\n",
      "will not trade every cluster\n",
      "ð§© Token 1375:  g â G. Instead, it will select\n",
      "ð§© Token 1376:  the clusters from GSR+ and GSRâ\n",
      "ð§© Token 1377:  that lead the\n",
      "to most proï¬\n",
      "ð§© Token 1378: table trades. To identify such clusters, we rank\n",
      "ð§© Token 1379:  them by their average Sharpe Ratio. Deï¿½\n",
      "ð§© Token 1380: ï¿½ï¿½ne\n",
      "the ranking function R : G\n",
      "ð§© Token 1381:  â {1, . . . , kï¿½\n",
      "ð§© Token 1382: ï¿½} such that\n",
      "Rval\n",
      "g =\n",
      "ð§© Token 1383: \n",
      "ó°\n",
      "hâG\n",
      "ð§© Token 1384: \n",
      "1\n",
      "ó°\n",
      "SRval\n",
      "ð§© Token 1385: \n",
      "h â¥ SRval\n",
      "g\n",
      "ï¿½\n",
      "ð§© Token 1386: ï¿½ï¿½\n",
      ",\n",
      "where 1(Â· )\n",
      "ð§© Token 1387:  is the indicator function which equals 1 if the condition\n",
      "ð§© Token 1388:  inside is true and 0 otherwise.\n",
      "The number\n",
      "ð§© Token 1389:  of traded clusters on either side (long and short\n",
      "ð§© Token 1390: ) will be upper-bounded by some hyper\n",
      "ð§© Token 1391: -\n",
      "parameter of our choice Î¸ ï¿½\n",
      "ð§© Token 1392: ï¿½ N which we set proportional to the number of\n",
      "ð§© Token 1393:  clusters. Namely, Î¸ = ï¿½\n",
      "ð§© Token 1394: ï¿½Ïkâ\n",
      "for some ï¿½\n",
      "ð§© Token 1395: ï¿½ â (0, 1), which has\n",
      "ð§© Token 1396:  been set to Ï = 0.5 to\n",
      "ð§© Token 1397:  maximize the Sharpe Ratio of the trading strategy\n",
      "\n",
      "ð§© Token 1398: in the training and validation samples 5. The actual\n",
      "ð§© Token 1399:  number of traded clusters will not be exactly Î¸\n",
      "ð§© Token 1400:  as\n",
      "5 The choice of Î¸ is just\n",
      "ð§© Token 1401: iï¬ed in detail in A.\n",
      "ð§© Token 1402: 2. The sensitivity of the trading strategyâ\n",
      "ð§© Token 1403: s out-of-sample performance to\n",
      "di\n",
      "ð§© Token 1404: ï¬erent values of Î¸ is examined\n",
      "ð§© Token 1405:  in Section 6 (âRobustness Checks\n",
      "ð§© Token 1406: â).\n",
      "18\n",
      "ð§© Token 1407: there is a natural bound coming from the cardinalities\n",
      "ð§© Token 1408:  of GSR+ and GSRâ. Hence\n",
      "ð§© Token 1409: , the actual number of\n",
      "long and short-\n",
      "ð§© Token 1410: traded clusters will be Î¸+ := min\n",
      "ð§© Token 1411: (Î¸, |GSR+ |)\n",
      "ð§© Token 1412:  and Î¸â := min(Î¸,\n",
      "ð§© Token 1413:  |GSRâ|). The set of traded\n",
      "ð§© Token 1414: \n",
      "clusters GÎ¸ is deï¿½\n",
      "ð§© Token 1415: ï¿½ned as\n",
      "GÎ¸ :=\n",
      "ï¿½\n",
      "ð§© Token 1416: ï¿½ï¿½ï¿½\n",
      "g â G | 1\n",
      "ð§© Token 1417:  â¤ Rval\n",
      "g â¤ Î¸+ ï¿½\n",
      "ð§© Token 1418: ï¿½ kâ â Î¸â < R\n",
      "ð§© Token 1419: val\n",
      "g â¤ kâ\n",
      "ï¿½\n",
      "ð§© Token 1420: ï¿½ï¿½\n",
      "= G+\n",
      "Î¸ ï¿½\n",
      "ð§© Token 1421: ï¿½ Gâ\n",
      "Î¸ ,\n",
      "where G\n",
      "ð§© Token 1422: +\n",
      "Î¸ := {g â G\n",
      "ð§© Token 1423:  | 1 â¤ Rval\n",
      "g â¤ Î¸\n",
      "ð§© Token 1424: +} is the set of long-traded\n",
      "ð§© Token 1425:  clusters, Gâ\n",
      "Î¸ := {g\n",
      "ð§© Token 1426:  â G | kâ â Î¸\n",
      "ð§© Token 1427: â < Rval\n",
      "g â¤\n",
      "kï¿½\n",
      "ð§© Token 1428: ï¿½} is the set of short-traded\n",
      "ð§© Token 1429:  clusters and, clearly, |GÎ¸|\n",
      "ð§© Token 1430:  = Î¸+ + Î¸â.6\n",
      "ð§© Token 1431:  In Appendix Algorithm 2., we can\n",
      "ï¿½\n",
      "ð§© Token 1432: ï¿½ï¿½nd the formal design of this algorithm.\n",
      "ð§© Token 1433: \n",
      "5.2.2 Stable Algorithm\n",
      "ð§© Token 1434: \n",
      "In this case, we prioritize the stability of\n",
      "ð§© Token 1435:  the cluster rankings by ensuring that the traded clusters\n",
      "\n",
      "ð§© Token 1436: minimize the rank diï¬erence of\n",
      "ð§© Token 1437:  the cluster-average Sharpe Ratios between the\n",
      "ð§© Token 1438:  training and validation\n",
      "samples. To begin,\n",
      "ð§© Token 1439:  we compute the rank of each cluster based on the\n",
      "ð§© Token 1440:  average Sharpe Ratios in both the\n",
      "training\n",
      "ð§© Token 1441:  and validation samples. This delivers {Rtr\n",
      "\n",
      "ð§© Token 1442: g }gâG and {Rval\n",
      "ð§© Token 1443: \n",
      "g }gâG, which provides\n",
      "ð§© Token 1444:  a measure of the\n",
      "relative performance of the clusters\n",
      "ð§© Token 1445:  within each sample.\n",
      "Next, we calculate the\n",
      "ð§© Token 1446:  absolute diï¬erence in ranks between the\n",
      "ð§© Token 1447:  training and validation samples for\n",
      "each cluster, which\n",
      "ð§© Token 1448:  allows us to measure the stability of each clusterï¿½\n",
      "ð§© Token 1449: ï¿½s performance between the two\n",
      "samples\n",
      "\n",
      "ð§© Token 1450: Î´g := |Rtr\n",
      "g â\n",
      "ð§© Token 1451:  Rval\n",
      "g | .\n",
      "Clusters are\n",
      "ð§© Token 1452:  then sorted based on their rank diï¬\n",
      "ð§© Token 1453: erences Î´g in descending order. To do\n",
      "ð§© Token 1454:  this, we can\n",
      "simply compute the ranking\n",
      "ð§© Token 1455:  of the ranking diï¬erences as\n",
      "\n",
      "ð§© Token 1456: R(Î´g) :=\n",
      "ï¿½\n",
      "ð§© Token 1457: ï¿½ï¿½\n",
      "hâG\n",
      "1 (\n",
      "ð§© Token 1458: Î´g â¥ Î´h) .\n",
      "\n",
      "ð§© Token 1459: Next, we select the top 2Î¸ ï¿½\n",
      "ð§© Token 1460: ï¿½ N clusters with the smallest rank diï¿½\n",
      "ð§© Token 1461: ï¿½erences, indicating the most stable\n",
      "clusters\n",
      "ð§© Token 1462:  across the training and validation samples. The selected clusters\n",
      "ð§© Token 1463:  now are\n",
      "GÎ¸ = {g ï¿½\n",
      "ð§© Token 1464: ï¿½ G | 1 â¤ R(Î´g\n",
      "ð§© Token 1465: ) â¤ 2Î¸} .\n",
      "Finally,\n",
      "ð§© Token 1466:  we determine the sets of long and short-tr\n",
      "ð§© Token 1467: aded clusters based on the average Sharpe Ratios\n",
      "ð§© Token 1468: \n",
      "in both the training and validation samples. In\n",
      "ð§© Token 1469:  particular, the set of long-traded clusters\n",
      "ð§© Token 1470:  (G+\n",
      "Î¸ ) are the\n",
      "\n",
      "ð§© Token 1471: ones that have positive average Sharpe Ratios in\n",
      "ð§© Token 1472:  both, training and validation samples\n",
      "G+\n",
      "\n",
      "ð§© Token 1473: Î¸ = {g â GÎ¸\n",
      "ð§© Token 1474:  | SRtr\n",
      "g > 0 â§ SR\n",
      "ð§© Token 1475: val\n",
      "g > 0},\n",
      "6 Alternatively,\n",
      "ð§© Token 1476:  we could trade the same number of clusters in the\n",
      "ð§© Token 1477:  long and short side by deï¬ning\n",
      "ð§© Token 1478:  a unique Î¸â :=\n",
      "min (\n",
      "ð§© Token 1479: Î¸, |GSR+|, |\n",
      "ð§© Token 1480: GSRâ|) such that GÎ¸\n",
      "ð§© Token 1481:  :=\n",
      "ó°g â G\n",
      "ð§© Token 1482:  | 1 â¤ Rval\n",
      "g â¤ Î¸\n",
      "ð§© Token 1483: â â¨ kâ â Î¸\n",
      "ð§© Token 1484: â < Rval\n",
      "g â¤ kï¿½\n",
      "ð§© Token 1485: ï¿½ó° and |GÎ¸\n",
      "ð§© Token 1486: | = 2Î¸â.\n",
      "19\n",
      "ð§© Token 1487: and by symmetry, short-traded clusters (\n",
      "ð§© Token 1488: Gâ\n",
      "Î¸ ) are the ones that\n",
      "ð§© Token 1489:  have negative average Sharpe Ratios in\n",
      "both\n",
      "ð§© Token 1490: , training and validation samples\n",
      "Gâ\n",
      "ï¿½\n",
      "ð§© Token 1491: ï¿½ = {g â GÎ¸ |\n",
      "ð§© Token 1492:  SRtr\n",
      "g < 0 â§ SRval\n",
      "ð§© Token 1493: \n",
      "g < 0} .\n",
      "This approach ensures\n",
      "ð§© Token 1494:  that we select the most stable clusters for trading,\n",
      "ð§© Token 1495:  reducing the risk associated\n",
      "with rank variability between the\n",
      "ð§© Token 1496:  training and validation samples, and ensuring that the direction\n",
      "ð§© Token 1497:  of\n",
      "the signal is consistent across the two splits\n",
      "ð§© Token 1498: . The ï¬nal output consists of\n",
      "ð§© Token 1499:  the sets of long-traded and\n",
      "short\n",
      "ð§© Token 1500: -traded clusters, which are then used to\n",
      "ð§© Token 1501:  implement the trading strategy. The implementation of the\n",
      "\n",
      "ð§© Token 1502: algorithm is methodically presented in Appendix Algorithm\n",
      "ð§© Token 1503:  3.\n",
      "[Insert Table 3 about here]\n",
      "ð§© Token 1504: \n",
      "In Table 3 we show the 26 clusters with\n",
      "ð§© Token 1505:  their proposed names (based on the articles they pool\n",
      "ð§© Token 1506:  together\n",
      "as shown in Appendix Table A1)\n",
      "ð§© Token 1507:  and the selection of long and short-traded\n",
      "ð§© Token 1508:  clusters according to each\n",
      "algorithm: â\n",
      "ð§© Token 1509: greedyâ and âstableâ\n",
      "ð§© Token 1510: . We write âlongâ for those\n",
      "ð§© Token 1511:  clusters g â G+\n",
      "Î¸ and\n",
      "ð§© Token 1512:  âshortâ for g â G\n",
      "ð§© Token 1513: â\n",
      "Î¸ .\n",
      "As we can see\n",
      "ð§© Token 1514: , trading clusters of news articles based on this procedure\n",
      "ð§© Token 1515:  is quite risky, as there is a high\n",
      "\n",
      "ð§© Token 1516: reliance of the signal on the past performance of\n",
      "ð§© Token 1517:  a cluster. For example, clusters 21 and 22\n",
      "ð§© Token 1518:  are linked to\n",
      "the ï¬nancial\n",
      "ð§© Token 1519:  performance of Repsol and Aena, respectively,\n",
      "ð§© Token 1520:  during the training and validation samples.\n",
      "Evidently\n",
      "ð§© Token 1521: , the future performance of these ï¬r\n",
      "ð§© Token 1522: ms can change, but the signal provided by the\n",
      "ð§© Token 1523:  algorithm\n",
      "will still indicate âlongâ\n",
      "ð§© Token 1524: . Additionally, some clusters are heavily built on spe\n",
      "ð§© Token 1525: ciï¬c events of the period\n",
      "\n",
      "ð§© Token 1526: of time they were constructed upon. For example,\n",
      "ð§© Token 1527:  cluster 17 pools articles related to the challenges of\n",
      "\n",
      "ð§© Token 1528: the tourism industry in Spain in Covid times,\n",
      "ð§© Token 1529:  and cluster 25 is related to the post-c\n",
      "ð§© Token 1530: ovid developments of\n",
      "Inditex and Acer\n",
      "ð§© Token 1531: inox. Thus, a clustering approach based\n",
      "ð§© Token 1532:  on embeddings is not generalizable over time\n",
      "ð§© Token 1533: . As\n",
      "the world evolves, clusters become outdated\n",
      "ð§© Token 1534:  and require constant recalibration to maintain their relevance\n",
      "ð§© Token 1535: \n",
      "and predictive power. Hence, any trading strategy\n",
      "ð§© Token 1536:  based solely on historical cluster performance is likely\n",
      "to\n",
      "ð§© Token 1537:  produce misguided trading signals over time\n",
      "[Insert Table\n",
      "ð§© Token 1538:  4 about here]\n",
      "In contrast, our LL\n",
      "ð§© Token 1539: M-based clustering methodology oï¬\n",
      "ð§© Token 1540: ers signiï¬cant advantages by\n",
      "ð§© Token 1541:  focusing on the\n",
      "fundamental nature of economic shocks\n",
      "ð§© Token 1542:  rather than historical patterns. This approach provides more\n",
      "\n",
      "ð§© Token 1543: robust and generalizable signals that are less susceptible\n",
      "ð§© Token 1544:  to temporal changes in market conditions. More-\n",
      "\n",
      "ð§© Token 1545: over, unlike the black box nature of vector embed\n",
      "ð§© Token 1546: dings, our methodology oï¬ers\n",
      "ð§© Token 1547:  transparency and inter-\n",
      "pretability in signal generation\n",
      "ð§© Token 1548: . This is evident in how the Greedy algorithm\n",
      "ð§© Token 1549: âs cluster selection closely\n",
      "aligns with\n",
      "ð§© Token 1550:  the direction of economic shocks: negative shocks typically correspond\n",
      "ð§© Token 1551:  to price decreases and\n",
      "positive shocks to increases.\n",
      "ð§© Token 1552: \n",
      "Looking at Table 4, we observe that both\n",
      "ð§© Token 1553:  algorithms consistently short articles classiï¬ed\n",
      "ð§© Token 1554:  as policy\n",
      "shocks, regardless of direction,\n",
      "ð§© Token 1555:  while going long on cluster 8, which contains approximately\n",
      "ð§© Token 1556:  one-third\n",
      "20\n",
      "ð§© Token 1557: of news articles (those categorized as undergoing ï¿½\n",
      "ð§© Token 1558: ï¿½nancial minor and positive shocks). This consistent\n",
      "ð§© Token 1559: \n",
      "shorting of policy shocks likely reï¿½\n",
      "ð§© Token 1560: ï¿½ects marketsâgeneral aversion to policy\n",
      "ð§© Token 1561:  uncertainty, as policy changes\n",
      "âeven positive ones\n",
      "ð§© Token 1562: â often create implementation uncertainty and take time for market\n",
      "ð§© Token 1563:  participants to\n",
      "fully price in. Interestingly, both\n",
      "ð§© Token 1564:  algorithms also exhibit seemingly counter-intuitive behavior by going\n",
      "ð§© Token 1565: \n",
      "long on negative major demand shocks and short on\n",
      "ð§© Token 1566:  positive major demand shocks. This pattern might\n",
      "suggest\n",
      "ð§© Token 1567:  a âmean reversionâ expectation in\n",
      "ð§© Token 1568:  the algorithms, where major demand shocks are viewed as\n",
      "ð§© Token 1569: \n",
      "temporary deviations that will eventually correct: negative\n",
      "ð§© Token 1570:  shocks present buying opportunities, while\n",
      "positive shocks signal\n",
      "ð§© Token 1571:  potential overvaluation.\n",
      "5.3 Trading\n",
      "ð§© Token 1572:  Rule & Portfolio Construction\n",
      "For a given selection\n",
      "ð§© Token 1573:  of clusters G+\n",
      "Î¸ and Gâ\n",
      "ð§© Token 1574: \n",
      "Î¸ , we launch trades and hold them\n",
      "ð§© Token 1575:  for L = 4 trading days.\n",
      "Formally\n",
      "ð§© Token 1576: , the trading rule for a pair (i,\n",
      "ð§© Token 1577:  j) â B at trading day d ï¿½\n",
      "ð§© Token 1578: ï¿½ Ëd is\n",
      "T RL,ï¿½\n",
      "ð§© Token 1579: ï¿½ã(i, j), dï¿½\n",
      "ð§© Token 1580: ï¿½ :=\n",
      "ó°»\n",
      "ï¿½\n",
      "ð§© Token 1581: ï¿½ï¿½ó°ó°¿\n",
      "ð§© Token 1582: \n",
      "ó°ó°ï¿½\n",
      "ð§© Token 1583: ï¿½ï¿½ï¿½\n",
      "+1 if [(i,\n",
      "ð§© Token 1584:  j) â Bg â§ g ï¿½\n",
      "ð§© Token 1585: ï¿½ G+\n",
      "Î¸ ] â§ d\n",
      "ð§© Token 1586:  â ( Ëdi\n",
      "0, \n",
      "ð§© Token 1587: Ëdi\n",
      "0 + L]\n",
      "0 if\n",
      "ð§© Token 1588:  [(i, j) â Bg ï¿½\n",
      "ð§© Token 1589: ï¿½ g ââ GÎ¸ ]\n",
      "ð§© Token 1590:  â¨ d ââ ( Ë\n",
      "ð§© Token 1591: di\n",
      "0, Ëdi\n",
      "0 +\n",
      "ð§© Token 1592:  L]\n",
      "â1 if [(i, j\n",
      "ð§© Token 1593: ) â Bg â§ g â\n",
      "ð§© Token 1594:  Gâ\n",
      "Î¸ ] â§ d ï¿½\n",
      "ð§© Token 1595: ï¿½ ( Ëdi\n",
      "0, Ë\n",
      "ð§© Token 1596: di\n",
      "0 + L]\n",
      ".\n",
      "In\n",
      "ð§© Token 1597:  this context, a portfolio is a collection of positions\n",
      "ð§© Token 1598:  taken in a ï¬rmâs\n",
      "ð§© Token 1599:  stocks according to T RL,Î¸ã\n",
      "ð§© Token 1600: (i, j), dã.\n",
      "\n",
      "ð§© Token 1601: In other words, it is the set of all\n",
      "ð§© Token 1602:  ã(i, j), dï¿½\n",
      "ð§© Token 1603: ï¿½ for which a trade is executed.\n",
      "P\n",
      "ð§© Token 1604:  := ó°\n",
      "ã(\n",
      "ð§© Token 1605: i, j), dã ï¿½\n",
      "ð§© Token 1606: ï¿½ï¿½ó° (i, j\n",
      "ð§© Token 1607: ) â B â§ d â \n",
      "ð§© Token 1608: Ëd â§ T RL,Î¸ï¿½\n",
      "ð§© Token 1609: ï¿½(i, j), dã ï¿½\n",
      "ð§© Token 1610: ï¿½= 0ó°\n",
      ".\n",
      "\n",
      "ð§© Token 1611: The set of open positions on a particular day d\n",
      "ð§© Token 1612:  â Ëd is deï¬\n",
      "ð§© Token 1613: ned as\n",
      "Pd := {(i,\n",
      "ð§© Token 1614:  j) â B | T RL,ï¿½\n",
      "ð§© Token 1615: ï¿½ã(i, j), dï¿½\n",
      "ð§© Token 1616: ï¿½ â= 0} ,\n",
      "and the\n",
      "ð§© Token 1617:  portfolio is rebalanced every day, so each\n",
      "ð§© Token 1618:  position (i, j) â Pd\n",
      "ð§© Token 1619:  receives a weight that is inversely\n",
      "proport\n",
      "ð§© Token 1620: ional to the total amount of open positions in that\n",
      "ð§© Token 1621:  day (i.e. 1/|P\n",
      "ð§© Token 1622: d|).7 This produces an equally-\n",
      "\n",
      "ð§© Token 1623: weighted rolling-portfolio similar to [36\n",
      "ð§© Token 1624: ] and [26]. The overlapping returns of the\n",
      "ð§© Token 1625:  portfolio at d â Ëd can\n",
      "\n",
      "ð§© Token 1626: be obtained as an average of the abnormal returns weighted\n",
      "ð§© Token 1627:  by the trading rule, which determines the\n",
      "direction\n",
      "ð§© Token 1628:  of each position (long or short), and scaled\n",
      "ð§© Token 1629:  by the number of open positions in that day,\n",
      "ð§© Token 1630: \n",
      "rP\n",
      "d := 1\n",
      "|P\n",
      "ð§© Token 1631: d|\n",
      "ó°\n",
      "(i\n",
      "ð§© Token 1632: ,j),âPd\n",
      "T RL\n",
      "ð§© Token 1633: ,Î¸ã(i, j),\n",
      "ð§© Token 1634:  dã Â· AR(i,j)\n",
      "ð§© Token 1635: \n",
      "d .\n",
      "In Figure 8 we plot the\n",
      "ð§© Token 1636:  cumulative gross returns of trading strategies based on KMe\n",
      "ð§© Token 1637: ans clustering\n",
      "(Panel A) and LL\n",
      "ð§© Token 1638: M clustering (Panel B) across diï¿½\n",
      "ð§© Token 1639: ï¿½ï¿½erent data splits\n",
      "7 Note that the\n",
      "ð§© Token 1640:  cardinality of the set of open positions at day\n",
      "ð§© Token 1641:  d â Ëd, denoted as\n",
      "ð§© Token 1642:  |Pd|, can be computed as the\n",
      "ð§© Token 1643:  sum of the\n",
      "absolute values of the trading rule\n",
      "ð§© Token 1644:  over all pairs (i, j) â\n",
      "ð§© Token 1645:  B for a given trading day d â \n",
      "ð§© Token 1646: Ëd.\n",
      "|Pd| =\n",
      "\n",
      "ð§© Token 1647: ó°\n",
      "(i,j)\n",
      "ð§© Token 1648: âB\n",
      "|T RL,Î¸\n",
      "ð§© Token 1649: ã(i, j), dã\n",
      "ð§© Token 1650: | .\n",
      "21\n",
      "ð§© Token 1651: [Insert Figure 8 about here]\n",
      "KMe\n",
      "ð§© Token 1652: ans. In panel A of Table 5 we show\n",
      "ð§© Token 1653:  the portfolio statistics of the benchmark model. As we\n",
      "ð§© Token 1654:  can\n",
      "see, both algorithms work well on the\n",
      "ð§© Token 1655:  data splits they were trained on: the Stable\n",
      "ð§© Token 1656:  algorithm works well on\n",
      "both, training and validation\n",
      "ð§© Token 1657:  data, while the Greedy algorithm does a good\n",
      "ð§© Token 1658:  job only on validation data as\n",
      "expected. However\n",
      "ð§© Token 1659: , this doesnât say anything about any\n",
      "ð§© Token 1660:  of these algorithms, as it is easy to make\n",
      "ð§© Token 1661:  proï¬table\n",
      "trades in-\n",
      "ð§© Token 1662: sample. The generalizability of the strategy is\n",
      "ð§© Token 1663:  determined out-of-sample in the test data\n",
      "ð§© Token 1664: . The\n",
      "empirical analysis reveals signi\n",
      "ð§© Token 1665: ï¬cant challenges in the strategyï¿½\n",
      "ð§© Token 1666: ï¿½s ability to maintain consistent performance\n",
      "across\n",
      "ð§© Token 1667:  diï¬erent time periods. During the\n",
      "ð§© Token 1668:  training and validation phases, the methodology shows promising\n",
      "\n",
      "ð§© Token 1669: results with annualized returns ranging from 26.6\n",
      "ð§© Token 1670: % to 47.7% and strong risk-\n",
      "ð§© Token 1671: adjusted performance metrics\n",
      "(Sharpe ratios between 2\n",
      "ð§© Token 1672: .0 and 3.2). However, this\n",
      "ð§© Token 1673:  performance deteriorates substantially in the test\n",
      "period,\n",
      "ð§© Token 1674:  where returns drop to modest levels (2.9\n",
      "ð§© Token 1675: % to 4.9% annually) with sign\n",
      "ð§© Token 1676: iï¬cantly lower Sharpe\n",
      "\n",
      "ð§© Token 1677: ratios (0.2 to 0.7\n",
      "ð§© Token 1678: ), suggesting that the strategyâs alpha-\n",
      "ð§© Token 1679: generating capability does not generalize well\n",
      "out\n",
      "ð§© Token 1680:  of sample. The distributional properties of returns in\n",
      "ð§© Token 1681:  the test period provide additional insights\n",
      "into the strategy\n",
      "ð§© Token 1682: âs behavior under true out-of-\n",
      "ð§© Token 1683: sample conditions. The shift from negative to strongly\n",
      "\n",
      "ð§© Token 1684: positive skewness (1.85 to 2\n",
      "ð§© Token 1685: .46) coupled with high excess kurtosis\n",
      "ð§© Token 1686:  (5.50 to 14.57) suggests\n",
      "ð§© Token 1687:  that the\n",
      "strategyâs return distribution\n",
      "ð§© Token 1688:  has fundamentally changed, characterized by more frequent small losses\n",
      "ð§© Token 1689: \n",
      "oï¬set by occasional large gains\n",
      "ð§© Token 1690: . This asymmetric return pattern, while potentially appealing\n",
      "ð§© Token 1691:  from a risk\n",
      "preference perspective, diï¿½\n",
      "ð§© Token 1692: ï¿½ï¿½ers markedly from the training period characteristics.\n",
      "ð§© Token 1693:  The tail risk measures\n",
      "further illuminate the strategy\n",
      "ð§© Token 1694: âs risk proï¬le,\n",
      "ð§© Token 1695:  with annualized 95% VaR ranging from -\n",
      "ð§© Token 1696: 7.8% to -18.9%\n",
      "ð§© Token 1697: \n",
      "and corresponding CVaR from -9.\n",
      "ð§© Token 1698: 7% to -26.8% in the\n",
      "ð§© Token 1699:  test period. These statistical properties, combined\n",
      "with\n",
      "ð§© Token 1700:  the strong dependence on historical cluster-speciï¿½\n",
      "ð§© Token 1701: ï¿½ï¿½c performance, indicate that the strategy fails\n",
      "ð§© Token 1702:  to\n",
      "identify stable and generalizable trading signals\n",
      "ð§© Token 1703: , likely due to its reliance on ï¬\n",
      "ð§© Token 1704: rm and industry-speciï¬c\n",
      "ð§© Token 1705: \n",
      "clustering patterns that do not persist out\n",
      "ð§© Token 1706:  of sample. As we can see in the plot\n",
      "ð§© Token 1707: , neither algorithm is\n",
      "able to generate a consistent\n",
      "ð§© Token 1708:  proï¬le of earnings, and the\n",
      "ð§© Token 1709:  statistics conï¬rm that proï¿½\n",
      "ð§© Token 1710: ï¿½ts are negligible, and\n",
      "would likely be\n",
      "ð§© Token 1711:  eaten away by exogenous market frictions (e\n",
      "ð§© Token 1712: .g. trading costs).\n",
      "[Insert Table\n",
      "ð§© Token 1713:  5 about here]\n",
      "LLM. Panel B\n",
      "ð§© Token 1714:  of Table 5 presents the performance metrics for our LL\n",
      "ð§© Token 1715: M-based approach. As before,\n",
      "both\n",
      "ð§© Token 1716:  algorithms perform really well on âseenâ\n",
      "ð§© Token 1717:  data. However, diï¬erent from\n",
      "ð§© Token 1718:  before, the Greedy algorithm\n",
      "works well also\n",
      "ð§© Token 1719:  on the Training Split (which it was not trained\n",
      "ð§© Token 1720:  on). More importantly, both algorithms\n",
      "do a\n",
      "ð§© Token 1721:  great job in the test data. As we can\n",
      "ð§© Token 1722:  see, both are able to achieve a consistent pro\n",
      "ð§© Token 1723: ï¬le of earnings\n",
      "through the split\n",
      "ð§© Token 1724: . The portfolio statistics reveal notable consistency in the strategy\n",
      "ð§© Token 1725: âs performance across\n",
      "diï¬\n",
      "ð§© Token 1726: erent time periods. During the training and validation phases\n",
      "ð§© Token 1727: , the methodology demonstrates solid\n",
      "performance with annualized\n",
      "ð§© Token 1728:  returns ranging from 16.0% to 28.\n",
      "ð§© Token 1729: 3% and Sharpe ratios between 1.4\n",
      "ð§© Token 1730:  and 2.9.\n",
      "This performance strengthens in\n",
      "ð§© Token 1731:  the test period, where returns increase to 30.\n",
      "ð§© Token 1732: 8%-37.2% annually with\n",
      "Shar\n",
      "ð§© Token 1733: pe ratios of 4.3-4.4\n",
      "ð§© Token 1734: , indicating that the strategyâs alpha-\n",
      "ð§© Token 1735: generating capability successfully generalizes\n",
      "22\n",
      "ð§© Token 1736: to out-of-sample conditions. The distribution\n",
      "ð§© Token 1737: al properties of returns provide evidence for the strategyï¿½\n",
      "ð§© Token 1738: ï¿½s\n",
      "robustness. The test period\n",
      "ð§© Token 1739:  maintains positive skewness (0.84 to\n",
      "ð§© Token 1740:  1.49) and moderate to high excess\n",
      "\n",
      "ð§© Token 1741: kurtosis (1.95 to 8.\n",
      "ð§© Token 1742: 30), indicating an asymmetric return pattern with more\n",
      "ð§© Token 1743:  frequent small losses oï¬set\n",
      "by\n",
      "ð§© Token 1744:  larger gains. This return distribution is complemented by\n",
      "ð§© Token 1745:  contained maximum drawdowns (1.1%\n",
      "\n",
      "ð§© Token 1746: to 1.5%) and strong Calmar ratios\n",
      "ð§© Token 1747:  (21.0 to 34.5) in\n",
      "ð§© Token 1748:  the test period. The tail risk measures further\n",
      "\n",
      "ð§© Token 1749: support the strategyâs risk management properties,\n",
      "ð§© Token 1750:  with annualized 95% VaR ranging from -\n",
      "ð§© Token 1751: 6.9% to -\n",
      "9.5\n",
      "ð§© Token 1752: %, and CVaR ranging from -9.\n",
      "ð§© Token 1753: 9% to -11.3% in the\n",
      "ð§© Token 1754:  test period. Taken together, the strategyâ\n",
      "ð§© Token 1755: s ability\n",
      "to sustain consistent out-of-\n",
      "ð§© Token 1756: sample performance metrics demonstrates that the LLM-based\n",
      "ð§© Token 1757:  clustering\n",
      "approach identiï¬\n",
      "ð§© Token 1758: es enduring trading signals that transcend speciï¿½\n",
      "ð§© Token 1759: ï¿½c market regimes.\n",
      "While our primary focus\n",
      "ð§© Token 1760:  has been on developing a methodology to anticipate market reactions\n",
      "ð§© Token 1761:  to\n",
      "news (i.e., identifying winners\n",
      "ð§© Token 1762:  and losers to assess the predictive power of our LL\n",
      "ð§© Token 1763: M-based approach),\n",
      "we also analyze the\n",
      "ð§© Token 1764:  trading intensity and implementation costs of the resulting strategies.\n",
      "ð§© Token 1765:  The detailed\n",
      "examination in A.8 reveals that\n",
      "ð§© Token 1766: , after accounting for transaction costs, the LLM\n",
      "ð§© Token 1767: -based approach maintains\n",
      "its superior performance relative to\n",
      "ð§© Token 1768:  KMeans, though with attenuated proï¿½\n",
      "ð§© Token 1769: ï¿½ï¿½tability. Therefore, practitioners\n",
      "interested\n",
      "ð§© Token 1770:  in the practical implementation of this strategy would beneï¿½\n",
      "ð§© Token 1771: ï¿½ï¿½t from optimizing the trading\n",
      "strategy\n",
      "ð§© Token 1772:  by incorporating transaction costs into their framework.\n",
      "6\n",
      "ð§© Token 1773: . Robustness Checks\n",
      "In our applications we\n",
      "ð§© Token 1774:  have worked with a holding period of L = 4\n",
      "ð§© Token 1775:  trading days and an upper bound\n",
      "on traded clusters\n",
      "ð§© Token 1776:  of Î¸ = â0.5\n",
      "ð§© Token 1777: kâ. As shown in A.\n",
      "ð§© Token 1778: 2, such choices result from the maximization of\n",
      "ð§© Token 1779:  the\n",
      "Sharpe Ratios in the train and\n",
      "ð§© Token 1780:  validation samples. All that is left is to check\n",
      "ð§© Token 1781:  whether our out-of-sample\n",
      "results are\n",
      "ð§© Token 1782:  sensitive to the choice of hyperparameters (L\n",
      "ð§© Token 1783: , Î¸). For this purpose, we evaluate\n",
      "ð§© Token 1784:  the variability\n",
      "of the Sharpe Ratios of\n",
      "ð§© Token 1785:  the test portfolio (SRPtest\n",
      ") to\n",
      "ð§© Token 1786:  changes in L and Î¸.\n",
      "First,\n",
      "ð§© Token 1787:  we focus on the holding period length of the beta\n",
      "ð§© Token 1788: -neutral strategy (L). For this purpose,\n",
      "ð§© Token 1789:  we\n",
      "ï¬x Î¸ = ï¿½\n",
      "ð§© Token 1790: ï¿½ï¿½0.5kâ and\n",
      "ð§© Token 1791: , for each clustering method, obtain the series\n",
      "ð§© Token 1792:  of Sharpe Ratios over a grid L (\n",
      "ð§© Token 1793: which\n",
      "ranges from 1 to 20 trading periods\n",
      "ð§© Token 1794: ). This delivers the series {SRPtest\n",
      "\n",
      "ð§© Token 1795: (L)}LâL, which we\n",
      "ð§© Token 1796:  then plot in\n",
      "two formats. On the left\n",
      "ð§© Token 1797:  side of Figure 9 we plot the distribution of Shar\n",
      "ð§© Token 1798: pe Ratios in the grid, and in the\n",
      "ð§© Token 1799: \n",
      "right side, we show the mapping L \n",
      "ð§© Token 1800: ó°â SRPtest\n",
      "(\n",
      "ð§© Token 1801: L) over L.\n",
      "[Insert Figure 9\n",
      "ð§© Token 1802:  about here]\n",
      "From Figure 9a it follows\n",
      "ð§© Token 1803:  that KMeans clustering produces a distribution that\n",
      "ð§© Token 1804:  is clearly left-skewed,\n",
      "while\n",
      "ð§© Token 1805:  the distribution of SRPtest\n",
      "for LLM\n",
      "ð§© Token 1806:  clustering is clearly right-skewed (\n",
      "ð§© Token 1807: Figure 9c). This conï¬r\n",
      "ð§© Token 1808: ms the\n",
      "fact that LLM clustering generates\n",
      "ð§© Token 1809:  Sharpe Ratios that are statistically higher than those\n",
      "ð§© Token 1810:  generated by\n",
      "KMeans. The plots in\n",
      "ð§© Token 1811:  the right-hand-side substantiate this observation\n",
      "ð§© Token 1812: : KMeans is only able to produce\n",
      "\n",
      "ð§© Token 1813: positive SRPtest\n",
      "for really short holding window\n",
      "ð§© Token 1814:  lengths (Figure 9b), while LLM clust\n",
      "ð§© Token 1815: ering, although\n",
      "23\n",
      "ð§© Token 1816: not always stable, is, in general, able\n",
      "ð§© Token 1817:  to produce positive Sharpe Ratios more consistently over\n",
      "ð§© Token 1818:  the grid\n",
      "(Figure 9d).\n",
      "We\n",
      "ð§© Token 1819:  then turn to analyze the sensitivity of SRPtest\n",
      "ð§© Token 1820: \n",
      "to diï¬erent values for the\n",
      "ð§© Token 1821:  upper bound on the number\n",
      "of traded clusters (\n",
      "ð§© Token 1822: Î¸). Now we ï¬x L\n",
      "ð§© Token 1823:  = 4 and deï¬ne a grid\n",
      "ð§© Token 1824:  Î¸, from where we can obtain {SR\n",
      "ð§© Token 1825: Ptest\n",
      "(Î¸)}Î¸ï¿½\n",
      "ð§© Token 1826: ï¿½Î¸.\n",
      "[Insert Figure 10 about\n",
      "ð§© Token 1827:  here]\n",
      "The results of this exercise are shown\n",
      "ð§© Token 1828:  in Figure 10. As we can see, in\n",
      "ð§© Token 1829:  Figure 10a the results are mixed\n",
      "for the\n",
      "ð§© Token 1830:  case of KMeans clustering. Namely\n",
      "ð§© Token 1831: , the Stable algorithm is able to generate positive\n",
      "ð§© Token 1832:  Sharpe Ratios\n",
      "but the Greedy algorithm\n",
      "ð§© Token 1833:  struggles to do so. In Figure 10b we\n",
      "ð§© Token 1834:  see what is happening: Stable works well\n",
      "\n",
      "ð§© Token 1835: with for low values of Î¸, while Gre\n",
      "ð§© Token 1836: edy only works for high values of Î¸.\n",
      "ð§© Token 1837:  This high reliance of the algorithms\n",
      "on speci\n",
      "ð§© Token 1838: ï¬c values of Î¸ points to\n",
      "ð§© Token 1839:  the instability of the trading strategy when employing KMe\n",
      "ð§© Token 1840: ans clustering.\n",
      "On the other hand,\n",
      "ð§© Token 1841:  Figure 10c shows a clear pattern for the case\n",
      "ð§© Token 1842:  of LLM clustering. Namely, the\n",
      "ð§© Token 1843:  mass\n",
      "accumulates at high and positive Shar\n",
      "ð§© Token 1844: pe Ratios. This observation is further substantiated\n",
      "ð§© Token 1845:  by Figure 10b,\n",
      "which shows that leaving\n",
      "ð§© Token 1846:  aside the fact that the greedy algorithm does bad for\n",
      "ð§© Token 1847:  really low values of Î¸ (i.e\n",
      "ð§© Token 1848: .:\n",
      "Î¸ â¤ 3), in general,\n",
      "ð§© Token 1849:  the trading strategy is now able to produce high,\n",
      "ð§© Token 1850:  positive and stable Sharpe Ratios\n",
      "across\n",
      "ð§© Token 1851:  diï¬erent values of Î¸.\n",
      "ð§© Token 1852: \n",
      "All in all, our results are robust to\n",
      "ð§© Token 1853:  hyperparameter variability, showing that LLM clust\n",
      "ð§© Token 1854: ering consis-\n",
      "tently beats a strategy\n",
      "ð§© Token 1855:  based on clustering embeddings with KMe\n",
      "ð§© Token 1856: ans.\n",
      "7. Conclusion\n",
      "This paper investigates\n",
      "ð§© Token 1857:  how information from business news aï¬ect\n",
      "ð§© Token 1858: s stock market prices. We analyze\n",
      "a dataset\n",
      "ð§© Token 1859:  of Spanish business articles during a particularly volatile period-\n",
      "ð§© Token 1860: the COVID-19 pandemic-\n",
      "and\n",
      "ð§© Token 1861:  examine ï¬rm-speciï¿½\n",
      "ð§© Token 1862: ï¿½c stock market reactions to news. We show\n",
      "ð§© Token 1863:  that transforming text into vector\n",
      "embeddings and\n",
      "ð§© Token 1864:  clustering them using KMeans yields clusters that\n",
      "ð§© Token 1865:  are ï¬rm-speciï¿½\n",
      "ð§© Token 1866: ï¿½c and industry-speciï¬\n",
      "ð§© Token 1867: c.\n",
      "However, the distribution of articles across\n",
      "ð§© Token 1868:  clusters is unstable over sequential data splits, indicating\n",
      "\n",
      "ð§© Token 1869: temporal instability. When we implement a cluster-\n",
      "ð§© Token 1870: based trading strategy-similar to portfolio sorts-on\n",
      "ð§© Token 1871: \n",
      "the KMeans clusters, we observe an\n",
      "ð§© Token 1872:  over-reliance on the past performance of a\n",
      "ð§© Token 1873:  cluster. That is, signals\n",
      "are short-\n",
      "ð§© Token 1874: lived due to temporal instability. Consequently, the out\n",
      "ð§© Token 1875: -of-sample proï¬tability\n",
      "ð§© Token 1876:  of the trading\n",
      "strategy is negligible, evid\n",
      "ð§© Token 1877: encing the methodâs poor temporal generaliz\n",
      "ð§© Token 1878: ability. Therefore, a model based\n",
      "on embed\n",
      "ð§© Token 1879: dings is superï¬cial and is\n",
      "ð§© Token 1880:  not able to anticipate market trends.\n",
      "As an\n",
      "ð§© Token 1881:  alternative, we develop a novel approach by guiding a\n",
      "ð§© Token 1882:  Large Language Model (LLM) through a\n",
      "\n",
      "ð§© Token 1883: structured news-parsing schema, enabling\n",
      "ð§© Token 1884:  it to analyze news-implied ï¬\n",
      "ð§© Token 1885: rm-speciï¬c economic shocks\n",
      "ð§© Token 1886: . The\n",
      "schema involves identifying the ï¿½\n",
      "ð§© Token 1887: ï¿½rms aï¬ected by the\n",
      "ð§© Token 1888:  articles and classifying the implied shocks on such\n",
      "\n",
      "ð§© Token 1889: ï¬rms by their type, magnitude\n",
      "ð§© Token 1890: , and direction. This LLM-based methodology\n",
      "ð§© Token 1891:  demonstrates several advan-\n",
      "tages over the\n",
      "ð§© Token 1892:  traditional clustering approach. Even in a volatile period\n",
      "ð§© Token 1893: , it produces stable distributions\n",
      "24\n",
      "ð§© Token 1894: of articles across clusters in sequential splits, demonstrating robust\n",
      "ð§© Token 1895:  temporal stability. Moreover, the re-\n",
      "s\n",
      "ð§© Token 1896: ulting trading signals are both long-lasting and\n",
      "ð§© Token 1897:  economically relevant, as they are based on fundamental\n",
      "\n",
      "ð§© Token 1898: economic shocks rather than statistical patterns. The results show\n",
      "ð§© Token 1899:  that the LLM-based trading strategy\n",
      "e\n",
      "ð§© Token 1900: ï¬ectively identiï¬\n",
      "ð§© Token 1901: es winners and losers, illustrating the parserâ\n",
      "ð§© Token 1902: s ability to anticipate market trends by\n",
      "compre\n",
      "ð§© Token 1903: hending the economic implications of ï¬rm\n",
      "ð§© Token 1904: -speciï¬c shocks. This\n",
      "ð§© Token 1905:  approach generates a consistent\n",
      "proï¬le\n",
      "ð§© Token 1906:  of earnings in the test set, with results robust\n",
      "ð§© Token 1907:  to the choice of hyperparameters-the holding\n",
      "ð§© Token 1908:  period\n",
      "length of the trading strategy and the number\n",
      "ð§© Token 1909:  of selected clusters for trading. Our ï¬\n",
      "ð§© Token 1910: ndings demonstrate\n",
      "a promising avenue: LLMs\n",
      "ð§© Token 1911: , when guided by appropriate economic frameworks, can help\n",
      "ð§© Token 1912:  predict market\n",
      "reactions to news through systematic class\n",
      "ð§© Token 1913: iï¬cation of economic shocks embedded in\n",
      "ð§© Token 1914:  ï¬nancial narratives.\n",
      "References\n",
      "\n",
      "ð§© Token 1915: [1] E. F. Fama,\n",
      "ð§© Token 1916:  Eï¬cient capital markets: A review\n",
      "ð§© Token 1917:  of theory and empirical work, J. Finance 25\n",
      "ð§© Token 1918:  (2)\n",
      "(1970) 383. doi\n",
      "ð§© Token 1919: :10.2307/2325486.\n",
      "ð§© Token 1920: \n",
      "URL http://dx.doi.org/\n",
      "ð§© Token 1921: 10.2307/2325486\n",
      "[\n",
      "ð§© Token 1922: 2] P. C. Tetlock, Giving\n",
      "ð§© Token 1923:  content to investor sentiment: The role of media in\n",
      "ð§© Token 1924:  the stock market, J. Finance\n",
      "62 (\n",
      "ð§© Token 1925: 3) (2007) 1139â1168\n",
      "ð§© Token 1926: . doi:10.1111/j.15\n",
      "ð§© Token 1927: 40-6261.2007.01232.\n",
      "ð§© Token 1928: x.\n",
      "URL http://dx.doi.\n",
      "ð§© Token 1929: org/10.1111/j.1540\n",
      "ð§© Token 1930: -6261.2007.01232.x\n",
      "ð§© Token 1931: \n",
      "[3] P. C. Tetlock\n",
      "ð§© Token 1932: , M. Saar-Tsechansky\n",
      "ð§© Token 1933: , S. Macskassy, More than words\n",
      "ð§© Token 1934: : Quantifying language to measure\n",
      "ï¬\n",
      "ð§© Token 1935: rmsâfundamentals, J.\n",
      "ð§© Token 1936:  Finance 63 (3) (2008) 1437\n",
      "ð§© Token 1937: â1467. doi:10.1111/\n",
      "ð§© Token 1938: j.1540-6261.2008.\n",
      "ð§© Token 1939: 01362.\n",
      "x.\n",
      "URL http://\n",
      "ð§© Token 1940: dx.doi.org/10.1111/\n",
      "ð§© Token 1941: j.1540-6261.2008.\n",
      "ð§© Token 1942: 01362.x\n",
      "[4] T.\n",
      "ð§© Token 1943:  Loughran, B. McDonald, When is\n",
      "ð§© Token 1944:  a liability not a liability? textual analysis, diction\n",
      "ð§© Token 1945: aries, and\n",
      "10ks, J. Finance\n",
      "ð§© Token 1946:  66 (1) (2011) 35â65\n",
      "ð§© Token 1947: . doi:10.1111/j.15\n",
      "ð§© Token 1948: 40-6261.2010.01625.\n",
      "ð§© Token 1949: x.\n",
      "URL http://dx.doi.\n",
      "ð§© Token 1950: org/10.1111/j.1540\n",
      "ð§© Token 1951: -6261.2010.01625.x\n",
      "ð§© Token 1952: \n",
      "[5] N. Jegadeesh\n",
      "ð§© Token 1953: , D. Wu, Word power: A new\n",
      "ð§© Token 1954:  approach for content analysis, J. Financ.\n",
      "ð§© Token 1955:  Econ. 110 (3)\n",
      "(2013\n",
      "ð§© Token 1956: ) 712â729. doi:10.\n",
      "ð§© Token 1957: 1016/j.jfineco.2013.\n",
      "ð§© Token 1958: 08.018.\n",
      "URL http://dx.\n",
      "ð§© Token 1959: doi.org/10.1016/j.\n",
      "ð§© Token 1960: jfineco.2013.08.018\n",
      "\n",
      "ð§© Token 1961: [6] J. Bollen, H.\n",
      "ð§© Token 1962:  Mao, X. Zeng, Twitter mood predicts\n",
      "ð§© Token 1963:  the stock market, J. Comput. Sci.\n",
      "ð§© Token 1964:  2 (1) (2011)\n",
      "1â\n",
      "ð§© Token 1965: Å8. doi:10.1016/\n",
      "ð§© Token 1966: j.jocs.2010.12.\n",
      "ð§© Token 1967: 007.\n",
      "URL http://dx.doi.\n",
      "ð§© Token 1968: org/10.1016/j.joc\n",
      "ð§© Token 1969: s.2010.12.007\n",
      "[7\n",
      "ð§© Token 1970: ] D. Garcia, Sentiment during recessions\n",
      "ð§© Token 1971: , J. Finance 68 (3) (2013\n",
      "ð§© Token 1972: ) 1267â1300. doi:10\n",
      "ð§© Token 1973: .1111/jofi.\n",
      "12027.\n",
      "ð§© Token 1974: \n",
      "URL http://dx.doi.org/\n",
      "ð§© Token 1975: 10.1111/jofi.12027\n",
      "\n",
      "ð§© Token 1976: 25\n",
      "ð§© Token 1977: [8] Z. T. Ke, B\n",
      "ð§© Token 1978: . Kelly, D. Xiu, Predicting\n",
      "ð§© Token 1979:  returns with text data, Tech. rep., National\n",
      "ð§© Token 1980:  Bureau of Economic\n",
      "Research (Aug. 2019).\n",
      "ð§© Token 1981:  doi:10.3386/w26186\n",
      "ð§© Token 1982: .\n",
      "URL http://dx.doi.org\n",
      "ð§© Token 1983: /10.3386/w26186\n",
      "\n",
      "ð§© Token 1984: [9] C.-C. Lee, Z\n",
      "ð§© Token 1985: . Gao, C.-L. Tsai\n",
      "ð§© Token 1986: , Bert-based stock market sentiment analysis, in\n",
      "ð§© Token 1987: : 2020 IEEE Inter-\n",
      "national Conference on Consumer\n",
      "ð§© Token 1988:  Electronics - Taiwan (ICCE-Taiwan),\n",
      "ð§© Token 1989:  IEEE, 2020, pp. 1âÅ\n",
      "ð§© Token 1990: 2.\n",
      "doi:10.1109/\n",
      "ð§© Token 1991: icce-taiwan49838.2020\n",
      "ð§© Token 1992: .9258102.\n",
      "URL http://dx\n",
      "ð§© Token 1993: .doi.org/10.1109/\n",
      "ð§© Token 1994: icce-taiwan49838.2020\n",
      "ð§© Token 1995: .9258102\n",
      "[10] F.\n",
      "ð§© Token 1996:  Wei, U. Nguyen, Stock trend prediction using\n",
      "ð§© Token 1997:  ï¬nancial market news and bert\n",
      "ð§© Token 1998: , in: Proceedings\n",
      "of the 12th International\n",
      "ð§© Token 1999:  Joint Conference on Knowledge Discovery, Knowledge Engineering and\n",
      "\n",
      "ð§© Token 2000: Knowledge Management, SCITEPRESS - Science and\n",
      "ð§© Token 2001:  Technology Publications, 2020, pp. 319â326\n",
      "ð§© Token 2002: .\n",
      "doi:10.5220/001\n",
      "ð§© Token 2003: 0172103190326.\n",
      "URL http\n",
      "ð§© Token 2004: ://dx.doi.org/10.52\n",
      "ð§© Token 2005: 20/0010172103190326\n",
      "\n",
      "ð§© Token 2006: [11] W. Antweiler, M\n",
      "ð§© Token 2007: . Z. Frank, Do us stock markets typically\n",
      "ð§© Token 2008:  overreact to corporate news stories?, SSRN\n",
      "\n",
      "ð§© Token 2009: Electron. J.doi:10.2\n",
      "ð§© Token 2010: 139/ssrn.878091.\n",
      "\n",
      "ð§© Token 2011: URL http://dx.doi.org/10\n",
      "ð§© Token 2012: .2139/ssrn.878091\n",
      "ð§© Token 2013: \n",
      "[12] S. Hansen, M.\n",
      "ð§© Token 2014:  McMahon, A. Prat, Transparency and deliber\n",
      "ð§© Token 2015: ation within the fomc: A computational\n",
      "\n",
      "ð§© Token 2016: linguistics approach, Q. J. E\n",
      "ð§© Token 2017: con. 133 (2) (2018) 8\n",
      "ð§© Token 2018: 01âÅ870. doi:10.\n",
      "ð§© Token 2019: 1093/qje/qjx045\n",
      "ð§© Token 2020: .\n",
      "URL http://dx.doi.org\n",
      "ð§© Token 2021: /10.1093/qje/q\n",
      "ð§© Token 2022: jx045\n",
      "[13] L. By\n",
      "ð§© Token 2023: bee, B. Kelly, A. Manela\n",
      "ð§© Token 2024: , D. Xiu, Business news and business\n",
      "ð§© Token 2025:  cycles, J. Finance 79 (5) (\n",
      "ð§© Token 2026: 2024)\n",
      "3105â3147.\n",
      "ð§© Token 2027:  doi:10.1111/jofi.13\n",
      "ð§© Token 2028: 377.\n",
      "URL http://dx.doi.\n",
      "ð§© Token 2029: org/10.1111/jofi.13\n",
      "ð§© Token 2030: 377\n",
      "[14] L. Bybee,\n",
      "ð§© Token 2031:  B. Kelly, Y. Su, Narrative\n",
      "ð§© Token 2032:  asset pricing: Interpretable systematic risk factors from news\n",
      "ð§© Token 2033: \n",
      "text, The Rev. Financ. Stud\n",
      "ð§© Token 2034: . 36 (12) (2023) 4\n",
      "ð§© Token 2035: 759â4787. doi:10.10\n",
      "ð§© Token 2036: 93/rfs/hhad042.\n",
      "ð§© Token 2037: \n",
      "URL http://dx.doi.org/\n",
      "ð§© Token 2038: 10.1093/rfs/hhad\n",
      "ð§© Token 2039: 042\n",
      "[15] G. Hober\n",
      "ð§© Token 2040: g, G. Phillips, Text-based network\n",
      "ð§© Token 2041:  industries and endogenous product diï¬erentiation\n",
      "ð§© Token 2042: , J.\n",
      "Polit. Econ. 124\n",
      "ð§© Token 2043:  (5) (2016) 1423â14\n",
      "ð§© Token 2044: 65. doi:10.1086/688\n",
      "ð§© Token 2045: 176.\n",
      "URL http://dx.doi.\n",
      "ð§© Token 2046: org/10.1086/688176\n",
      "\n",
      "ð§© Token 2047: [16] Q. Chen, Stock movement prediction\n",
      "ð§© Token 2048:  with ï¬nancial news using contextualized\n",
      "ð§© Token 2049:  embedding from bert,\n",
      "arXiv\n",
      "ð§© Token 2050:  preprint arXiv:2107.08\n",
      "ð§© Token 2051: 721doi:10.48550/AR\n",
      "ð§© Token 2052: XIV.2107.08721.\n",
      "ð§© Token 2053: \n",
      "URL https://arxiv.org/\n",
      "ð§© Token 2054: abs/2107.08721\n",
      "[\n",
      "ð§© Token 2055: 17] E. Benincasa, J.\n",
      "ð§© Token 2056:  Fu, M. Mishra, A. Paran\n",
      "ð§© Token 2057: jape, Diï¬erent shades of\n",
      "ð§© Token 2058:  green: Estimating the green bond\n",
      "premium\n",
      "ð§© Token 2059:  using natural language processing, SSRN Electron.\n",
      "ð§© Token 2060:  J.doi:10.2139/ss\n",
      "ð§© Token 2061: rn.4198065.\n",
      "URL http://\n",
      "ð§© Token 2062: dx.doi.org/10.2139\n",
      "ð§© Token 2063: /ssrn.4198065\n",
      "26\n",
      "ð§© Token 2064: [18] M. Jha, H.\n",
      "ð§© Token 2065:  Liu, A. Manela, Does ï¿½\n",
      "ð§© Token 2066: ï¿½nance beneï¬t society? a\n",
      "ð§© Token 2067:  language embedding approach, Rev.\n",
      "Financ\n",
      "ð§© Token 2068: . Stud., Forthcomingdoi:10.2\n",
      "ð§© Token 2069: 139/ssrn.3655263.\n",
      "\n",
      "ð§© Token 2070: URL http://dx.doi.org/10\n",
      "ð§© Token 2071: .2139/ssrn.3655263\n",
      "ð§© Token 2072: \n",
      "[19] C. L. Zhang,\n",
      "ð§© Token 2073:  Feel the market: An attempt to identify additional factor\n",
      "ð§© Token 2074:  in the capital asset pricing\n",
      "model (capm\n",
      "ð§© Token 2075: ) using generative pre-trained transformer (g\n",
      "ð§© Token 2076: pt) and bidirectional encoder representa\n",
      "ð§© Token 2077: -\n",
      "tions from transformers (bert), SS\n",
      "ð§© Token 2078: RN Electron. J.doi:10.\n",
      "ð§© Token 2079: 2139/ssrn.4521946.\n",
      "ð§© Token 2080: \n",
      "URL http://dx.doi.org/\n",
      "ð§© Token 2081: 10.2139/ssrn.45219\n",
      "ð§© Token 2082: 46\n",
      "[20] X. Gabaix\n",
      "ð§© Token 2083: , R. S. J. Koijen\n",
      "ð§© Token 2084: , M. Yogo, Asset embeddings\n",
      "ð§© Token 2085: , SSRN Electron. J.doi:\n",
      "ð§© Token 2086: 10.2139/ssrn.\n",
      "450\n",
      "ð§© Token 2087: 7511.\n",
      "URL http://dx.doi\n",
      "ð§© Token 2088: .org/10.2139/ssrn\n",
      "ð§© Token 2089: .4507511\n",
      "[21] D.\n",
      "ð§© Token 2090:  Cutler, J. Poterba, L.\n",
      "ð§© Token 2091:  Summers, What Moves Stock Prices?, 1988. doi\n",
      "ð§© Token 2092: :10.3386/w2538.\n",
      "ð§© Token 2093: \n",
      "URL http://dx.doi.org/\n",
      "ð§© Token 2094: 10.3386/w2538\n",
      "[\n",
      "ð§© Token 2095: 22] M. L. Mitchell, J.\n",
      "ð§© Token 2096:  H. Mulherin, The impact of public\n",
      "ð§© Token 2097:  information on the stock market, J. Finance\n",
      "\n",
      "ð§© Token 2098: 49 (3) (1994) 923â\n",
      "ð§© Token 2099: 950. doi:10.2307/23\n",
      "ð§© Token 2100: 29211.\n",
      "URL http://dx.doi\n",
      "ð§© Token 2101: .org/10.2307/2329\n",
      "ð§© Token 2102: 211\n",
      "[23] S. R. Baker\n",
      "ð§© Token 2103: , N. Bloom, S. J. Davis\n",
      "ð§© Token 2104: , Measuring economic policy uncertainty, Q. J\n",
      "ð§© Token 2105: . Econ. 131 (4)\n",
      "(\n",
      "ð§© Token 2106: 2016) 1593â1636. doi:\n",
      "ð§© Token 2107: 10.1093/qje/qj\n",
      "ð§© Token 2108: w024.\n",
      "URL http://dx.doi\n",
      "ð§© Token 2109: .org/10.1093/qje\n",
      "ð§© Token 2110: /qjw024\n",
      "[24] S\n",
      "ð§© Token 2111: . Baker, N. Bloom, S. Davis\n",
      "ð§© Token 2112: , M. Sammon, What Triggers\n",
      "ð§© Token 2113:  Stock Market Jumps?, 2021. doi:\n",
      "\n",
      "ð§© Token 2114: 10.3386/w28687.\n",
      "\n",
      "ð§© Token 2115: URL http://dx.doi.org/10\n",
      "ð§© Token 2116: .3386/w28687\n",
      "[25\n",
      "ð§© Token 2117: ] A. Manela, A. Moreira\n",
      "ð§© Token 2118: , News implied volatility and disaster concerns, J.\n",
      "ð§© Token 2119:  Financ. Econ. 123 (1)\n",
      "ð§© Token 2120:  (2017)\n",
      "137â162. doi:\n",
      "ð§© Token 2121: 10.1016/j.jfineco.\n",
      "ð§© Token 2122: 2016.01.032.\n",
      "URL http://\n",
      "ð§© Token 2123: dx.doi.org/10.1016/\n",
      "ð§© Token 2124: j.jfineco.2016.01.\n",
      "ð§© Token 2125: 032\n",
      "[26] W. S. Chan\n",
      "ð§© Token 2126: , Stock price reaction to news and no-news\n",
      "ð§© Token 2127: : drift and reversal after headlines, J. Fin\n",
      "ð§© Token 2128: anc.\n",
      "Econ. 70 (2)\n",
      "ð§© Token 2129:  (2003) 223â260. doi:10\n",
      "ð§© Token 2130: .1016/s0304-405x(\n",
      "ð§© Token 2131: 03)00146-6.\n",
      "URL http\n",
      "ð§© Token 2132: ://dx.doi.org/10.1016\n",
      "ð§© Token 2133: /S0304-405X(03)\n",
      "ð§© Token 2134: 00146-6\n",
      "[27] P.\n",
      "ð§© Token 2135:  Oncharoen, P. Vateekul\n",
      "ð§© Token 2136: , Deep learning for stock market prediction using event embed\n",
      "ð§© Token 2137: ding and\n",
      "technical indicators, in: 2018 5\n",
      "ð§© Token 2138: th International Conference on Advanced Informatics: Concept\n",
      "ð§© Token 2139:  Theory\n",
      "and Applications (ICAICTA), IEEE\n",
      "ð§© Token 2140: , 2018, pp. 19âÅ24\n",
      "ð§© Token 2141: . doi:10.1109/icaict\n",
      "ð§© Token 2142: a.2018.8541310.\n",
      "\n",
      "ð§© Token 2143: URL http://dx.doi.org/10\n",
      "ð§© Token 2144: .1109/icaicta.2018.\n",
      "ð§© Token 2145: 8541310\n",
      "27\n",
      "ð§© Token 2146: [28] A. Lopez-Lira,\n",
      "ð§© Token 2147:  Y. Tang, Can chatgpt forecast stock\n",
      "ð§© Token 2148:  price movements? return predictability and large\n",
      "language\n",
      "ð§© Token 2149:  models, SSRN Electron. J.doi\n",
      "ð§© Token 2150: :10.2139/ssrn.44\n",
      "ð§© Token 2151: 12788.\n",
      "URL http://dx.doi\n",
      "ð§© Token 2152: .org/10.2139/ssrn\n",
      "ð§© Token 2153: .4412788\n",
      "[29] Y.\n",
      "ð§© Token 2154:  Chen, B. T. Kelly, D.\n",
      "ð§© Token 2155:  Xiu, Expected returns and large language models\n",
      "ð§© Token 2156: , Available at SSRN\n",
      "4416687.\n",
      "ð§© Token 2157: \n",
      "URL https://papers.ssrn.com\n",
      "ð§© Token 2158: /sol3/papers.cfm?ab\n",
      "ð§© Token 2159: stract_id=4416687\n",
      "[30\n",
      "ð§© Token 2160: ] A. Vaswani, N. Sh\n",
      "ð§© Token 2161: azeer, N. Parmar, J.\n",
      "ð§© Token 2162:  Uszkoreit, L. Jones,\n",
      "ð§© Token 2163:  A. N. Gomez, Å. Kaiser\n",
      "ð§© Token 2164: , I. Polosukhin,\n",
      "Att\n",
      "ð§© Token 2165: ention is all you need, Adv. Neural Inf\n",
      "ð§© Token 2166: . Process. Syst. 30. doi:\n",
      "ð§© Token 2167: 10.48550/ARXIV.17\n",
      "ð§© Token 2168: 06.03762.\n",
      "URL https://\n",
      "ð§© Token 2169: doi.org/10.48550/ar\n",
      "ð§© Token 2170: xiv.1706.03762\n",
      "\n",
      "ð§© Token 2171: [31] J. Devlin, M.-\n",
      "ð§© Token 2172: W. Chang, K. Lee, K.\n",
      "ð§© Token 2173:  Toutanova, Bert: Pre-training of\n",
      "ð§© Token 2174:  deep bidirectional transformers\n",
      "for language understanding\n",
      "ð§© Token 2175: , arXiv preprint arXiv:\n",
      "ð§© Token 2176: 1810.04805doi:10.48\n",
      "ð§© Token 2177: 550/ARXIV.1810.04\n",
      "ð§© Token 2178: 805.\n",
      "URL https://arxiv.\n",
      "ð§© Token 2179: org/abs/1810.04805\n",
      "\n",
      "ð§© Token 2180: [32] A. Radford, K.\n",
      "ð§© Token 2181:  Narasimhan, T. Salimans\n",
      "ð§© Token 2182: , I. Sutskever, et al\n",
      "ð§© Token 2183: ., Improving language understanding by\n",
      "generative pre\n",
      "ð§© Token 2184: -training.\n",
      "[33] T. Le\n",
      "ð§© Token 2185:  Scao, A. Fan, C. Ak\n",
      "ð§© Token 2186: iki, E. Pavlick, S.\n",
      "ð§© Token 2187:  Ili, D. Hesslow, R.\n",
      "ð§© Token 2188:  Castagn, A. S. Luccion\n",
      "ð§© Token 2189: i, F. Yvon,\n",
      "M.\n",
      "ð§© Token 2190:  Gall, et al., Bloom: A 176b\n",
      "ð§© Token 2191: -parameter open-access multilingual language mode\n",
      "ð§© Token 2192: ldoi:10.48550/\n",
      "AR\n",
      "ð§© Token 2193: XIV.2211.05100.\n",
      "\n",
      "ð§© Token 2194: URL https://arxiv.org/abs\n",
      "ð§© Token 2195: /2211.05100\n",
      "[34]\n",
      "ð§© Token 2196:  H. Touvron, T. Lavril\n",
      "ð§© Token 2197: , G. Izacard, X. Martin\n",
      "ð§© Token 2198: et, M.-A. Lachaux,\n",
      "ð§© Token 2199:  T. Lacroix, B. Rozi\n",
      "ð§© Token 2200: Ã© re, N. Goyal,\n",
      "E\n",
      "ð§© Token 2201: . Hambro, F. Azhar, et\n",
      "ð§© Token 2202:  al., Llama: Open and eï¿½\n",
      "ð§© Token 2203: ï¿½cient foundation language models, arXiv pre\n",
      "ð§© Token 2204: print\n",
      "arXiv:2302.139\n",
      "ð§© Token 2205: 71.\n",
      "[35] H. Jiang,\n",
      "ð§© Token 2206:  S. Z. Li, H. Wang,\n",
      "ð§© Token 2207:  Pervasive underreaction: Evidence from high\n",
      "ð§© Token 2208: -frequency data, J. Financ.\n",
      "\n",
      "ð§© Token 2209: Econ. 141 (2) (2021\n",
      "ð§© Token 2210: ) 573â599. doi:10.\n",
      "ð§© Token 2211: 1016/j.jfineco.2021\n",
      "ð§© Token 2212: .04.003.\n",
      "URL http://dx\n",
      "ð§© Token 2213: .doi.org/10.1016/j\n",
      "ð§© Token 2214: .jfineco.2021.04.\n",
      "ð§© Token 2215: 003\n",
      "[36] N. Jegade\n",
      "ð§© Token 2216: esh, S. Titman, Returns to buying\n",
      "ð§© Token 2217:  winners and selling losers: Implications for stock market\n",
      "ð§© Token 2218: \n",
      "eï¬ciency, J.\n",
      "ð§© Token 2219:  Finance 48 (1) (1993) 65â\n",
      "ð§© Token 2220: 91. doi:10.1111/j.\n",
      "ð§© Token 2221: 1540-6261.1993.tb\n",
      "ð§© Token 2222: 04702.x.\n",
      "URL http://dx\n",
      "ð§© Token 2223: .doi.org/10.1111/j\n",
      "ð§© Token 2224: .1540-6261.1993.t\n",
      "ð§© Token 2225: b04702.x\n",
      "28\n",
      "ð§© Token 2226: Table 1: Summary Statistics of Articles by Data Split\n",
      "ð§© Token 2227: \n",
      "Data Split Time Period # Articles # Words Voc\n",
      "ð§© Token 2228: abulary Size\n",
      "Train 24/06/2020 â\n",
      "ð§© Token 2229:  12/02/2021 1254 3274\n",
      "ð§© Token 2230: 13 26762\n",
      "Validation 12/02/\n",
      "ð§© Token 2231: 2021 â 21/06/2021 8\n",
      "ð§© Token 2232: 36 232912 22265\n",
      "Test 21/\n",
      "ð§© Token 2233: 06/2021 â 30/09/20\n",
      "ð§© Token 2234: 21 523 140495 16474\n",
      "All 24\n",
      "ð§© Token 2235: /06/2020 â 30/09/20\n",
      "ð§© Token 2236: 21 2613 700820 42603\n",
      "Note:\n",
      "ð§© Token 2237:  Summary statistics by data splits and for the whole sample\n",
      "ð§© Token 2238: . We provide the period spanned by each data\n",
      "ð§© Token 2239: \n",
      "split, the number of articles, the number\n",
      "ð§© Token 2240:  of words, and the vocabulary size. Articles have\n",
      "ð§© Token 2241:  been preprocessed following\n",
      "standard NLP practices\n",
      "ð§© Token 2242: .\n",
      "29\n",
      "ð§© Token 2243: Table 2: Function calling schema\n",
      "Function Prompt Options\n",
      "ð§© Token 2244: \n",
      "1. firms âList all the ï¿½\n",
      "ð§© Token 2245: ï¿½ï¿½rms aï¬ected by\n",
      "ð§© Token 2246:  the events\n",
      "narrated in the articleï¿½\n",
      "ð§© Token 2247: ï¿½ array\n",
      "1.1. firm â\n",
      "ð§© Token 2248: Iterate over each firm in firmsâ string\n",
      "ð§© Token 2249: \n",
      "1.2. ticker âState\n",
      "ð§© Token 2250:  the stock market ticker of firm â string\n",
      "ð§© Token 2251: \n",
      "1.3. shock_type â\n",
      "ð§© Token 2252: What type of shock does this article\n",
      "imply\n",
      "ð§© Token 2253:  on firm ?â\n",
      "{demand, supply\n",
      "ð§© Token 2254: , ï¬nancial,\n",
      "technology,\n",
      "ð§© Token 2255:  policy}\n",
      "1.4. shock_m\n",
      "ð§© Token 2256: agnitude âHow much impact is this shock\n",
      "ð§© Token 2257: \n",
      "expected to have on firm?â {\n",
      "ð§© Token 2258: minor, major}\n",
      "1.5.\n",
      "ð§© Token 2259:  shock_direction âIn what direction is this\n",
      "ð§© Token 2260:  shock expected\n",
      "to impact firm?â {\n",
      "ð§© Token 2261: positive, negative}\n",
      "This table outlines the structure\n",
      "ð§© Token 2262:  of the function calling schema we designed to guide the\n",
      "ð§© Token 2263:  LLM through the analysis\n",
      "of news-impl\n",
      "ð§© Token 2264: ied ï¬rm-speciï¿½\n",
      "ð§© Token 2265: ï¿½c economic shocks. The âFunctionï¿½\n",
      "ð§© Token 2266: ï¿½ column speciï¬ces the name\n",
      "ð§© Token 2267:  of the tool passed to\n",
      "the LLM.\n",
      "ð§© Token 2268:  We can understand the umbrella function firms as running a\n",
      "ð§© Token 2269:  loop over each of its arguments, with the\n",
      "\n",
      "ð§© Token 2270: indented subfunctions being referred to the spe\n",
      "ð§© Token 2271: ciï¬c argument passed to them.\n",
      "ð§© Token 2272:  The âPromptâ column provides an\n",
      "ð§© Token 2273: \n",
      "example of the simpliï¬ed\n",
      "ð§© Token 2274:  instructions given to the LLM (the actual prompts\n",
      "ð§© Token 2275:  are longer as the LLM needs clear\n",
      "and\n",
      "ð§© Token 2276:  detailed instructions, with useful examples for context). Finally\n",
      "ð§© Token 2277: , the âOptionsâ column imposes the\n",
      "ð§© Token 2278:  answer\n",
      "format that the LLM must follow.\n",
      "ð§© Token 2279:  For example, in firms, the âarray\n",
      "ð§© Token 2280: â option indicates that the answer must be\n",
      "\n",
      "ð§© Token 2281: an enumeration of ï¬rms,\n",
      "ð§© Token 2282:  while the âstringâ option in the\n",
      "ð§© Token 2283:  subfunctions firm and ticker indicates that the\n",
      "ð§© Token 2284:  answer\n",
      "must be a single string. Finally,\n",
      "ð§© Token 2285:  the shock_ subfunctions ask the LLM\n",
      "ð§© Token 2286:  to choose from a predeï¬ned\n",
      "ð§© Token 2287:  set of possible\n",
      "responses.\n",
      "30\n",
      "ð§© Token 2288: Table 3: Mapping of embeddings-\n",
      "ð§© Token 2289: based KMeans clusters to Trading Signals\n",
      "\n",
      "ð§© Token 2290: Cluster Greedy Stable\n",
      "0 Miscellaneous (\n",
      "ð§© Token 2291: Colonial, Acciona, Amadeus,\n",
      "ð§© Token 2292:  Grifols, Endesa, IAG,\n",
      "ð§© Token 2293: \n",
      "Bankinter...) short\n",
      "1 Quarterly & Semi\n",
      "ð§© Token 2294: -Annual Earnings Reports short\n",
      "2 BB\n",
      "ð§© Token 2295: VA & Sabadell: Financial Performance & Strategic\n",
      "ð§© Token 2296:  Movements short\n",
      "3 TelefÃ³nica &\n",
      "ð§© Token 2297:  Cellnex: Telecommunications Tower Sales & Market Dynamics long\n",
      "ð§© Token 2298:  long\n",
      "4 CaixaBank: Mergers\n",
      "ð§© Token 2299:  and Strategic Moves in the Banking Sector\n",
      "5 Tele\n",
      "ð§© Token 2300: fÃ³nica, Indra, & MÃ¡sM\n",
      "ð§© Token 2301: Ã³vil: Regulatory and Strategic Moves in Telecom long\n",
      "ð§© Token 2302: \n",
      "6 Siemens Gamesa: Supply Agreements\n",
      "ð§© Token 2303: , Proï¬tability Targets in\n",
      "ð§© Token 2304:  Renewable\n",
      "Energy short\n",
      "7 Cellnex:\n",
      "ð§© Token 2305:  Strategic Acquisitions and Financial Moves in Telecom Infrastructure long\n",
      "ð§© Token 2306: \n",
      "8 Acciona, Endesa, Enag\n",
      "ð§© Token 2307: Ã¡s & Naturgy: Strategic Moves & Regulatory\n",
      "ð§© Token 2308: \n",
      "Developments in the Energy Sector long\n",
      "9\n",
      "ð§© Token 2309:  Repsol: Strategic Moves and Challenges in the Energy\n",
      "ð§© Token 2310:  Sector long\n",
      "10 Ferrovial, Acciona\n",
      "ð§© Token 2311: : Strategic Expansions and Financial Maneuvers\n",
      "ð§© Token 2312:  in\n",
      "Infrastructure short short\n",
      "11 Solaria\n",
      "ð§© Token 2313: : Strategic Moves and Market Challenges in Renewable Energy\n",
      "ð§© Token 2314:  long long\n",
      "12 Iberdrola:\n",
      "ð§© Token 2315:  Strategic Collaborations and Renewable Energy Developments short\n",
      "ð§© Token 2316: \n",
      "13 IAG: Financial Performance long\n",
      "14\n",
      "ð§© Token 2317:  Santander & CaixaBank: Financial Moves\n",
      "ð§© Token 2318:  and Sustainability Initiatives short\n",
      "15 ACS\n",
      "ð§© Token 2319:  & Acciona: Strategic Movements and Infrastructure Projects\n",
      "ð§© Token 2320:  short short\n",
      "16 TelefÃ³nica: Financial\n",
      "ð§© Token 2321:  Performance and Strategic Moves long\n",
      "17 MeliÃ¡\n",
      "ð§© Token 2322:  and Spanish Tourism Sector: Challenges Amidst the Pand\n",
      "ð§© Token 2323: emic short\n",
      "18 Takeover Bids for N\n",
      "ð§© Token 2324: aturgy and MÃ¡sMÃ³vil short\n",
      "\n",
      "ð§© Token 2325: 19 Naturgy: Financial Performance short short\n",
      "\n",
      "ð§© Token 2326: 20 PharmaMar, Grifols: Regulatory Appro\n",
      "ð§© Token 2327: vals and Market Moves in the\n",
      "Pharmaceutical\n",
      "ð§© Token 2328:  Sector long long\n",
      "21 Repsol: Financial Performance\n",
      "ð§© Token 2329:  long long\n",
      "22 Aena: Financial Performance long\n",
      "ð§© Token 2330:  long\n",
      "23 EnagÃ¡s, Endesa,\n",
      "ð§© Token 2331:  Iberdrola, Red ElÃ©ct\n",
      "ð§© Token 2332: rica: Regulatory and Market Challenges\n",
      "in the Energy\n",
      "ð§© Token 2333:  Sector short\n",
      "24 BBVA, Caixa\n",
      "ð§© Token 2334: Bank, Banco Sabadell: Layo\n",
      "ð§© Token 2335: ï¬s and Restructuring long long\n",
      "ð§© Token 2336: \n",
      "25 Inditex, Acerinox:\n",
      "ð§© Token 2337:  Market Performance and Strategic Developments in the\n",
      "Post\n",
      "ð§© Token 2338: -Covid Context short short\n",
      "Note:\n",
      "ð§© Token 2339:  Mapping of embeddings-based KMe\n",
      "ð§© Token 2340: ans clusters to their Trading Signal (long/short\n",
      "ð§© Token 2341: ) for the two proposed\n",
      "cluster-selection\n",
      "ð§© Token 2342:  algorithms (Greedy and Stable). The Gre\n",
      "ð§© Token 2343: edy algorithm longs (shorts) clusters that\n",
      "ð§© Token 2344:  maximize (mini-\n",
      "mize) the cluster\n",
      "ð§© Token 2345: -average-SR in the validation sample subject to\n",
      "ð§© Token 2346:  a positivity (negativity) constraint, while\n",
      "ð§© Token 2347:  the Stable\n",
      "algorithm longs (sh\n",
      "ð§© Token 2348: orts) clusters that minimize the rank diï¿½\n",
      "ð§© Token 2349: ï¿½erence between the training and validation rankings of\n",
      "\n",
      "ð§© Token 2350: the cluster-average-SRâs subject\n",
      "ð§© Token 2351:  to a positivity (negativity) constraint,\n",
      "ð§© Token 2352:  which is now imposed on both sample splits.\n",
      "\n",
      "ð§© Token 2353: In both algorithms, the cardinality of each leg\n",
      "ð§© Token 2354:  is upper-bounded by a hyperparameter\n",
      "ð§© Token 2355:  Î¸. Cluster labels are proposed\n",
      "based on\n",
      "ð§© Token 2356:  the articles they pool.\n",
      "31\n",
      "ð§© Token 2357: Table 4: Mapping of LLM-based\n",
      "ð§© Token 2358:  clusters to Trading Signals\n",
      "Cluster Greedy\n",
      "ð§© Token 2359:  Stable\n",
      "0 (demand, minor, positive\n",
      "ð§© Token 2360: )\n",
      "1 (demand, minor, negative)\n",
      "ð§© Token 2361:  short\n",
      "2 (demand, major, positive)\n",
      "ð§© Token 2362:  short short\n",
      "3 (demand, major, negative\n",
      "ð§© Token 2363: ) long long\n",
      "4 (supply, minor\n",
      "ð§© Token 2364: , positive) long\n",
      "5 (supply,\n",
      "ð§© Token 2365:  minor, negative) short\n",
      "6 (supply\n",
      "ð§© Token 2366: , major, positive) long\n",
      "7 (supp\n",
      "ð§© Token 2367: ly, major, negative) short\n",
      "8 (\n",
      "ð§© Token 2368: ï¬nancial, minor, positive)\n",
      "ð§© Token 2369:  long long\n",
      "9 (ï¬nancial\n",
      "ð§© Token 2370: , minor, negative) short\n",
      "10 (ï¿½\n",
      "ð§© Token 2371: ï¿½ï¿½nancial, major, positive) long\n",
      "ð§© Token 2372: \n",
      "11 (ï¬nancial, major\n",
      "ð§© Token 2373: , negative) short\n",
      "12 (technology, minor\n",
      "ð§© Token 2374: , positive) long\n",
      "13 (technology, minor\n",
      "ð§© Token 2375: , negative)\n",
      "14 (technology, major,\n",
      "ð§© Token 2376:  positive) short\n",
      "15 (technology, major,\n",
      "ð§© Token 2377:  negative)\n",
      "16 (policy, minor, positive\n",
      "ð§© Token 2378: ) short short\n",
      "17 (policy, minor,\n",
      "ð§© Token 2379:  negative) short short\n",
      "18 (policy, major\n",
      "ð§© Token 2380: , positive) short short\n",
      "19 (policy,\n",
      "ð§© Token 2381:  major, negative) short short\n",
      "Note: M\n",
      "ð§© Token 2382: apping of LLM-based clusters to their Trading\n",
      "ð§© Token 2383:  Signal (long/short) for the two proposed\n",
      "ð§© Token 2384:  cluster-selection\n",
      "algorithms (Greedy\n",
      "ð§© Token 2385:  and Stable). The Greedy algorithm longs\n",
      "ð§© Token 2386:  (shorts) clusters that maximize (minimize\n",
      "ð§© Token 2387: ) the cluster-\n",
      "average-SR in the\n",
      "ð§© Token 2388:  validation sample subject to a positivity (negativity\n",
      "ð§© Token 2389: ) constraint, while the Stable algorithm longs\n",
      "ð§© Token 2390: \n",
      "(shorts) clusters that minimize the rank\n",
      "ð§© Token 2391:  diï¬erence between the training and validation\n",
      "ð§© Token 2392:  rankings of the cluster-\n",
      "average-SRï¿½\n",
      "ð§© Token 2393: ï¿½s subject to a positivity (negativity\n",
      "ð§© Token 2394: ) constraint, which is now imposed on both sample\n",
      "ð§© Token 2395:  splits. In both\n",
      "algorithms, the\n",
      "ð§© Token 2396:  cardinality of each leg is upper-bounded\n",
      "ð§© Token 2397:  by a hyperparameter Î¸. Each cluster\n",
      "ð§© Token 2398:  corresponds to a type\n",
      "of news-implied\n",
      "ð§© Token 2399:  ï¬rm-speciï¬\n",
      "ð§© Token 2400: c shock identiï¬ed by our\n",
      "ð§© Token 2401:  LLM according to the function calling schema.\n",
      "\n",
      "ð§© Token 2402: 32\n",
      "ð§© Token 2403: Table 5: Portfolio Statistics Comparison: KMe\n",
      "ð§© Token 2404: ans vs LLM Clustering\n",
      "(a\n",
      "ð§© Token 2405: ) Panel A: Statistics of PKMeans\n",
      "\n",
      "ð§© Token 2406: Split Algo. Cum.\n",
      "Ret.\n",
      "\n",
      "ð§© Token 2407: Avg.\n",
      "Ret.\n",
      "St.\n",
      "Dev\n",
      "ð§© Token 2408: .\n",
      "Sharpe\n",
      "Ra-\n",
      "tio\n",
      "ð§© Token 2409: \n",
      "Sortino\n",
      "Ra-\n",
      "tio\n",
      "\n",
      "ð§© Token 2410: Max.\n",
      "DD\n",
      "Calmar\n",
      "Ratio\n",
      "ð§© Token 2411: \n",
      "Skew. Exc.\n",
      "Kurt\n",
      "ð§© Token 2412: .\n",
      "VaR\n",
      "95%\n",
      "CVa\n",
      "ð§© Token 2413: R\n",
      "95%\n",
      "All Greedy 1.\n",
      "ð§© Token 2414: 070 5.3 9.7 0.5\n",
      "ð§© Token 2415:  0.6 -6.9 0.8\n",
      "ð§© Token 2416:  -0.45 4.04 -13.\n",
      "ð§© Token 2417: 7 -22.9\n",
      "Stable 1.\n",
      "ð§© Token 2418: 489 35.8 16.8 1.8\n",
      "ð§© Token 2419:  2.2 -7.6 4.7\n",
      "ð§© Token 2420:  0.19 5.08 -22.6\n",
      "ð§© Token 2421:  -36.1\n",
      "Train Greedy 0.\n",
      "ð§© Token 2422: 959 -6.2 11.7 -\n",
      "ð§© Token 2423: 0.5 -0.5 -6.\n",
      "ð§© Token 2424: 9 -0.9 -0.52 2\n",
      "ð§© Token 2425: .72 -18.3 -28.5\n",
      "ð§© Token 2426: \n",
      "Stable 1.250 40.4 19\n",
      "ð§© Token 2427: .6 1.7 2.0 -7\n",
      "ð§© Token 2428: .6 5.3 -0.22 3\n",
      "ð§© Token 2429: .24 -29.3 -43.1\n",
      "ð§© Token 2430: \n",
      "Validation Greedy 1.088 26.\n",
      "ð§© Token 2431: 8 7.3 3.3 3.7\n",
      "ð§© Token 2432:  -3.5 7.8 -0.\n",
      "ð§© Token 2433: 47 1.17 -9.5 -15\n",
      "ð§© Token 2434: .9\n",
      "Stable 1.149 47.\n",
      "ð§© Token 2435: 6 13.3 2.9 3.5\n",
      "ð§© Token 2436:  -3.6 13.1 -0.\n",
      "ð§© Token 2437: 19 1.76 -18.3 -28\n",
      "ð§© Token 2438: .1\n",
      "Test Greedy 1.014 4\n",
      "ð§© Token 2439: .9 6.9 0.7 1.\n",
      "ð§© Token 2440: 0 -3.6 1.4 1.\n",
      "ð§© Token 2441: 85 5.50 -7.8 -9\n",
      "ð§© Token 2442: .7\n",
      "Stable 1.008 2.\n",
      "ð§© Token 2443: 9 14.3 0.2 0.3\n",
      "ð§© Token 2444:  -4.6 0.6 2.46\n",
      "ð§© Token 2445:  14.57 -18.9 -26.\n",
      "ð§© Token 2446: 8\n",
      "(b) Panel B: Statistics of\n",
      "ð§© Token 2447:  PLLM\n",
      "Split Algo. Cum.\n",
      "ð§© Token 2448: \n",
      "Ret.\n",
      "Avg.\n",
      "Ret.\n",
      "\n",
      "ð§© Token 2449: St.\n",
      "Dev.\n",
      "Sharpe\n",
      "Ra\n",
      "ð§© Token 2450: -\n",
      "tio\n",
      "Sortino\n",
      "Ra-\n",
      "ð§© Token 2451: \n",
      "tio\n",
      "Max.\n",
      "DD\n",
      "Cal\n",
      "ð§© Token 2452: mar\n",
      "Ratio\n",
      "Skew. Exc\n",
      "ð§© Token 2453: .\n",
      "Kurt.\n",
      "VaR\n",
      "95\n",
      "ð§© Token 2454: %\n",
      "CVaR\n",
      "95%\n",
      "All\n",
      "ð§© Token 2455:  Greedy 1.310 23.1 9.\n",
      "ð§© Token 2456: 6 2.2 2.9 -6.\n",
      "ð§© Token 2457: 3 3.7 1.47 9.93\n",
      "ð§© Token 2458:  -13.6 -18.9\n",
      "St\n",
      "ð§© Token 2459: able 1.365 27.0 8.6\n",
      "ð§© Token 2460:  2.8 3.4 -5.9\n",
      "ð§© Token 2461:  4.6 0.28 2.24 -\n",
      "ð§© Token 2462: 11.9 -16.9\n",
      "Train Gre\n",
      "ð§© Token 2463: edy 1.112 17.6 11.4\n",
      "ð§© Token 2464:  1.4 1.9 -6.3\n",
      "ð§© Token 2465:  2.8 1.65 9.00 -\n",
      "ð§© Token 2466: 15.7 -21.0\n",
      "Stable\n",
      "ð§© Token 2467:  1.177 28.3 9.9 2\n",
      "ð§© Token 2468: .5 3.0 -5.9 4\n",
      "ð§© Token 2469: .8 0.16 1.71 -13\n",
      "ð§© Token 2470: .5 -19.6\n",
      "Validation Gre\n",
      "ð§© Token 2471: edy 1.091 28.0 8.\n",
      "ð§© Token 2472: 2 3.0 4.0 -3.\n",
      "ð§© Token 2473: 1 9.1 0.14 1.37\n",
      "ð§© Token 2474:  -10.6 -16.8\n",
      "St\n",
      "ð§© Token 2475: able 1.048 14.2 7.0\n",
      "ð§© Token 2476:  1.9 2.1 -1.9\n",
      "ð§© Token 2477:  7.4 0.25 1.37 -\n",
      "ð§© Token 2478: 11.1 -14.7\n",
      "Test Gre\n",
      "ð§© Token 2479: edy 1.084 30.8 6.\n",
      "ð§© Token 2480: 2 4.3 6.0 -1.\n",
      "ð§© Token 2481: 5 21.0 1.49 8.30\n",
      "ð§© Token 2482:  -6.9 -9.9\n",
      "St\n",
      "ð§© Token 2483: able 1.100 37.2 7.1\n",
      "ð§© Token 2484:  4.4 7.2 -1.1\n",
      "ð§© Token 2485:  34.5 0.84 1.95 -\n",
      "ð§© Token 2486: 9.5 -11.3\n",
      "Note:\n",
      "ð§© Token 2487:  Portfolio statistics of trading strategies based on clusters obtained\n",
      "ð§© Token 2488:  from KMeans (Panel A) and LL\n",
      "ð§© Token 2489: M (Panel\n",
      "B) approaches. The statistics\n",
      "ð§© Token 2490:  provided include performance metrics (Cumulative Return,\n",
      "ð§© Token 2491:  Average Return (%)), risk\n",
      "measures (Standard Dev\n",
      "ð§© Token 2492: iation (%), Maximum Drawdown (%), Value at\n",
      "ð§© Token 2493:  Risk (%), Conditional Value at Risk (%)),\n",
      "ð§© Token 2494: \n",
      "risk-adjusted performance ratios (Sharpe Ratio\n",
      "ð§© Token 2495: , Sortino Ratio, Calmar Ratio), and\n",
      "ð§© Token 2496:  return distribution characteristics\n",
      "(Skewness,\n",
      "ð§© Token 2497:  Excess Kurtosis). These statistics are provided for\n",
      "ð§© Token 2498:  both cluster-selection algorithms: Greedy and St\n",
      "ð§© Token 2499: able.\n",
      "Except for the Cumulative Return,\n",
      "ð§© Token 2500:  all returns are annualized. The Sharpe Ratio\n",
      "ð§© Token 2501:  is computed using the daily returns,\n",
      "assuming 252\n",
      "ð§© Token 2502:  trading days in a year. The Sortino Ratio\n",
      "ð§© Token 2503:  is calculated using the daily downside returns. The\n",
      "\n",
      "ð§© Token 2504: Maximum Drawdown is the maximum loss from a peak\n",
      "ð§© Token 2505:  to a trough. The Calmar Ratio is the\n",
      "ð§© Token 2506:  ratio of the annualized\n",
      "return to the maximum\n",
      "ð§© Token 2507:  drawdown. Skewness measures the asymmetry\n",
      "ð§© Token 2508:  of the return distribution, while Kurtosis\n",
      "qu\n",
      "ð§© Token 2509: antiï¬es the tailsâth\n",
      "ð§© Token 2510: ickness. The Value at Risk (VaR\n",
      "ð§© Token 2511: ) and Conditional Value at Risk (CVa\n",
      "ð§© Token 2512: R) are calculated at a\n",
      "95% con\n",
      "ð§© Token 2513: ï¬dence level. The Greedy\n",
      "ð§© Token 2514:  algorithm longs (shorts) clusters that maximize\n",
      "ð§© Token 2515:  (minimize) the cluster-average-\n",
      "\n",
      "ð§© Token 2516: SR in the validation sample subject to a positivity\n",
      "ð§© Token 2517:  (negativity) constraint, while the Stable\n",
      "ð§© Token 2518:  algorithm longs (shorts)\n",
      "clusters\n",
      "ð§© Token 2519:  that minimize the rank diï¬erence between\n",
      "ð§© Token 2520:  the training and validation rankings of the cluster-average\n",
      "ð§© Token 2521: -SRâs\n",
      "subject to a pos\n",
      "ð§© Token 2522: itivity (negativity) constraint, which is now\n",
      "ð§© Token 2523:  imposed on both sample splits. In both algorithms,\n",
      "ð§© Token 2524:  the\n",
      "cardinality of each leg is upper\n",
      "ð§© Token 2525: -bounded by a hyperparameter Î¸\n",
      "ð§© Token 2526: . The holding period of the beta-neutral positions\n",
      "ð§© Token 2527:  is\n",
      "set to L = 4 trading days for\n",
      "ð§© Token 2528:  both approaches. The number of traded clusters is ï¿½\n",
      "ð§© Token 2529: ï¿½ = 0.5k = 13 for K\n",
      "ð§© Token 2530: Means (kâ = 26\n",
      "cl\n",
      "ð§© Token 2531: usters) and Î¸ = 0.5k\n",
      "ð§© Token 2532:  = 10 for LLM (kâ =\n",
      "ð§© Token 2533:  20 clusters). The selection criteria for these hyperparam\n",
      "ð§© Token 2534: eters (L, Î¸) is\n",
      "based\n",
      "ð§© Token 2535:  on maximizing the Sharpe Ratios of the train\n",
      "ð§© Token 2536:  and validation samples.\n",
      "33\n",
      "ð§© Token 2537: Figure 1: Word Cloud of all the dataset\n",
      "\n",
      "ð§© Token 2538: Note: This Word Cloud visualizes the most frequent\n",
      "ð§© Token 2539:  words in our dataset of Spanish business news articles.\n",
      "ð§© Token 2540:  Larger\n",
      "words correspond to higher frequencies. The\n",
      "ð§© Token 2541:  color of the words is purely for visual diï¿½\n",
      "ð§© Token 2542: ï¿½ï¿½erentiation and holds no\n",
      "additional\n",
      "ð§© Token 2543:  meaning. The most prominent words include âemp\n",
      "ð§© Token 2544: resaâ (ï¬rm), ï¿½\n",
      "ð§© Token 2545: ï¿½compaÃ±Ã­aâ (company),\n",
      "ð§© Token 2546:  and âespaÃ±aâ\n",
      "(\n",
      "ð§© Token 2547: Spain), reinforcing that the dataset primarily comprises Spanish business\n",
      "ð§© Token 2548:  news, with a prevalence of technical\n",
      "terms such\n",
      "ð§© Token 2549:  as âbeneï¬cio\n",
      "ð§© Token 2550:  netoâ (net proï¬\n",
      "ð§© Token 2551: t), âprecio objetivo\n",
      "ð§© Token 2552: â (target price), âproy\n",
      "ð§© Token 2553: ectoâ (project), and â\n",
      "ð§© Token 2554: operaciÃ³nâ\n",
      "(operation).\n",
      "\n",
      "ð§© Token 2555: 34\n",
      "ð§© Token 2556: Figure 2: Histogram of # News Articles per\n",
      "ð§© Token 2557:  Day and # Words per Article\n",
      "0 5 10\n",
      "ð§© Token 2558:  15 20 25 300\n",
      "10\n",
      "20\n",
      "30\n",
      "ð§© Token 2559: Frequency\n",
      "HistogramDensity\n",
      "0ï¿½\n",
      "ð§© Token 2560: ï¿½ï¿½00\n",
      "0â¿02\n",
      "\n",
      "ð§© Token 2561: 0â¿04\n",
      "0â¿\n",
      "ð§© Token 2562: 06\n",
      "0â¿08\n",
      "0ï¿½\n",
      "ð§© Token 2563: ï¿½ï¿½10Density\n",
      "(a) Number\n",
      "ð§© Token 2564:  of News Articles per Day\n",
      "0 25050075\n",
      "ð§© Token 2565: 0100012500\n",
      "200\n",
      "400\n",
      "600F\n",
      "ð§© Token 2566: requency\n",
      "HistogramDensity\n",
      "0ï¿½\n",
      "ð§© Token 2567: ï¿½000\n",
      "0â¿001\n",
      "0\n",
      "ð§© Token 2568: â¿002\n",
      "0â¿003\n",
      "ð§© Token 2569: \n",
      "0â¿004\n",
      "Density\n",
      "\n",
      "ð§© Token 2570: (b) Number of Words per Article\n",
      "Note\n",
      "ð§© Token 2571: : Panel (a) displays the distribution of the\n",
      "ð§© Token 2572:  number of news articles published per day, with most\n",
      "ð§© Token 2573:  days having\n",
      "between 5 and 10 articles. Panel\n",
      "ð§© Token 2574:  (b) shows the distribution of the number of\n",
      "ð§© Token 2575:  words per article, where the majority\n",
      "are between\n",
      "ð§© Token 2576:  70 and 280 words, suggesting concise reporting. However\n",
      "ð§© Token 2577: , the long right tail indicates instances of more\n",
      "\n",
      "ð§© Token 2578: comprehensive coverage.\n",
      "35\n",
      "ð§© Token 2579: Figure 3: Time Series of Number of Articles per\n",
      "ð§© Token 2580:  Day and 30-Period Moving Average\n",
      "Jul\n",
      "ð§© Token 2581: 2020 Sep Nov Jan2021 Mar May Jul Sep\n",
      "ð§© Token 2582: 0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "\n",
      "ð§© Token 2583: 25\n",
      "30# Articles Published\n",
      "# of Articles\n",
      "ð§© Token 2584:  Pulished Per DayMA (30)\n",
      "Note\n",
      "ð§© Token 2585: : The time series shows the daily number of news\n",
      "ð§© Token 2586:  articles published, characterized by signiï¬\n",
      "ð§© Token 2587: cant variability with\n",
      "occasional sharp spikes\n",
      "ð§© Token 2588: . The 30-day moving average smooths these\n",
      "ð§© Token 2589:  ï¬uctuations, revealing an average publication\n",
      "ð§© Token 2590: \n",
      "rate of 5 to 10 articles per day.\n",
      "ð§© Token 2591: \n",
      "36\n",
      "ð§© Token 2592: Figure 4: Imputation of the eï¿½\n",
      "ð§© Token 2593: ï¿½ï¿½ective treatment date ( Ëdi\n",
      "\n",
      "ð§© Token 2594: 0)\n",
      "Case 1: Treatment date is the\n",
      "ð§© Token 2595:  same as the publication date; Ëdi\n",
      "\n",
      "ð§© Token 2596: 0 = di\n",
      "0\n",
      "Case 1a:\n",
      "ð§© Token 2597:  News article published in a trading date and\n",
      "before\n",
      "ð§© Token 2598:  the market opens; Ëdi\n",
      "0 ï¿½\n",
      "ð§© Token 2599: ï¿½ Ëd â§ ti\n",
      "0 <\n",
      "ð§© Token 2600:  09:30\n",
      "09:3000:00 17\n",
      "ð§© Token 2601: :30 00:00\n",
      "Closing\n",
      "price\n",
      "ð§© Token 2602: \n",
      "Case 1b: News article published in a\n",
      "ð§© Token 2603:  trading date\n",
      "and during market hours; Ë\n",
      "ð§© Token 2604: di\n",
      "0 â Ëd â§\n",
      "ð§© Token 2605:  ti\n",
      "0 â [09:30,\n",
      "ð§© Token 2606:  17:30]\n",
      "09:3000:00\n",
      "ð§© Token 2607:  17:30 00:00\n",
      "Closing\n",
      "\n",
      "ð§© Token 2608: price\n",
      "Case 2: Treatment date is the next\n",
      "ð§© Token 2609:  closest trading day to publication; Ëdi\n",
      "\n",
      "ð§© Token 2610: 0 = Î(di\n",
      "0)\n",
      "\n",
      "ð§© Token 2611: Case 2a: News article published in a trading\n",
      "ð§© Token 2612:  day but\n",
      "after the market is closed for that\n",
      "ð§© Token 2613:  day; di\n",
      "0 â Ëd\n",
      "ð§© Token 2614:  â§ ti\n",
      "0 >\n",
      "17:30\n",
      "ð§© Token 2615: \n",
      "09:3000:00 17:30 00\n",
      "ð§© Token 2616: :00\n",
      "Closing\n",
      "price\n",
      "09:\n",
      "ð§© Token 2617: 3000:00 17:30\n",
      "Closing\n",
      "\n",
      "ð§© Token 2618: price\n",
      "Case 2b: News article published in\n",
      "ð§© Token 2619:  a non-trading\n",
      "day; di\n",
      "\n",
      "ð§© Token 2620: 0 ââ Ëd\n",
      "00\n",
      "ð§© Token 2621: :00 09:30 17:30 00:\n",
      "ð§© Token 2622: 00\n",
      "Closing\n",
      "price\n",
      "00:00\n",
      "ð§© Token 2623: \n",
      "Note: This ï¬gure illustrates\n",
      "ð§© Token 2624:  how we determine the eï¬ective treatment\n",
      "ð§© Token 2625:  date for news articles based on their publication\n",
      "tim\n",
      "ð§© Token 2626: ing relative to market hours. The Spanish stock market\n",
      "ð§© Token 2627:  operates from 09:30 to 17:30 on\n",
      "ð§© Token 2628:  trading days. News\n",
      "published during a trading day\n",
      "ð§© Token 2629:  during or before trading hours aï¬ect\n",
      "ð§© Token 2630: s stock prices on the same day (Cases\n",
      "ð§© Token 2631:  1a and\n",
      "1b), while news published\n",
      "ð§© Token 2632:  after market close or on non-trading days\n",
      "ð§© Token 2633:  aï¬ects prices on the next\n",
      "ð§© Token 2634:  available trading day\n",
      "(Cases 2a and\n",
      "ð§© Token 2635:  2b). This temporal mapping ensures we correctly align\n",
      "ð§© Token 2636:  news publication with the ï¬rst opportunity\n",
      "ð§© Token 2637: \n",
      "for market reaction.\n",
      "37\n",
      "ð§© Token 2638: Figure 5: Average Silhouette Scores in the\n",
      "ð§© Token 2639:  Training data\n",
      "0 10 20 30 40 50 60\n",
      "ð§© Token 2640:  70 80 90 100\n",
      "Cluster Size (k\n",
      "ð§© Token 2641: )\n",
      "0â¿050\n",
      "0ï¿½\n",
      "ð§© Token 2642: ï¿½ï¿½055\n",
      "0â¿060\n",
      "\n",
      "ð§© Token 2643: 0â¿065\n",
      "0ï¿½\n",
      "ð§© Token 2644: ï¿½070\n",
      "0â¿075\n",
      "Average\n",
      "ð§© Token 2645:  Silhouette Score\n",
      "kÂ§= 26\n",
      "\n",
      "ð§© Token 2646: Note: The plot presents the average silhouette scores calculated\n",
      "ð§© Token 2647:  on the training data Dtr for various cluster sizes\n",
      "ð§© Token 2648: \n",
      "k ranging from 2 to 100. The silhouette\n",
      "ð§© Token 2649:  score measures how well data points ï¬t\n",
      "ð§© Token 2650:  within their assigned cluster by\n",
      "comparing intra-\n",
      "ð§© Token 2651: cluster cohesion with inter-cluster separation.\n",
      "ð§© Token 2652:  A higher silhouette score (closer to +1\n",
      "ð§© Token 2653: ) indicates\n",
      "better-deï¬ned\n",
      "ð§© Token 2654:  clusters. The optimal number of clusters, kï¿½\n",
      "ð§© Token 2655: ï¿½ = 26, which maximizes the average silhouette\n",
      "ð§© Token 2656:  score, is\n",
      "marked by a vertical dashed green\n",
      "ð§© Token 2657:  line.\n",
      "38\n",
      "ð§© Token 2658: Figure 6: Distribution of articles through KMeans\n",
      "ð§© Token 2659:  clusters\n",
      "(a) All data (D)\n",
      "ð§© Token 2660: \n",
      "01234567891011121314\n",
      "ð§© Token 2661: 1516171819202122232425\n",
      "ð§© Token 2662: \n",
      "Cluster\n",
      "0\n",
      "50\n",
      "100\n",
      "\n",
      "ð§© Token 2663: 150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "\n",
      "ð§© Token 2664: 400Number of Articles\n",
      "0â¿00\n",
      "ð§© Token 2665: \n",
      "0â¿01\n",
      "0ï¿½\n",
      "ð§© Token 2666: ï¿½02\n",
      "0â¿03\n",
      "0\n",
      "ð§© Token 2667: â¿04\n",
      "0â¿05\n",
      "ð§© Token 2668: \n",
      "Density\n",
      "(b) Training data (\n",
      "ð§© Token 2669: Dtr)\n",
      "0123456789101\n",
      "ð§© Token 2670: 1121314151617181920212\n",
      "ð§© Token 2671: 22324250100200\n",
      "0â¿\n",
      "ð§© Token 2672: 00\n",
      "0â¿01\n",
      "0ï¿½\n",
      "ð§© Token 2673: ï¿½ï¿½02\n",
      "0â¿03\n",
      "\n",
      "ð§© Token 2674: 0â¿04\n",
      "0â¿\n",
      "ð§© Token 2675: 05\n",
      "0â¿06\n",
      "0ï¿½\n",
      "ð§© Token 2676: ï¿½ï¿½07\n",
      "(c) Validation data\n",
      "ð§© Token 2677:  (Dval)\n",
      "0123456789\n",
      "ð§© Token 2678: 10111213141516171819202\n",
      "ð§© Token 2679: 1222324250\n",
      "50\n",
      "100\n",
      "0\n",
      "ð§© Token 2680: â¿00\n",
      "0â¿01\n",
      "ð§© Token 2681: \n",
      "0â¿02\n",
      "0ï¿½\n",
      "ð§© Token 2682: ï¿½03\n",
      "0â¿04\n",
      "(\n",
      "ð§© Token 2683: d) Test data (Dtest)\n",
      "01\n",
      "ð§© Token 2684: 2345678910111213141516\n",
      "ð§© Token 2685: 17181920212223242502040\n",
      "ð§© Token 2686: 60\n",
      "0â¿00\n",
      "0ï¿½\n",
      "ð§© Token 2687: ï¿½ï¿½01\n",
      "0â¿02\n",
      "\n",
      "ð§© Token 2688: 0â¿03\n",
      "0â¿\n",
      "ð§© Token 2689: 04\n",
      "Note: This ï¬gure\n",
      "ð§© Token 2690:  presents the distribution of articles across the kâ\n",
      "ð§© Token 2691:  = 26 clusters, where the centroids were\n",
      "ð§© Token 2692:  determined\n",
      "by applying the KMeans algorithm to\n",
      "ð§© Token 2693:  the article embeddings from the training data.\n",
      "ð§© Token 2694:  Panel (a) shows the distribution\n",
      "for the\n",
      "ð§© Token 2695:  entire dataset (D), while Panels (b\n",
      "ð§© Token 2696: ), (c), and (d) illustrate the\n",
      "ð§© Token 2697:  distributions for the training (Dtr), validation\n",
      "\n",
      "ð§© Token 2698: (Dval), and test (Dtest)\n",
      "ð§© Token 2699:  datasets, respectively. The diï¬erences\n",
      "ð§© Token 2700:  in distribution across splits suggest some temporal\n",
      "instability\n",
      "ð§© Token 2701:  in the clustering results.\n",
      "39\n",
      "ð§© Token 2702: Figure 7: Distribution of articles through LLM clusters\n",
      "ð§© Token 2703: \n",
      "(a) All data (D)\n",
      "\n",
      "ð§© Token 2704: 0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\n",
      "ð§© Token 2705: 5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "ð§© Token 2706: 10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "\n",
      "ð§© Token 2707: 16\n",
      "17\n",
      "18\n",
      "19\n",
      "Cluster\n",
      "ð§© Token 2708: \n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "ð§© Token 2709: \n",
      "1000Number of Articles\n",
      "0â¿\n",
      "ð§© Token 2710: 000\n",
      "0â¿025\n",
      "0ï¿½\n",
      "ð§© Token 2711: ï¿½ï¿½050\n",
      "0â¿075\n",
      "\n",
      "ð§© Token 2712: 0â¿100\n",
      "0â¿\n",
      "ð§© Token 2713: 125\n",
      "0â¿150\n",
      "0ï¿½\n",
      "ð§© Token 2714: ï¿½ï¿½175\n",
      "Density\n",
      "(b)\n",
      "ð§© Token 2715:  Training data (Dtr)\n",
      "012345\n",
      "ð§© Token 2716: 678910111214161718190200\n",
      "ð§© Token 2717: 400\n",
      "0â¿000\n",
      "0ï¿½\n",
      "ð§© Token 2718: ï¿½ï¿½025\n",
      "0â¿050\n",
      "\n",
      "ð§© Token 2719: 0â¿075\n",
      "0â¿\n",
      "ð§© Token 2720: 100\n",
      "0â¿125\n",
      "0ï¿½\n",
      "ð§© Token 2721: ï¿½ï¿½150\n",
      "0â¿175\n",
      "\n",
      "ð§© Token 2722: (c) Validation data (Dval)\n",
      "ð§© Token 2723: \n",
      "01234567891011121314\n",
      "ð§© Token 2724: 161718190100200300\n",
      "0ï¿½\n",
      "ð§© Token 2725: ï¿½ï¿½00\n",
      "0â¿02\n",
      "\n",
      "ð§© Token 2726: 0â¿04\n",
      "0â¿\n",
      "ð§© Token 2727: 06\n",
      "0â¿08\n",
      "0ï¿½\n",
      "ð§© Token 2728: ï¿½ï¿½10\n",
      "0â¿12\n",
      "\n",
      "ð§© Token 2729: 0â¿14\n",
      "0â¿\n",
      "ð§© Token 2730: 16\n",
      "(d) Test data (Dtest\n",
      "ð§© Token 2731: )\n",
      "012345678910111214\n",
      "ð§© Token 2732: 161718190\n",
      "100\n",
      "200\n",
      "0\n",
      "ð§© Token 2733: â¿00\n",
      "0â¿02\n",
      "ð§© Token 2734: \n",
      "0â¿04\n",
      "0ï¿½\n",
      "ð§© Token 2735: ï¿½06\n",
      "0â¿08\n",
      "0\n",
      "ð§© Token 2736: â¿10\n",
      "0â¿12\n",
      "ð§© Token 2737: \n",
      "0â¿14\n",
      "Note: This\n",
      "ð§© Token 2738:  ï¬gure presents the distribution of news\n",
      "ð§© Token 2739:  articles across clusters derived using an LLM-based\n",
      "ð§© Token 2740:  approach.\n",
      "The upper plot shows the distribution for\n",
      "ð§© Token 2741:  the entire dataset (D), while the lower plots\n",
      "ð§© Token 2742:  display the distributions for\n",
      "the training (Dtr\n",
      "ð§© Token 2743: ), validation (Dval), and test (D\n",
      "ð§© Token 2744: test) datasets. Clusters 8, 9,\n",
      "ð§© Token 2745:  10, and 11, which capture ï¬\n",
      "ð§© Token 2746: nancial\n",
      "events or shocks, dominate the distribution\n",
      "ð§© Token 2747: , with cluster 8 (ï¬nancial\n",
      "ð§© Token 2748: , minor, positive) representing approximately\n",
      "one-\n",
      "ð§© Token 2749: third of the dataset. This cluster includes articles related\n",
      "ð§© Token 2750:  to ï¬nancial reports with mildly positive\n",
      "ð§© Token 2751:  outcomes,\n",
      "potentially oï¬ering\n",
      "ð§© Token 2752:  insight for long trading signals. Unlike KMeans\n",
      "ð§© Token 2753:  clustering with embeddings, this LLM\n",
      "ð§© Token 2754: -based\n",
      "clustering shows stable distributions across\n",
      "ð§© Token 2755:  data splits, highlighting the robustness of this method\n",
      "ð§© Token 2756:  over time.\n",
      "40\n",
      "ð§© Token 2757: 41\n",
      "ð§© Token 2758: Figure 8: Comparison of Cumulative Gross Returns across\n",
      "ð§© Token 2759:  Clustering Approaches\n",
      "(a) Panel\n",
      "ð§© Token 2760:  A: Cumulative Gross Returns of PKMeans\n",
      "ð§© Token 2761: \n",
      "Jul2020 Sep Nov Jan2021 Mar May\n",
      "ð§© Token 2762:  Jul Sep\n",
      "Time (trading days)\n",
      "\n",
      "ð§© Token 2763: 0â¿95\n",
      "1â¿\n",
      "ð§© Token 2764: 00\n",
      "1â¿05\n",
      "1ï¿½\n",
      "ð§© Token 2765: ï¿½ï¿½10\n",
      "1â¿15\n",
      "\n",
      "ð§© Token 2766: 1â¿20\n",
      "1â¿\n",
      "ð§© Token 2767: 25\n",
      "1â¿30\n",
      "Cum\n",
      "ð§© Token 2768: ulative Returns\n",
      "Train Validation Test\n",
      "Greedy\n",
      "ð§© Token 2769: Stable\n",
      "(b) Panel B: Cum\n",
      "ð§© Token 2770: ulative Gross Returns of PLLM\n",
      "Jul2020\n",
      "ð§© Token 2771:  Sep Nov Jan2021 Mar May Jul Sep\n",
      "\n",
      "ð§© Token 2772: Time (trading days)\n",
      "1ï¿½\n",
      "ð§© Token 2773: ï¿½000\n",
      "1â¿025\n",
      "1\n",
      "ð§© Token 2774: â¿050\n",
      "1â¿075\n",
      "ð§© Token 2775: \n",
      "1â¿100\n",
      "1ï¿½\n",
      "ð§© Token 2776: ï¿½125\n",
      "1â¿150\n",
      "1\n",
      "ð§© Token 2777: â¿175\n",
      "Cumulative Returns\n",
      "\n",
      "ð§© Token 2778: Train Validation Test\n",
      "GreedyStable\n",
      "\n",
      "ð§© Token 2779: Note: This ï¬gure presents the\n",
      "ð§© Token 2780:  cumulative gross returns of trading strategies based on KMe\n",
      "ð§© Token 2781: ans clustering (Panel A) and LLM\n",
      "ð§© Token 2782: \n",
      "clustering (Panel B) across di\n",
      "ð§© Token 2783: ï¬erent data splits. For both approaches\n",
      "ð§© Token 2784: , the holding period of the beta-neutral strategies\n",
      "ð§© Token 2785:  is set\n",
      "to L = 4 trading days.\n",
      "ð§© Token 2786:  The number of traded clusters diï¬ers\n",
      "ð§© Token 2787:  between approaches: Î¸ = â0\n",
      "ð§© Token 2788: .5kâ = 13 for K\n",
      "ð§© Token 2789: Means (kâ = 26\n",
      "cl\n",
      "ð§© Token 2790: usters) and Î¸ = â0\n",
      "ð§© Token 2791: .5kâ = 10 for LL\n",
      "ð§© Token 2792: M (k = 20 clusters). The selection criteria\n",
      "ð§© Token 2793:  for these parameters is based on maximizing\n",
      "the Shar\n",
      "ð§© Token 2794: pe Ratios of the train and validation samples.\n",
      "ð§© Token 2795:  The Test split is higlhighted with\n",
      "ð§© Token 2796:  a yellow background.\n",
      "42\n",
      "ð§© Token 2797: Figure 9: Sensitivity of SRPtest\n",
      "\n",
      "ð§© Token 2798: to the holding window length (L)\n",
      "(\n",
      "ð§© Token 2799: a) KMeans: Distribution of SRP\n",
      "ð§© Token 2800: test\n",
      "(L)\n",
      "Â°7ï¿½\n",
      "ð§© Token 2801: ï¿½5Â°5â¿0Â°2\n",
      "ð§© Token 2802: â¿50â¿02ï¿½\n",
      "ð§© Token 2803: ï¿½55â¿07â¿5\n",
      "ð§© Token 2804: Sharpe Ratio (Test)0â¿\n",
      "ð§© Token 2805: 000â¿050â¿100ï¿½\n",
      "ð§© Token 2806: ï¿½ï¿½150â¿200â¿\n",
      "ð§© Token 2807: 25Density\n",
      "GreedyStable\n",
      "(\n",
      "ð§© Token 2808: b) KMeans: Series of SRP\n",
      "ð§© Token 2809: test\n",
      "(L)\n",
      "2 5 7 101\n",
      "ð§© Token 2810: 2151720Holding period length (L)\n",
      "ð§© Token 2811: \n",
      "Â°4Â°2024Sharpe Ratio (\n",
      "ð§© Token 2812: Test)\n",
      "GreedyStable\n",
      "(c\n",
      "ð§© Token 2813: ) LLM: Distribution of SRPtest\n",
      "\n",
      "ð§© Token 2814: (L)\n",
      "Â°4 Â°2 0 2\n",
      "ð§© Token 2815:  4 6 8Sharpe Ratio (Test)0\n",
      "ð§© Token 2816: â¿000â¿050ï¿½\n",
      "ð§© Token 2817: ï¿½100â¿150â¿200\n",
      "ð§© Token 2818: â¿25Density\n",
      "GreedySt\n",
      "ð§© Token 2819: able\n",
      "(d) LLM: Series of\n",
      "ð§© Token 2820:  SRPtest\n",
      "(L)\n",
      "2 5\n",
      "ð§© Token 2821:  7 1012151720Holding period length (\n",
      "ð§© Token 2822: L)Â°1012345Sharpe Ratio (\n",
      "ð§© Token 2823: Test)\n",
      "GreedyStable\n",
      "Note:\n",
      "ð§© Token 2824:  This ï¬gure examines the sensitivity of\n",
      "ð§© Token 2825:  the Sharpe Ratios (SRPtest\n",
      "\n",
      "ð§© Token 2826: ) of the test portfolio to changes in the\n",
      "\n",
      "ð§© Token 2827: holding window length (L), with Î¸ ï¿½\n",
      "ð§© Token 2828: ï¿½ï¿½xed at â0.\n",
      "ð§© Token 2829: 5kâ. Panels (a\n",
      "ð§© Token 2830: ) and (b) display the distribution and time\n",
      "ð§© Token 2831:  series\n",
      "of SRPtest\n",
      "(L)\n",
      "ð§© Token 2832:  for KMeans clustering, respectively, while\n",
      "ð§© Token 2833:  Panels (c) and (d) present\n",
      "ð§© Token 2834:  the same for the LLM-based\n",
      "cl\n",
      "ð§© Token 2835: ustering. The left-hand panels show the\n",
      "ð§© Token 2836:  skewness of the distributions: KMeans\n",
      "ð§© Token 2837:  clustering results in a left-skewed\n",
      "ð§© Token 2838: \n",
      "distribution of Sharpe Ratios, whereas\n",
      "ð§© Token 2839:  the LLM-based approach yields a right-\n",
      "ð§© Token 2840: skewed distribution, indicating higher\n",
      "proï¿½\n",
      "ð§© Token 2841: ï¿½ï¿½tability. The right-hand panels\n",
      "ð§© Token 2842:  highlight that KMeans clustering only produces positive\n",
      "ð§© Token 2843:  Sharpe Ratios for very\n",
      "short holding periods\n",
      "ð§© Token 2844: , whereas the LLM-based clustering shows\n",
      "ð§© Token 2845:  more consistent positive performance across a wider\n",
      "range of\n",
      "ð§© Token 2846:  L values, though with some variability.\n",
      "43\n",
      "ð§© Token 2847: Figure 10: Sensitivity of SRPtest\n",
      "\n",
      "ð§© Token 2848: to the upper bound on the number of traded clusters\n",
      "ð§© Token 2849:  on each side (Î¸)\n",
      "(a\n",
      "ð§© Token 2850: ) KMeans: Distribution of SRPtest\n",
      "ð§© Token 2851: \n",
      "(Î¸)\n",
      "Â°6 Â°4\n",
      "ð§© Token 2852:  Â°2 0 2 4Sharpe Ratio (Test\n",
      "ð§© Token 2853: )0â¿00â¿10\n",
      "ð§© Token 2854: â¿20â¿30ï¿½\n",
      "ð§© Token 2855: ï¿½40â¿50â¿60\n",
      "ð§© Token 2856: â¿7Density\n",
      "GreedySt\n",
      "ð§© Token 2857: able\n",
      "(b) KMeans: Series\n",
      "ð§© Token 2858:  of SRPtest\n",
      "(Î¸)\n",
      "\n",
      "ð§© Token 2859: 1234567891011121314Number of\n",
      "ð§© Token 2860:  traded clusters (Âµ)\n",
      "Â°3Â°\n",
      "ð§© Token 2861: 2Â°1012Sharpe Ratio (Test)\n",
      "ð§© Token 2862: \n",
      "GreedyStable\n",
      "(c) LL\n",
      "ð§© Token 2863: M: Distribution of SRPtest\n",
      "(ï¿½\n",
      "ð§© Token 2864: ï¿½)\n",
      "Â°2 0 2 4 6Shar\n",
      "ð§© Token 2865: pe Ratio (Test)0â¿000\n",
      "ð§© Token 2866: â¿250â¿500ï¿½\n",
      "ð§© Token 2867: ï¿½751â¿001â¿251\n",
      "ð§© Token 2868: â¿501â¿752ï¿½\n",
      "ð§© Token 2869: ï¿½00Density\n",
      "GreedyStable\n",
      "\n",
      "ð§© Token 2870: (d) LLM: Series of SRP\n",
      "ð§© Token 2871: test\n",
      "(Î¸)\n",
      "1234567\n",
      "ð§© Token 2872: 891011121314Number of traded clusters (\n",
      "ð§© Token 2873: Âµ)\n",
      "012345Sharpe Ratio\n",
      "ð§© Token 2874:  (Test)\n",
      "GreedyStable\n",
      "Note\n",
      "ð§© Token 2875: : This ï¬gure displays the sensitivity\n",
      "ð§© Token 2876:  of the Sharpe Ratios (SRPtest\n",
      "ð§© Token 2877: \n",
      ") to variations in the upper bound on the\n",
      "ð§© Token 2878: \n",
      "number of traded clusters (Î¸), with\n",
      "ð§© Token 2879:  L ï¬xed at 4. Pan\n",
      "ð§© Token 2880: els (a) and (b) show the\n",
      "ð§© Token 2881:  distribution and series of SRPtest\n",
      "(ï¿½\n",
      "ð§© Token 2882: ï¿½)\n",
      "for KMeans clustering,\n",
      "ð§© Token 2883:  respectively, while Panels (c) and (\n",
      "ð§© Token 2884: d) illustrate the same for LLM-based\n",
      "ð§© Token 2885:  clustering. For\n",
      "KMeans, the\n",
      "ð§© Token 2886:  results are mixed: the Stable algorithm generates positive\n",
      "ð§© Token 2887:  Sharpe Ratios for low Î¸ values,\n",
      "ð§© Token 2888:  whereas the\n",
      "Greedy algorithm performs better with high\n",
      "ð§© Token 2889:  Î¸ values, indicating sensitivity and instability. In\n",
      "ð§© Token 2890:  contrast, the LLM-\n",
      "based clustering\n",
      "ð§© Token 2891:  shows a more consistent pattern, with a concentration of\n",
      "ð§© Token 2892:  positive Sharpe Ratios across a broader\n",
      "range\n",
      "ð§© Token 2893:  of Î¸ values, suggesting greater robustness and\n",
      "ð§© Token 2894:  stability in the trading strategy.\n",
      "44\n",
      "ð§© Token 2895: A. Appendix\n",
      "A.1 KMeans\n",
      "ð§© Token 2896:  Algorithm\n",
      "Algorithm 1. KMeans\n",
      "ð§© Token 2897:  Clustering Algorithm\n",
      "1: Input:\n",
      "ð§© Token 2898:  Embedding vectors {e1, e2\n",
      "ð§© Token 2899: , . . . , eN }, number of\n",
      "ð§© Token 2900:  clusters k\n",
      "2: Output: Cluster assignments {\n",
      "ð§© Token 2901: D1, D2, . . . ,\n",
      "ð§© Token 2902:  Dk}, centroids {c1,\n",
      "ð§© Token 2903:  c2, . . . , ck}\n",
      "ð§© Token 2904: \n",
      "3: Initialize centroids {c\n",
      "ð§© Token 2905: 1, c2, . . . , c\n",
      "ð§© Token 2906: k} randomly\n",
      "4: repeat\n",
      "5:\n",
      "ð§© Token 2907:  Assignment Step:\n",
      "6: for each vector e\n",
      "ð§© Token 2908: i do\n",
      "7: Assign ei to\n",
      "ð§© Token 2909:  the nearest centroid:\n",
      "g = arg min\n",
      "ð§© Token 2910: ââ{1,...,k\n",
      "ð§© Token 2911: }\n",
      "ó°ei â cï¿½\n",
      "ð§© Token 2912: ï¿½ï¿½ó°2\n",
      "2\n",
      "\n",
      "ð§© Token 2913: 8: Update cluster assignments: Dg â\n",
      "ð§© Token 2914:  Dg âª {i}\n",
      "9:\n",
      "ð§© Token 2915:  end for\n",
      "10: Update Step:\n",
      "11\n",
      "ð§© Token 2916: : for each cluster Dg do\n",
      "12:\n",
      "ð§© Token 2917:  Recalculate centroid cg:\n",
      "\n",
      "ð§© Token 2918: cg = 1\n",
      "|Dg|\n",
      "\n",
      "ð§© Token 2919: ó°\n",
      "iâDg\n",
      "ð§© Token 2920: \n",
      "ei\n",
      "13: end for\n",
      "14:\n",
      "ð§© Token 2921:  until cluster assignments no longer change\n",
      "15: Return\n",
      "ð§© Token 2922:  cluster assignments {D1, D2, .\n",
      "ð§© Token 2923:  . . , Dk} and centroids\n",
      "ð§© Token 2924:  {c1, c2, . . .\n",
      "ð§© Token 2925:  , ck}\n",
      "45\n",
      "ð§© Token 2926: A.2 Hyperparameter Choice Justiï¿½\n",
      "ð§© Token 2927: ï¿½ï¿½cation\n",
      "Our hyperparameters are L\n",
      "ð§© Token 2928:  and Î¸. Recall that L denotes the number\n",
      "ð§© Token 2929:  of trading days over which we\n",
      "hold the positions\n",
      "ð§© Token 2930:  in the beta-neutral strategy, while Î¸\n",
      "ð§© Token 2931:  represents the upper bound on each side (long and\n",
      "ð§© Token 2932: \n",
      "short) for the amount of clusters we select\n",
      "ð§© Token 2933:  for the trading strategy. The speciï¿½\n",
      "ð§© Token 2934: ï¿½c choice of hyperparameters\n",
      "we made\n",
      "ð§© Token 2935:  for the results presented in the paper were:\n",
      "\n",
      "ð§© Token 2936: L = 4\n",
      "Î¸ = â\n",
      "ð§© Token 2937: 0.5kâ\n",
      "where k\n",
      "ð§© Token 2938:  represents the number of clusters (26 for KMe\n",
      "ð§© Token 2939: ans clustering, and 20 for LLM clust\n",
      "ð§© Token 2940: ering). This\n",
      "choice is not arbitrary nor opportun\n",
      "ð§© Token 2941: istic. Instead, it results from the maximization\n",
      "ð§© Token 2942:  of the Sharpe Ratio\n",
      "of the portfolio in\n",
      "ð§© Token 2943:  the train and validation samples for both KMeans\n",
      "ð§© Token 2944:  and LLM clustering. This choice\n",
      "pro\n",
      "ð§© Token 2945: cedure is completely based on in-sample criteria\n",
      "ð§© Token 2946:  and it prevents lookahead bias. The justi\n",
      "ð§© Token 2947: ï¬cation for\n",
      "such choices is made\n",
      "ð§© Token 2948:  below.\n",
      "A.2.1 KMe\n",
      "ð§© Token 2949: ans Clustering\n",
      "In Figure A1 we\n",
      "ð§© Token 2950:  can see that a choice of L = 4 in\n",
      "ð§© Token 2951:  the training and validation splits generates the most\n",
      "stable\n",
      "ð§© Token 2952:  Sharpe Ratio. Namely, In the train\n",
      "ð§© Token 2953:  set (Figure A1a), it makes more\n",
      "ð§© Token 2954:  sense to choose low values of\n",
      "L (less\n",
      "ð§© Token 2955:  than 4) to maximize the SR. However,\n",
      "ð§© Token 2956:  in the validation set (Figure A1b),\n",
      "ð§© Token 2957:  it makes more sense\n",
      "to choose higher values of\n",
      "ð§© Token 2958:  L. The choice of L = 4 represents a\n",
      "ð§© Token 2959:  balanced compromise, providing a stable\n",
      "Sharpe Ratio\n",
      "ð§© Token 2960:  proï¬le across both splits, ensuring\n",
      "ð§© Token 2961:  consistent in-sample performance.\n",
      "[Insert Figure\n",
      "ð§© Token 2962:  A1 about here]\n",
      "On the other hand\n",
      "ð§© Token 2963: , the choice of Î¸ = â\n",
      "ð§© Token 2964: 0.5 Â· 26â = 13\n",
      "ð§© Token 2965:  is a choice that pursues stability in the Shar\n",
      "ð§© Token 2966: pe\n",
      "Ratio of the train and validation portfolios\n",
      "ð§© Token 2967: . As we can see from Figure A2,\n",
      "ð§© Token 2968:  the Sharpe Ratios tend to\n",
      "conver\n",
      "ð§© Token 2969: ge to the highest and most stable value when we\n",
      "ð§© Token 2970:  choose the highest possible value of Î¸.\n",
      "\n",
      "ð§© Token 2971: [Insert Figure A2 about here]\n",
      "A\n",
      "ð§© Token 2972: .2.2 LLM Clustering\n",
      "\n",
      "ð§© Token 2973: Following a similar logic as below, the choice of\n",
      "ð§© Token 2974:  L = 4 sets a consensus between the maximization\n",
      "ð§© Token 2975:  of\n",
      "SRPtr\n",
      "and SRPval\n",
      "\n",
      "ð§© Token 2976: . That is, maximizing SRPtr\n",
      "requires lower\n",
      "ð§© Token 2977:  holding period lengths (the maximizer is\n",
      "L\n",
      "ð§© Token 2978:  = 4), while maximizing SRPval\n",
      "requires\n",
      "ð§© Token 2979:  increasing the window length. Among this contradiction, from\n",
      "ð§© Token 2980: \n",
      "Figure A3 it follows that L = 4\n",
      "ð§© Token 2981:  stands as a perfect choice to balance the maximization\n",
      "ð§© Token 2982:  requirements in\n",
      "both samples, generating a stable choice\n",
      "ð§© Token 2983:  for the holding period window length.\n",
      "[Insert\n",
      "ð§© Token 2984:  Figure A3 about here]\n",
      "46\n",
      "ð§© Token 2985: Finally, the same conclusion as in KMeans\n",
      "ð§© Token 2986:  applies here. By selecting Î¸ = ï¿½\n",
      "ð§© Token 2987: ï¿½0.5 Â· 20â =\n",
      "ð§© Token 2988:  10, we get\n",
      "a stable Sharpe Ratio\n",
      "ð§© Token 2989: . Even though we observe that SRPtr\n",
      "(\n",
      "ð§© Token 2990: L) falls momentarily at Î¸ = 10 for\n",
      "ð§© Token 2991:  the\n",
      "Greedy algorithm, it still constitutes a\n",
      "ð§© Token 2992:  good choice. Conversely, at Î¸ = 10\n",
      "ð§© Token 2993:  the greedy algorithm sees a\n",
      "jump in SRP\n",
      "ð§© Token 2994: val\n",
      "(L) (see Figure A4\n",
      "ð§© Token 2995: ). All in all, we can easily conclude that\n",
      "ð§© Token 2996:  Î¸ = â0.5k\n",
      "ð§© Token 2997: â arises as a good\n",
      "hyperp\n",
      "ð§© Token 2998: amrameter choice also for LLM clustering\n",
      "ð§© Token 2999: .\n",
      "[Insert Figure A4 about here]\n",
      "ð§© Token 3000: \n",
      "47\n",
      "ð§© Token 3001: A.3 Cluster-Average Sharpe Ratios\n",
      "ð§© Token 3002: \n",
      "The distribution of cluster-average Sharpe Rat\n",
      "ð§© Token 3003: ios across diï¬erent clusters reveals distinct\n",
      "ð§© Token 3004:  patterns\n",
      "between KMeans and LLM-\n",
      "ð§© Token 3005: based clustering approaches, as illustrated in Figure A\n",
      "ð§© Token 3006: 5\n",
      "Panel A presents the results for KMe\n",
      "ð§© Token 3007: ans clustering, where we observe remarkably consistent dist\n",
      "ð§© Token 3008: ribu-\n",
      "tional patterns across all three\n",
      "ð§© Token 3009:  data splits. The distributions are approximately symmetric around\n",
      "ð§© Token 3010:  zero,\n",
      "with the majority of Sharpe ratios\n",
      "ð§© Token 3011:  falling within the [â5, 5] range\n",
      "ð§© Token 3012: . The training set exhibits the high-\n",
      "est\n",
      "ð§© Token 3013:  density peak (approximately 0.17), followed closely\n",
      "ð§© Token 3014:  by the test set, while the validation set shows\n",
      "ð§© Token 3015:  a\n",
      "slightly lower peak density of about 0\n",
      "ð§© Token 3016: .125. Notable in the validation set are\n",
      "ð§© Token 3017:  small secondary peaks at the\n",
      "tails (around Â±\n",
      "ð§© Token 3018: 15), suggesting the presence of a few clusters with\n",
      "ð§© Token 3019:  extreme performance characteristics.\n",
      "This consistency across splits suggests\n",
      "ð§© Token 3020:  that the KMeans clustering approach produces stable\n",
      "ð§© Token 3021:  performance\n",
      "groupings.\n",
      "Panel B displays the\n",
      "ð§© Token 3022:  results for LLM-based clustering, revealing\n",
      "ð§© Token 3023:  more heterogeneous distributions\n",
      "across the splits.\n",
      "ð§© Token 3024:  The validation set demonstrates a pronounced peak near zero with\n",
      "ð§© Token 3025:  a maximum density\n",
      "of 0.2, indicating\n",
      "ð§© Token 3026:  strong concentration of performance in this region. In contrast\n",
      "ð§© Token 3027: , the training set exhibits\n",
      "a markedly diï¿½\n",
      "ð§© Token 3028: ï¿½ï¿½erent pattern, with a ï¬\n",
      "ð§© Token 3029: atter, more dispersed distribution extending from â20 to\n",
      "ð§© Token 3030:  +20,\n",
      "suggesting greater performance variability across\n",
      "ð§© Token 3031:  clusters. The test set presents an intermediate case,\n",
      "ð§© Token 3032: \n",
      "with moderate concentration around zero but maintaining signi\n",
      "ð§© Token 3033: ï¬cant mass in the positive region\n",
      "ð§© Token 3034: . This\n",
      "heterogeneity across splits might indicate that\n",
      "ð§© Token 3035:  the LLM-based clustering captures more nuanced\n",
      "ð§© Token 3036:  and\n",
      "potentially time-varying patterns\n",
      "ð§© Token 3037:  in the underlying data.\n",
      "The contrasting patterns between\n",
      "ð§© Token 3038:  the two clustering approaches suggest diï¬\n",
      "ð§© Token 3039: erent strengths: KMeans\n",
      "provides more\n",
      "ð§© Token 3040:  stable and consistent performance groupings, while LLM\n",
      "ð§© Token 3041: -based clustering potentially cap-\n",
      "tures\n",
      "ð§© Token 3042:  more complex relationships, albeit with greater variability across di\n",
      "ð§© Token 3043: ï¬erent data splits.\n",
      "[Insert\n",
      "ð§© Token 3044:  Figure A5 about here]\n",
      "48\n",
      "ð§© Token 3045: A.4 Optimal Cluster Selection Algorithms\n",
      "ð§© Token 3046: \n",
      "Algorithm 2. Greedy Selection | Top\n",
      "ð§© Token 3047:  average Sharpe Ratio in Validation Set\n",
      "1\n",
      "ð§© Token 3048: : Input: Set of clusters G = {1\n",
      "ð§© Token 3049: , 2, . . . , kâ\n",
      "ð§© Token 3050: }, Sharpe Ratios in the validation sample {\n",
      "ð§© Token 3051: SR(i,j)\n",
      "L }(\n",
      "ð§© Token 3052: i,j)âBval ,\n",
      "\n",
      "ð§© Token 3053: maximum number of traded clusters Î¸ â N\n",
      "ð§© Token 3054:  (usually, Î¸ â kâ\n",
      "ð§© Token 3055: )\n",
      "2: Output: Set of long-\n",
      "ð§© Token 3056: traded clusters G+\n",
      "Î¸ and set\n",
      "ð§© Token 3057:  of short-traded clusters Gâ\n",
      "ï¿½\n",
      "ð§© Token 3058: ï¿½\n",
      "Step #1: Compute Cluster Average\n",
      "ð§© Token 3059:  Sharpe Ratios in Validation Set\n",
      "3\n",
      "ð§© Token 3060: : for each g â G do\n",
      "4\n",
      "ð§© Token 3061: : Compute average Sharpe Ratio SRval\n",
      "\n",
      "ð§© Token 3062: g â 1\n",
      "|Bvalg |\n",
      "ð§© Token 3063: \n",
      "ó°\n",
      "(i,j\n",
      "ð§© Token 3064: )âBvalg SR(i,\n",
      "ð§© Token 3065: j)\n",
      "L\n",
      "5: end for\n",
      "\n",
      "ð§© Token 3066: Step #2: Identify Positive and Negative Shar\n",
      "ð§© Token 3067: pe Ratio Clusters\n",
      "6: Deï¿½\n",
      "ð§© Token 3068: ï¿½ne Gval\n",
      "SR+ â {\n",
      "ð§© Token 3069: g â G | SRval\n",
      "g >\n",
      "ð§© Token 3070:  0}\n",
      "7: Deï¬ne\n",
      "ð§© Token 3071:  Gval\n",
      "SRâ â {g ï¿½\n",
      "ð§© Token 3072: ï¿½ G | SRval\n",
      "g < 0}\n",
      "ð§© Token 3073: \n",
      "Step #3: Rank Clusters by Average\n",
      "ð§© Token 3074:  Sharpe Ratio in the Validation Set\n",
      "8\n",
      "ð§© Token 3075: : for each g â G do\n",
      "9\n",
      "ð§© Token 3076: : Rank the average Sharpe Ratio Rval\n",
      "\n",
      "ð§© Token 3077: g â ó°\n",
      "h\n",
      "ð§© Token 3078: âG 1\n",
      "ó°\n",
      "\n",
      "ð§© Token 3079: SRval\n",
      "h â¥ SRval\n",
      "g\n",
      "\n",
      "ð§© Token 3080: ó°\n",
      "10: end for\n",
      "\n",
      "ð§© Token 3081: Step #4: Select Top Î¸ Clusters\n",
      "ð§© Token 3082: \n",
      "11: Deï¬ne Î¸\n",
      "ð§© Token 3083: + â min(Î¸, |G\n",
      "ð§© Token 3084: val\n",
      "SR+ |) ; G+\n",
      "\n",
      "ð§© Token 3085: Î¸ â {g â G |\n",
      "ð§© Token 3086:  1 â¤ Rval\n",
      "g â¤ Î¸+\n",
      "ð§© Token 3087: }\n",
      "12: Deï¬ne ï¿½\n",
      "ð§© Token 3088: ï¿½â â min(Î¸, |\n",
      "ð§© Token 3089: Gval\n",
      "SRâ|) ; Gâ\n",
      "ð§© Token 3090: \n",
      "Î¸ â {g â G\n",
      "ð§© Token 3091:  | kâ â Î¸â < R\n",
      "ð§© Token 3092: val\n",
      "g â¤ kâ}\n",
      "13\n",
      "ð§© Token 3093: : Return Long-traded clusters G+\n",
      "\n",
      "ð§© Token 3094: Î¸ , Short-traded clusters Gâ\n",
      "ð§© Token 3095: \n",
      "Î¸\n",
      "49\n",
      "ð§© Token 3096: Algorithm 3. Rank Stability | Minimal Rank\n",
      "ð§© Token 3097:  Diï¬erence between Train & Validation\n",
      "ð§© Token 3098:  Sets\n",
      "1: Input: Set of clusters G\n",
      "ð§© Token 3099:  = {1, 2, . . . ,\n",
      "ð§© Token 3100:  kâ}, Sharpe Ratios in the\n",
      "ð§© Token 3101:  training and validation sample\n",
      "{SR(i,\n",
      "ð§© Token 3102: j)\n",
      "L }(i,j)\n",
      "ð§© Token 3103: âBtr and {SR(i,\n",
      "ð§© Token 3104: j)\n",
      "L }(i,j)\n",
      "ð§© Token 3105: âBval , maximum number of traded clusters\n",
      "ð§© Token 3106:  Î¸\n",
      "2: Output: Set of long\n",
      "ð§© Token 3107: -traded clusters G+\n",
      "Î¸ and\n",
      "ð§© Token 3108:  set of short-traded clusters Gâ\n",
      "\n",
      "ð§© Token 3109: Î¸\n",
      "Step #1: Compute Cluster\n",
      "ð§© Token 3110:  Average Sharpe Ratios in Training & Validation\n",
      "ð§© Token 3111:  Set\n",
      "3: for each g â G\n",
      "ð§© Token 3112:  do\n",
      "4: Compute average Sharpe Ratio\n",
      "ð§© Token 3113:  in Btr : SRtr\n",
      "g â\n",
      "ð§© Token 3114:  1\n",
      "|Btrg |\n",
      "ï¿½\n",
      "ð§© Token 3115: ï¿½ï¿½\n",
      "(i,j)â\n",
      "ð§© Token 3116: Btrg SR(i,j)\n",
      "\n",
      "ð§© Token 3117: L\n",
      "5: Compute average Sharpe Ratio\n",
      "ð§© Token 3118:  in Bval : SRval\n",
      "g â\n",
      "ð§© Token 3119:  1\n",
      "|Bvalg |\n",
      "ï¿½\n",
      "ð§© Token 3120: ï¿½ï¿½\n",
      "(i,j)â\n",
      "ð§© Token 3121: Bvalg SR(i,j)\n",
      "\n",
      "ð§© Token 3122: L\n",
      "6: end for\n",
      "Step #2\n",
      "ð§© Token 3123: : Rank Clusters\n",
      "7: for each g\n",
      "ð§© Token 3124:  â G do\n",
      "8: Rank the average\n",
      "ð§© Token 3125:  Sharpe Ratios in Btr : Rtr\n",
      "ð§© Token 3126: \n",
      "g â ó°\n",
      "\n",
      "ð§© Token 3127: hâG 1\n",
      "ó°\n",
      "ð§© Token 3128: \n",
      "SRtr\n",
      "h â¥ SRtr\n",
      "g\n",
      "ð§© Token 3129: \n",
      "ó°\n",
      "9: Rank the\n",
      "ð§© Token 3130:  average Sharpe Ratios in Bval : R\n",
      "ð§© Token 3131: val\n",
      "g â ó°\n",
      "ð§© Token 3132: \n",
      "hâG 1\n",
      "ï¿½\n",
      "ð§© Token 3133: ï¿½\n",
      "SRval\n",
      "h â¥ SRval\n",
      "\n",
      "ð§© Token 3134: g\n",
      "ó°\n",
      "10: end\n",
      "ð§© Token 3135:  for\n",
      "Step #3: Calculate Rank Di\n",
      "ð§© Token 3136: ï¬erences\n",
      "11: for each g\n",
      "ð§© Token 3137:  â G do\n",
      "12: Calculate rank\n",
      "ð§© Token 3138:  diï¬erence Î´g â\n",
      "ð§© Token 3139:  |Rtr\n",
      "g â Rval\n",
      "g\n",
      "ð§© Token 3140:  |\n",
      "13: end for\n",
      "Step #4\n",
      "ð§© Token 3141: : Select Top Î¸ Clusters with Smallest\n",
      "ð§© Token 3142:  Rank Diï¬erences\n",
      "14: for\n",
      "ð§© Token 3143:  each g â G do\n",
      "15: Rank\n",
      "ð§© Token 3144:  the rank diï¬erence : R(\n",
      "ð§© Token 3145: Î´g) â ï¿½\n",
      "ð§© Token 3146: ï¿½\n",
      "hâG 1 (Î´\n",
      "ð§© Token 3147: g â¥ Î´h)\n",
      "16: end\n",
      "ð§© Token 3148:  for\n",
      "17: Select top 2Î¸ clusters\n",
      "ð§© Token 3149:  with smallest Î´g: GÎ¸ =\n",
      "ð§© Token 3150:  {g â G | 1 â¤ R(\n",
      "ð§© Token 3151: Î´g) â¤ 2Î¸}\n",
      "\n",
      "ð§© Token 3152: # Step 5: Determine Long and Short Pos\n",
      "ð§© Token 3153: itions\n",
      "18: Deï¬ne G\n",
      "ð§© Token 3154: +\n",
      "Î¸ = {g â G\n",
      "ð§© Token 3155: Î¸ | SRtr\n",
      "g > 0 and\n",
      "ð§© Token 3156:  SRval\n",
      "g > 0}\n",
      "19:\n",
      "ð§© Token 3157:  Deï¬ne Gâ\n",
      "Î¸\n",
      "ð§© Token 3158:  = {g â GÎ¸ | SR\n",
      "ð§© Token 3159: tr\n",
      "g < 0 and SRval\n",
      "g\n",
      "ð§© Token 3160:  < 0}\n",
      "20: Return Long-tr\n",
      "ð§© Token 3161: aded clusters G+\n",
      "Î¸ , Short-\n",
      "ð§© Token 3162: traded clusters Gâ\n",
      "Î¸\n",
      "A\n",
      "ð§© Token 3163: .5 Sample of articles for each cluster\n",
      "50\n",
      "ð§© Token 3164: Table A1: KMeans clustering.\n",
      "ð§© Token 3165:  Proposed name for the clusters and sample of 3\n",
      "ð§© Token 3166:  articles for each cluster.\n",
      "# Title Articles\n",
      "\n",
      "ð§© Token 3167: 0 Miscellaneous (Colonial, Acciona, Am\n",
      "ð§© Token 3168: adeus, Grifols,\n",
      "Endesa\n",
      "ð§© Token 3169: , IAG, Bankinter...)\n",
      "â¢ Colonial\n",
      "ð§© Token 3170:  forecasts rental income of EUR338m in 2020\n",
      "\n",
      "ð§© Token 3171: â¢ Accionaâs asset sales will allow\n",
      "ð§© Token 3172:  it to grow in renewables\n",
      "â¢ Sabadell\n",
      "ð§© Token 3173:  recommends selling Amadeus shares due to worse sales\n",
      "ð§© Token 3174:  forecast.\n",
      "1 Quarterly & Semi-Annual\n",
      "ð§© Token 3175:  Earnings Reports\n",
      "â¢ EnagÃ¡ s 1\n",
      "ð§© Token 3176: H net proï¬t falls 9.\n",
      "ð§© Token 3177: 8% due to lower income and extraordinary items.\n",
      "ð§© Token 3178: \n",
      "â¢ Iberdrola: Net pro\n",
      "ð§© Token 3179: ï¬t of EUR1.025m\n",
      "ð§© Token 3180:  in Q1\n",
      "â¢ Santander almost quintu\n",
      "ð§© Token 3181: ples Q1 proï¬t due to\n",
      "ð§© Token 3182:  absence of Covid provisions.\n",
      "2 BBVA\n",
      "ð§© Token 3183:  & Sabadell: Financial Performance & Strategic\n",
      "\n",
      "ð§© Token 3184: Movements\n",
      "â¢ Interest rate hike in Turkey favors\n",
      "ð§© Token 3185:  BBVAâs net interest margin\n",
      "â¢\n",
      "ð§© Token 3186:  Sabadell reorganizes business in Spain following the\n",
      "ð§© Token 3187:  arrival of the new CEO.\n",
      "â¢ Fitch\n",
      "ð§© Token 3188:  downgrades Banco Sabadellâs\n",
      "ð§© Token 3189:  rating one notch to low grade.\n",
      "3 Tele\n",
      "ð§© Token 3190: fÃ³ nica & Cellnex: Telecommunications Tower\n",
      "ð§© Token 3191:  Sales\n",
      "& Market Dynamics\n",
      "â¢ TelefÃ³\n",
      "ð§© Token 3192:  nica shares soar after selling towers of its subsidiary\n",
      "ð§© Token 3193:  in Europe and Latin America.\n",
      "â¢ Telef\n",
      "ð§© Token 3194: Ã³ nica hires Goldman Sachs to sell its British\n",
      "ð§© Token 3195:  tower business\n",
      "â¢ Dutch Competition Authority authorizes Cell\n",
      "ð§© Token 3196: nex to integrate 3,150 Deutsche Telekom\n",
      "ð§© Token 3197:  towers.\n",
      "4 CaixaBank: Mer\n",
      "ð§© Token 3198: gers and Strategic Moves in the\n",
      "Banking Sector\n",
      "ð§© Token 3199: \n",
      "â¢ CaixaBank and Bankia approve\n",
      "ð§© Token 3200:  their merger project\n",
      "â¢ CaixaBank closes\n",
      "ð§© Token 3201:  its ï¬rst issuance of green bonds\n",
      "ð§© Token 3202:  in pounds for 500 million\n",
      "â¢ Caixa\n",
      "ð§© Token 3203: Bank-Bankia merger could generate EUR500m\n",
      "ð§© Token 3204:  in savings\n",
      "5 TelefÃ³ nica,\n",
      "ð§© Token 3205:  Indra, & MÃ¡ sMÃ³ vil:\n",
      "ð§© Token 3206:  Regulatory and\n",
      "Strategic Moves in Telecom\n",
      "â¢\n",
      "ð§© Token 3207:  Indra to partner with TelefÃ³ nica in\n",
      "ð§© Token 3208:  the deployment of ï¬ber optics in Germany\n",
      "ð§© Token 3209: .\n",
      "â¢ TelefÃ³ nica launches a\n",
      "ð§© Token 3210:  buyback oï¬er for its hybrid\n",
      "ð§© Token 3211:  bonds of EUR1.000m.\n",
      "â¢\n",
      "ð§© Token 3212:  EU refers Liberty Global and TelefÃ³ nica\n",
      "ð§© Token 3213:  agreement to UK regulator\n",
      "6 Siemens Gamesa\n",
      "ð§© Token 3214: : Supply Agreements, Proï¬t\n",
      "ð§© Token 3215: ability\n",
      "Targets in Renewable Energy\n",
      "\n",
      "ð§© Token 3216: â¢ Siemens Gamesa will supply turbines to El\n",
      "ð§© Token 3217: awanâs 150 MW wind farm in Spain\n",
      "ð§© Token 3218: .\n",
      "â¢ Siemens Gamesa lowers its pro\n",
      "ð§© Token 3219: ï¬tability target for 2021.\n",
      "\n",
      "ð§© Token 3220: â¢ Siemens Gamesa will supply 160 MW for\n",
      "ð§© Token 3221:  the largest wind farm in the Philippines.\n",
      "7\n",
      "ð§© Token 3222:  Cellnex: Strategic Acquisitions and Financial Moves in\n",
      "ð§© Token 3223: \n",
      "Telecom Infrastructure\n",
      "â¢ Cellnex launches a\n",
      "ð§© Token 3224:  EUR1.850m debt issue\n",
      "â¢ Cell\n",
      "ð§© Token 3225: nex agrees to buy 10,500 telecommunications towers in\n",
      "ð§© Token 3226:  France for EUR5.200m\n",
      "â¢ Ben\n",
      "ð§© Token 3227: etton family sells 2.5% of Cell\n",
      "ð§© Token 3228: nex to Singapore sovereign fund\n",
      "8 Acciona,\n",
      "ð§© Token 3229:  Endesa, EnagÃ¡ s & Natur\n",
      "ð§© Token 3230: gy: Strategic Moves\n",
      "& Regulatory Developments in\n",
      "ð§© Token 3231:  the Energy Sector\n",
      "â¢ Naturgy and En\n",
      "ð§© Token 3232: agÃ¡ s study project to produce green hydrogen in\n",
      "ð§© Token 3233:  Asturias\n",
      "â¢ Break of ties between Algeria\n",
      "ð§© Token 3234:  and Morocco may damage gas ï¬ow to\n",
      "ð§© Token 3235:  Spain\n",
      "â¢ Acciona: Energy business IPO on\n",
      "ð§© Token 3236:  track for 1H\n",
      "9 Repsol: Strategic\n",
      "ð§© Token 3237:  Moves and Challenges in the Energy\n",
      "Sector\n",
      "\n",
      "ð§© Token 3238: â¢ Repsol to produce green hydrogen at Petron\n",
      "ð§© Token 3239: or reï¬nery in 2022\n",
      "â¢\n",
      "ð§© Token 3240:  Repsol and Talgo to jointly promote the creation\n",
      "ð§© Token 3241:  of renewable hydrogen trains\n",
      "â¢ Repsol gains access\n",
      "ð§© Token 3242:  to a portfolio of renewable assets in Chile through a\n",
      "ð§© Token 3243:  joint venture\n",
      "10 Ferrovial, Acciona\n",
      "ð§© Token 3244: : Strategic Expansions and Financial\n",
      "Mane\n",
      "ð§© Token 3245: uvers in Infrastructure\n",
      "â¢ Ferrovial closes\n",
      "ð§© Token 3246:  the sale of Broadspectrum to Ventia for\n",
      "ð§© Token 3247:  EUR291m\n",
      "â¢ Acciona awarded the construction\n",
      "ð§© Token 3248:  of 2 roads in Poland for EUR642m\n",
      "\n",
      "ð§© Token 3249: â¢ Renfe awards on-board services contract to\n",
      "ð§© Token 3250:  Ferrovial for EUR272m\n",
      "11 Sol\n",
      "ð§© Token 3251: aria: Strategic Moves and Market Challenges in\n",
      "Ren\n",
      "ð§© Token 3252: ewable Energy\n",
      "â¢ Solaria invests EUR220\n",
      "ð§© Token 3253: m in Europeâs largest photovolt\n",
      "ð§© Token 3254: aic park.\n",
      "â¢ Solaria will supply energy\n",
      "ð§© Token 3255:  to Shell and Axpo with Europeâs\n",
      "ð§© Token 3256:  largest photovoltaic plant\n",
      "â¢ Goldman Sachs\n",
      "ð§© Token 3257:  downgrades Solaria recommendation after stock rise.\n",
      "\n",
      "ð§© Token 3258: 12 Iberdrola: Strategic Collaborations\n",
      "ð§© Token 3259:  and Renewable\n",
      "Energy Developments\n",
      "â¢ I\n",
      "ð§© Token 3260: berdrola will build a self-cons\n",
      "ð§© Token 3261: umption plant for Lactalis factory in Spain.\n",
      "ð§© Token 3262: \n",
      "â¢ Iberdrola and Mapfre\n",
      "ð§© Token 3263:  launch a renewable energy co-investment vehicle in\n",
      "ð§© Token 3264:  Spain.\n",
      "â¢ Iberdrola partners\n",
      "ð§© Token 3265:  with Mitsubishi to decarbonize the industry\n",
      "ð§© Token 3266: .\n",
      "51\n",
      "ð§© Token 3267: 13 IAG: Financial Performance\n",
      "â¢ IAG\n",
      "ð§© Token 3268:  Q3 results worse than expected\n",
      "â¢ IAG\n",
      "ð§© Token 3269:  burns cash faster than anticipated\n",
      "â¢ IAG stock\n",
      "ð§© Token 3270:  may be pricing in a second capital increase\n",
      "14\n",
      "ð§© Token 3271:  Santander & CaixaBank: Financial Moves\n",
      "ð§© Token 3272:  and\n",
      "Sustainability Initiatives\n",
      "â¢ Ca\n",
      "ð§© Token 3273: ixaBank mobilizes EUR12.000m\n",
      "ð§© Token 3274:  in sustainable ï¬nancing in the ï¿½\n",
      "ð§© Token 3275: ï¿½ï¿½rst 9 months of 2020.\n",
      "\n",
      "ð§© Token 3276: â¢ EIB and Banco Santander will inject\n",
      "ð§© Token 3277:  EUR587m into Portuguese SMEs.\n",
      "â¢\n",
      "ð§© Token 3278:  Banco Santander, leader in renewable project ï¿½\n",
      "ð§© Token 3279: ï¿½ï¿½nancing in 2020.\n",
      "15 ACS\n",
      "ð§© Token 3280:  & Acciona: Strategic Movements and\n",
      "Inf\n",
      "ð§© Token 3281: rastructure Projects\n",
      "â¢ ACS and Acciona win contracts\n",
      "ð§© Token 3282:  for new Australian airport worth EUR164m.\n",
      "\n",
      "ð§© Token 3283: â¢ Acciona awarded 3 contracts to operate wastewater treatment\n",
      "ð§© Token 3284:  plants in Sardinia for EUR210m.\n",
      "\n",
      "ð§© Token 3285: â¢ ACS expects net proï¬t to\n",
      "ð§© Token 3286:  grow by around 30% in 2021\n",
      "16 Tele\n",
      "ð§© Token 3287: fÃ³ nica: Financial Performance and Strategic Moves\n",
      "ð§© Token 3288: \n",
      "â¢ Reduction in TelefÃ³ nicaï¿½\n",
      "ð§© Token 3289: ï¿½s debt will improve analystsâperception\n",
      "ð§© Token 3290: \n",
      "â¢ TelefÃ³ nicaâs\n",
      "ð§© Token 3291:  proï¬t more than doubles in Q\n",
      "ð§© Token 3292: 1 due to lower ï¬nancial expenses\n",
      "ð§© Token 3293: .\n",
      "â¢ TelefÃ³ nica, Am\n",
      "ð§© Token 3294: Ã© rica MÃ³ vil and TIM buy the\n",
      "ð§© Token 3295:  mobile network of Brazilâs Oi.\n",
      "ð§© Token 3296: \n",
      "17 MeliÃ¡ and Spanish Tourism Sector:\n",
      "ð§© Token 3297:  Challenges Amidst\n",
      "the Pandemic\n",
      "â¢ Mel\n",
      "ð§© Token 3298: iÃ¡ : Spanish hotel sector faces another uncertain summer\n",
      "ð§© Token 3299:  with cautious optimism.\n",
      "â¢ MeliÃ¡ claims\n",
      "ð§© Token 3300:  EUR116m from the Spanish government for pandemic\n",
      "ð§© Token 3301: -related damages.\n",
      "â¢ MeliÃ¡ :\n",
      "ð§© Token 3302:  Local Covid-19 lockdowns will continue to\n",
      "ð§© Token 3303:  aï¬ect MeliÃ¡ .\n",
      "\n",
      "ð§© Token 3304: 18 Takeover Bids for Naturgy and\n",
      "ð§© Token 3305:  MÃ¡ sMÃ³ vil\n",
      "â¢ Australian fund\n",
      "ð§© Token 3306:  IFM launches EUR5.000m bid for\n",
      "ð§© Token 3307:  22.69% of Naturgy.\n",
      "\n",
      "ð§© Token 3308: â¢ Polygon fund asks CNMV to review\n",
      "ð§© Token 3309:  and alter the bid for MÃ¡ sMÃ³\n",
      "ð§© Token 3310:  vil.\n",
      "â¢ IFM accepts Spanish government conditions\n",
      "ð§© Token 3311:  in partial bid for Naturgy.\n",
      "19\n",
      "ð§© Token 3312:  Naturgy: Financial Performance\n",
      "â¢ Natur\n",
      "ð§© Token 3313: gy presents \"weak\" 2020 results\n",
      "â¢ N\n",
      "ð§© Token 3314: aturgy may revise its strategic plan upwards due to\n",
      "ð§© Token 3315:  gas prices.\n",
      "â¢ Bank of America sees upside\n",
      "ð§© Token 3316:  potential for Naturgy based on fundamentals.\n",
      "\n",
      "ð§© Token 3317: 20 PharmaMar, Grifols: Regulatory Appro\n",
      "ð§© Token 3318: vals and Market\n",
      "Moves in the Pharmaceutical Sector\n",
      "ð§© Token 3319: \n",
      "â¢ EU court annuls European Commissionâ\n",
      "ð§© Token 3320: s refusal to market PharmaMar drug.\n",
      "â¢\n",
      "ð§© Token 3321:  Grifols starts issuing EUR2.000m\n",
      "ð§© Token 3322:  bonds to buy Biotest.\n",
      "â¢ Pharma\n",
      "ð§© Token 3323: Mar announces approval of lurbinectedin for\n",
      "ð§© Token 3324:  lung cancer in Australia.\n",
      "21 Repsol:\n",
      "ð§© Token 3325:  Financial Performance\n",
      "â¢ Repsol: Net loss of\n",
      "ð§© Token 3326:  EUR3.289m in 2020.\n",
      "â¢\n",
      "ð§© Token 3327:  Repsol reports a loss of EUR711m\n",
      "ð§© Token 3328:  in Q4 due to exploration and production provisions\n",
      "\n",
      "ð§© Token 3329: â¢ Repsol posts a loss of EUR94m\n",
      "ð§© Token 3330:  in Q3 due to provisions and lower reï¿½\n",
      "ð§© Token 3331: ï¿½ï¿½ning margins.\n",
      "22 Aena:\n",
      "ð§© Token 3332:  Financial Performance\n",
      "â¢ JPMorgan raises Aenaâ\n",
      "ð§© Token 3333: s target price to EUR155 from EUR135.\n",
      "ð§© Token 3334: \n",
      "â¢ Aena risks a revenue cut of up\n",
      "ð§© Token 3335:  to EUR2.000m due to rents.\n",
      "ð§© Token 3336: \n",
      "â¢ Aena loses EUR170.7m\n",
      "ð§© Token 3337:  in 1H as passenger traï¬c\n",
      "ð§© Token 3338:  plummets due to the pandemic.\n",
      "\n",
      "ð§© Token 3339: 23 EnagÃ¡ s, Endesa, I\n",
      "ð§© Token 3340: berdrola, Red ElÃ© ct\n",
      "ð§© Token 3341: rica: Regulatory\n",
      "and Market Challenges in the Energy\n",
      "ð§© Token 3342:  Sector\n",
      "â¢ Spanish electric utilities will remain under pressure\n",
      "ð§© Token 3343:  in the stock market\n",
      "â¢ Spanish government measures are\n",
      "ð§© Token 3344:  bad news for the electric sector.\n",
      "â¢ Spain\n",
      "ð§© Token 3345: âs electricity price closes February with a 52\n",
      "ð§© Token 3346: % drop vs January\n",
      "24 BBVA, Ca\n",
      "ð§© Token 3347: ixaBank, Banco Sabadell:\n",
      "ð§© Token 3348:  Layoï¬s and\n",
      "Restruct\n",
      "ð§© Token 3349: uring\n",
      "â¢ CaixaBank proposes to unions\n",
      "ð§© Token 3350:  a redundancy plan aï¬ecting 8\n",
      "ð§© Token 3351: ,291 employees.\n",
      "â¢ Banco Santander\n",
      "ð§© Token 3352:  closes its redundancy plan with 3,572 voluntary exits\n",
      "ð§© Token 3353:  and 19 dismissals\n",
      "â¢ Sabadell prepares\n",
      "ð§© Token 3354:  an adjustment plan aï¬ecting 2\n",
      "ð§© Token 3355: ,000 employees\n",
      "25 Inditex, Acer\n",
      "ð§© Token 3356: inox: Market Performance and Strategic\n",
      "Developments\n",
      "ð§© Token 3357:  in the Post-Covid Context\n",
      "â¢\n",
      "ð§© Token 3358:  Inditex reopens 94% of its stores\n",
      "ð§© Token 3359:  worldwide after Covid-19 pandemic.\n",
      "\n",
      "ð§© Token 3360: â¢ Sale of Nippon Steel in Acerin\n",
      "ð§© Token 3361: ox is negative, but expected.\n",
      "â¢ Ind\n",
      "ð§© Token 3362: itex stock already prices in a strong business recovery\n",
      "ð§© Token 3363: .\n",
      "52\n",
      "ð§© Token 3364: Table A2: LLM clustering. Sample\n",
      "ð§© Token 3365:  of 3 articles for each cluster.\n",
      "# Title\n",
      "ð§© Token 3366:  Articles\n",
      "0 Demand, Minor, Positive\n",
      "â¢\n",
      "ð§© Token 3367:  MeliÃ¡ âs recovery will be fast\n",
      "ð§© Token 3368: , but it will not be completed until 2023\n",
      "ð§© Token 3369: \n",
      "â¢ Tourism sector aid in Spain will have a\n",
      "ð§© Token 3370:  limited impact on listed companies\n",
      "â¢ Spanish airports will\n",
      "ð§© Token 3371:  recover pre-pandemic traï¬\n",
      "ð§© Token 3372: c by the end of 2025\n",
      "1 Demand,\n",
      "ð§© Token 3373:  Minor, Negative\n",
      "â¢ Tallgrass will contribute fewer\n",
      "ð§© Token 3374:  dividends to EnagÃ¡ s -JPMorgan\n",
      "ð§© Token 3375:  Cazenove\n",
      "â¢ Aenaâ\n",
      "ð§© Token 3376: s stock decline is due to sector visibility -Sab\n",
      "ð§© Token 3377: adell\n",
      "â¢ ObservaTUR believes Spain\n",
      "ð§© Token 3378: âs economic situation will worsen and calls for\n",
      "ð§© Token 3379:  more measures\n",
      "2 Demand, Major, Positive\n",
      "\n",
      "ð§© Token 3380: â¢ Solaria invests EUR220m in Europeï¿½\n",
      "ð§© Token 3381: ï¿½s largest photovoltaic park\n",
      "â¢\n",
      "ð§© Token 3382:  Acciona will build Sao Paulo metro line for EUR\n",
      "ð§© Token 3383: 2.3 billion\n",
      "â¢ Inditex returns\n",
      "ð§© Token 3384:  to proï¬t in H1 and\n",
      "ð§© Token 3385:  continues to recover from the pandemic\n",
      "3 Demand\n",
      "ð§© Token 3386: , Major, Negative\n",
      "â¢ Passenger traï¿½\n",
      "ð§© Token 3387: ï¿½c at Aena airports falls 79.9\n",
      "ð§© Token 3388: % year-on-year in September\n",
      "â¢\n",
      "ð§© Token 3389:  UPDATE: Naturgyâs net pro\n",
      "ð§© Token 3390: ï¬t falls 45.6% in\n",
      "ð§© Token 3391:  9m due to Covid-19 impact\n",
      "\n",
      "ð§© Token 3392: â¢ Possible capital increase by IAG already priced in\n",
      "ð§© Token 3393: \n",
      "4 Supply, Minor, Positive\n",
      "â¢ Rep\n",
      "ð§© Token 3394: sol returns to proï¬t in Q\n",
      "ð§© Token 3395: 2 due to crude price increase\n",
      "â¢ Natur\n",
      "ð§© Token 3396: gy receives LNG supply contract for ships for 2\n",
      "ð§© Token 3397:  years in Spain\n",
      "â¢ Acciona EnergÃ­\n",
      "ð§© Token 3398:  a starts up 238 MW photovoltaic complex\n",
      "ð§© Token 3399:  in Chile\n",
      "5 Supply, Minor, Negative\n",
      "\n",
      "ð§© Token 3400: â¢ EnagÃ¡ s operating results worse than expected\n",
      "ð§© Token 3401:  -Bankinter\n",
      "â¢ IFM rules out extending\n",
      "ð§© Token 3402:  acceptance period for Naturgy takeover bid and changing\n",
      "ð§© Token 3403: \n",
      "conditions\n",
      "â¢ Changes in Siemens Games\n",
      "ð§© Token 3404: aâs onshore wind business will take\n",
      "ð§© Token 3405:  time\n",
      "6 Supply, Major, Positive\n",
      "â¢\n",
      "ð§© Token 3406:  Capital Energy wins renewable auction in Spain\n",
      "â¢ Rep\n",
      "ð§© Token 3407: sol expects to start exploiting its huge gas reserve in\n",
      "ð§© Token 3408:  Brazil in 2026\n",
      "â¢ Repsol will invest\n",
      "ð§© Token 3409:  EUR657m to expand its industrial complex in S\n",
      "ð§© Token 3410: ines, Portugal\n",
      "7 Supply, Major, Negative\n",
      "ð§© Token 3411: \n",
      "â¢ Iberdrola halts $\n",
      "ð§© Token 3412: 1.2 billion investment in Mexico\n",
      "â¢ 85\n",
      "ð§© Token 3413: % of Acciona workers at Nissan agree to contract\n",
      "ð§© Token 3414:  termination\n",
      "â¢ CaixaBank reduces workforce adjustment\n",
      "ð§© Token 3415:  by 500 employees to 7,791 -Source\n",
      "ð§© Token 3416: \n",
      "8 Financial, Minor, Positive\n",
      "â¢ Norwegian\n",
      "ð§© Token 3417:  fund Norges Bank takes 1.14%\n",
      "ð§© Token 3418:  stake in Naturgy amid IFM takeover bid\n",
      "ð§© Token 3419: \n",
      "â¢ Sabadell closes green bond issue for\n",
      "ð§© Token 3420:  EUR500m -Source\n",
      "â¢ Caixa\n",
      "ð§© Token 3421: Bank-Bankia merger goals are credible -De\n",
      "ð§© Token 3422: utsche Bank\n",
      "9 Financial, Minor, Negative\n",
      "\n",
      "ð§© Token 3423: â¢ UPDATE2: Bankiaâs pro\n",
      "ð§© Token 3424: ï¬t falls 57.6% in\n",
      "ð§© Token 3425:  2020 due to provisions for pandemic impact\n",
      "â¢\n",
      "ð§© Token 3426:  Iberdrola bond spreads will not be\n",
      "ð§© Token 3427:  aï¬ected by GalÃ¡ nï¿½\n",
      "ð§© Token 3428: ï¿½s indictment for now\n",
      "â¢ Court maintains precaution\n",
      "ð§© Token 3429: ary suspension of rent payments to Aena\n",
      "10\n",
      "ð§© Token 3430:  Financial, Major, Positive\n",
      "â¢ UPDATE: End\n",
      "ð§© Token 3431: esaâs net proï¬t\n",
      "ð§© Token 3432:  soars in 2020 due to lower impairment charges\n",
      "\n",
      "ð§© Token 3433: â¢ TelefÃ³ nica will reduce debt by\n",
      "ð§© Token 3434:  EUR5bn after closing Virgin Media and O2\n",
      "ð§© Token 3435:  merger\n",
      "â¢ Fluidra buys US company S\n",
      "ð§© Token 3436: .R. Smith for $240m\n",
      "53\n",
      "ð§© Token 3437: 11 Financial, Major, Negative\n",
      "â¢ UPDATE3\n",
      "ð§© Token 3438: : Banco Santander reports EUR8.771\n",
      "ð§© Token 3439: bn loss in 2020 due to Covid charges\n",
      "\n",
      "ð§© Token 3440: â¢ Bankinter downgrades Grifols recommendation to\n",
      "ð§© Token 3441:  neutral from buy\n",
      "â¢ BBVA reduces layo\n",
      "ð§© Token 3442: ï¬s to 3,361 and proposes\n",
      "ð§© Token 3443:  early retirement at 52 with 65% salary\n",
      "12\n",
      "ð§© Token 3444:  Technology, Minor, Positive\n",
      "â¢ Siemens Games\n",
      "ð§© Token 3445: a to supply turbines for 298MW wind farm in\n",
      "ð§© Token 3446:  the US\n",
      "â¢ Repsol and TÃ ï¿½\n",
      "ð§© Token 3447: ï¿½cnicas Reunidas team up to\n",
      "ð§© Token 3448:  develop decarbonization technologies\n",
      "â¢ European Commission funds\n",
      "ð§© Token 3449:  Repsol and EnagÃ¡ s renewable hydrogen project\n",
      "ð§© Token 3450: \n",
      "13 Technology, Minor, Negative â¢ Cellnex\n",
      "ð§© Token 3451:  and REE apply for EU funds to develop rural\n",
      "ð§© Token 3452:  mobile networks\n",
      "14 Technology, Major, Positive\n",
      "\n",
      "ð§© Token 3453: â¢ TelefÃ³ nica and Allianz\n",
      "ð§© Token 3454:  partner to deploy ï¬ber in Germany\n",
      "\n",
      "ð§© Token 3455: â¢ Iberdrola partners with Cosmo\n",
      "ð§© Token 3456:  to develop 600 MW of oï¬shore\n",
      "ð§© Token 3457:  wind in Japan\n",
      "â¢ TelefÃ³ nica\n",
      "ð§© Token 3458:  estimates 5G network will require over EUR6bn\n",
      "ð§© Token 3459:  in Spain\n",
      "15 Technology, Major, Negative\n",
      "\n",
      "ð§© Token 3460: 16 Policy, Minor, Positive\n",
      "â¢ Enag\n",
      "ð§© Token 3461: Ã¡ s promotes 34 hydrogen and 21 biometh\n",
      "ð§© Token 3462: ane proposals to recovery funds\n",
      "â¢ Iberd\n",
      "ð§© Token 3463: rola president sees need to reform taxation to make\n",
      "ð§© Token 3464:  renewables competitive\n",
      "â¢ New electricity tariï¿½\n",
      "ð§© Token 3465: ï¿½ in Spain aims to change consumer habits -Experts\n",
      "ð§© Token 3466: \n",
      "17 Policy, Minor, Negative\n",
      "â¢ Spanish\n",
      "ð§© Token 3467:  government measures hurt Iberdrola -IG\n",
      "ð§© Token 3468: \n",
      "â¢ Spanish government plans law to reduce CO2\n",
      "ð§© Token 3469:  price impact on electricity bills -Source\n",
      "â¢ I\n",
      "ð§© Token 3470: berdrola CEO criticizes electricity reform in\n",
      "ð§© Token 3471:  Spain for \"unexpected charges\"\n",
      "18 Policy\n",
      "ð§© Token 3472: , Major, Positive\n",
      "â¢ Endesa is Spain\n",
      "ð§© Token 3473: âs future green leader, but trades at\n",
      "ð§© Token 3474:  a discount\n",
      "â¢ TCI fund supports ACSï¿½\n",
      "ð§© Token 3475: ï¿½s interest in ASPI and will reject Italy\n",
      "ð§© Token 3476: âs oï¬er\n",
      "â¢\n",
      "ð§© Token 3477:  Cellnex acquisition in France reassures investors -B\n",
      "ð§© Token 3478: erenberg\n",
      "19 Policy, Major, Negative\n",
      "\n",
      "ð§© Token 3479: â¢ Sabadell does not expect improvement in partial\n",
      "ð§© Token 3480:  takeover bid price for Naturgy\n",
      "â¢ R\n",
      "ð§© Token 3481: enta 4 downgrades Naturgy to underweight\n",
      "ð§© Token 3482:  after government measures\n",
      "â¢ Bankinter warns of uncertainties\n",
      "ð§© Token 3483:  over Iberdrola stock\n",
      "54\n",
      "ð§© Token 3484: A.6 Function Calling with Llama-3\n",
      "ð§© Token 3485: \n",
      "Algorithm 4. Function Calling Workï¿½\n",
      "ð§© Token 3486: ï¿½ow for Llama-3\n",
      "Require\n",
      "ð§© Token 3487: : D: Dataset of news articles\n",
      "\n",
      "ð§© Token 3488: Ensure: Structured JSON output for each article\n",
      "ð§© Token 3489: \n",
      "1: Initialize Llama-3 model\n",
      "ð§© Token 3490:  via GroqCloud API\n",
      "2: for each\n",
      "ð§© Token 3491:  article i â D do â² Iter\n",
      "ð§© Token 3492: ate over each article in the dataset\n",
      "3:\n",
      "ð§© Token 3493:  Message: System â² Deï¬\n",
      "ð§© Token 3494: ne the role and task for the LLM\n",
      "\n",
      "ð§© Token 3495: âYou are a function calling LLM that\n",
      "ð§© Token 3496:  analyzes business news in Spanish. For every article\n",
      "ð§© Token 3497: ,\n",
      "identify the ï¬rms\n",
      "ð§© Token 3498:  that are directly aï¬ected by the\n",
      "ð§© Token 3499:  news and classify the shocks in type,\n",
      "m\n",
      "ð§© Token 3500: agnitude and directionâ\n",
      "4: Message\n",
      "ð§© Token 3501: : User â² User provides the article text\n",
      "ð§© Token 3502:  as input\n",
      "Content: prompt Pi containing the text\n",
      "ð§© Token 3503:  of article i\n",
      "5: Tool: news_\n",
      "ð§© Token 3504: parser â² Deï¬ne the\n",
      "ð§© Token 3505:  news_parser function\n",
      "Parameters: {firms\n",
      "ð§© Token 3506: : array of objects}, where each object contains:\n",
      "ð§© Token 3507: \n",
      "â¢ firm: string (âeach one\n",
      "ð§© Token 3508:  ï¬rm in firms â)\n",
      "\n",
      "ð§© Token 3509: â¢ ticker: string (âstock market\n",
      "ð§© Token 3510:  tickerâ)\n",
      "â¢ shock_type\n",
      "ð§© Token 3511: : enum {demand, supply, ï¬\n",
      "ð§© Token 3512: nancial, policy, technology}\n",
      "â¢ shock\n",
      "ð§© Token 3513: _magnitude: enum {minor,\n",
      "ð§© Token 3514:  major}\n",
      "â¢ shock_direction: enum {\n",
      "ð§© Token 3515: positive, negative}\n",
      "6: Send initial messages\n",
      "ð§© Token 3516:  and tools to Llama-3 â²\n",
      "ð§© Token 3517:  Initiate interaction with the LLM\n",
      "7:\n",
      "ð§© Token 3518:  Retrieve response from Llama-3 ï¿½\n",
      "ð§© Token 3519: ï¿½ Get the initial response from the LLM\n",
      "\n",
      "ð§© Token 3520: 8: if Function call is requested by Llama\n",
      "ð§© Token 3521: -3 then â² Check if the LL\n",
      "ð§© Token 3522: M needs to call a function\n",
      "9: Exec\n",
      "ð§© Token 3523: ute news_parser function with provided arguments ï¿½\n",
      "ð§© Token 3524: ï¿½ Run the function\n",
      "10: Append function\n",
      "ð§© Token 3525:  response to the conversation â² Include function output\n",
      "ð§© Token 3526:  in the dialogue\n",
      "11: Send updated messages to\n",
      "ð§© Token 3527:  Llama-3 â² Continue the conversation\n",
      "ð§© Token 3528:  with new information\n",
      "12: Retrieve ï¿½\n",
      "ð§© Token 3529: ï¿½nal response from Llama-3 ï¿½\n",
      "ð§© Token 3530: ï¿½ï¿½ Get the ï¬nal output\n",
      "ð§© Token 3531:  from the LLM\n",
      "13: end if\n",
      "\n",
      "ð§© Token 3532: 14: Extract and store structured JSON output ï¿½\n",
      "ð§© Token 3533: ï¿½ Save the processed data\n",
      "15: end for\n",
      "ð§© Token 3534: \n",
      "A.7 Why not using a diï¿½\n",
      "ð§© Token 3535: ï¿½ï¿½erent benchmark?\n",
      "In evaluating our novel\n",
      "ð§© Token 3536:  Large Language Model (LLM) methodology for class\n",
      "ð§© Token 3537: ifying news-implied ï¬rm-\n",
      "ð§© Token 3538: \n",
      "speciï¬c shocks, it\n",
      "ð§© Token 3539:  is imperative to establish a robust and relevant benchmark.\n",
      "ð§© Token 3540:  Our chosen benchmark\n",
      "involves transforming news articles into\n",
      "ð§© Token 3541:  high-dimensional vector embeddings followed by clust\n",
      "ð§© Token 3542: ering these\n",
      "55\n",
      "ð§© Token 3543: embeddings using the KMeans algorithm.\n",
      "ð§© Token 3544:  This section delineates the rationale behind selecting KMe\n",
      "ð§© Token 3545: ans\n",
      "clustering of vector embeddings\n",
      "ð§© Token 3546:  over other potential benchmarks such as sentiment analysis and topic\n",
      "ð§© Token 3547: \n",
      "modeling.\n",
      "Why not Sentiment Analysis\n",
      "ð§© Token 3548:  as a benchmark?\n",
      "Sentiment analysis is a\n",
      "ð§© Token 3549:  widely recognized technique in natural language processing that aims to\n",
      "ð§© Token 3550:  de-\n",
      "termine the emotional tone conveyed in\n",
      "ð§© Token 3551:  a text, typically categorizing content as positive,\n",
      "ð§© Token 3552:  negative, or\n",
      "neutral. While sentiment analysis provides\n",
      "ð§© Token 3553:  a straightforward approach to gauging the general tone of\n",
      "ð§© Token 3554: \n",
      "news articles, it falls short in several critical\n",
      "ð§© Token 3555:  aspects when juxtaposed with our objectives.\n",
      "First\n",
      "ð§© Token 3556: , sentiment analysis is not suï¬ci\n",
      "ð§© Token 3557: ently granular. Our LLM methodology classi\n",
      "ð§© Token 3558: ï¬es news articles into\n",
      "20 distinct\n",
      "ð§© Token 3559:  categories of economic shocks while sentiment analysis classiï¿½\n",
      "ð§© Token 3560: ï¿½ï¿½es articles in a coarse manner,\n",
      "\n",
      "ð§© Token 3561: typically into positive, negative, or neutral categories,\n",
      "ð§© Token 3562:  which is inadequate for benchmarking a detailed\n",
      "class\n",
      "ð§© Token 3563: iï¬cation model.\n",
      "Second,\n",
      "ð§© Token 3564:  sentiment analysis predominantly focuses on the linguistic and emotional aspects\n",
      "ð§© Token 3565:  of the text,\n",
      "which do not necessarily correlate\n",
      "ð§© Token 3566:  with the economic impact on ï¬rms\n",
      "ð§© Token 3567: . For instance, a neutral-toned\n",
      "\n",
      "ð§© Token 3568: article could describe a signiï¬c\n",
      "ð§© Token 3569: ant economic event, while a positive sentiment might not\n",
      "ð§© Token 3570:  always translate\n",
      "to favorable economic outcomes. Consequently,\n",
      "ð§© Token 3571:  the sentiment does not provide direct insights into the\n",
      "\n",
      "ð§© Token 3572: economic consequences, making it an economically irrelevant benchmark for\n",
      "ð§© Token 3573:  our purposes.\n",
      "Third, sentiment analysis algorithms are\n",
      "ð§© Token 3574:  often sensitive to linguistic subtleties, leading to\n",
      "ð§© Token 3575:  inconsistent\n",
      "results across diï¬erent languages\n",
      "ð§© Token 3576:  and contexts. For example, sarcasm or idi\n",
      "ð§© Token 3577: omatic expressions can distort\n",
      "sentiment scores, undermining\n",
      "ð§© Token 3578:  the reliability of sentiment analysis as a benchmark. This\n",
      "ð§© Token 3579:  variability poses\n",
      "a challenge for standardization, especially\n",
      "ð§© Token 3580:  in a multilingual context. For instance, the\n",
      "ð§© Token 3581:  sentiment derived\n",
      "from analyzing the text in English may\n",
      "ð§© Token 3582:  signiï¬cantly diï¿½\n",
      "ð§© Token 3583: ï¿½er from the sentiment in Spanish.\n",
      "Fourth\n",
      "ð§© Token 3584: , sentiment analysis is not robust in the sense that\n",
      "ð§© Token 3585:  diï¬erent sentiment analysis tools yield\n",
      "\n",
      "ð§© Token 3586: divergent assessments of the same text. As\n",
      "ð§© Token 3587:  shown below, we observe considerable diï¬\n",
      "ð§© Token 3588: erences in the\n",
      "identiï¬ed\n",
      "ð§© Token 3589:  sentiment when applying multiple sentiment analysis providers to a spe\n",
      "ð§© Token 3590: ciï¬c article. This lack\n",
      "\n",
      "ð§© Token 3591: of consistency undermines the reliability of sentiment analysis as a\n",
      "ð§© Token 3592:  benchmark, making it unsuitable for\n",
      "our purposes\n",
      "ð§© Token 3593: .\n",
      "Sentiment analysis is highly sensitive to the\n",
      "ð§© Token 3594:  speciï¬c tool or model employed\n",
      "ð§© Token 3595: . Here, we demon-\n",
      "strate this by\n",
      "ð§© Token 3596:  analyzing a piece of business news using various popular sentiment\n",
      "ð§© Token 3597:  analysis\n",
      "tools: TextBlob, text2\n",
      "ð§© Token 3598: data, VADER, and FinBERT\n",
      "ð§© Token 3599: . The methods vary signiï¬c\n",
      "ð§© Token 3600: antly in both\n",
      "their approach to sentiment determination and\n",
      "ð§© Token 3601:  the output they provide, as illustrated below.8\n",
      "ð§© Token 3602: \n",
      "8Note that applying Loughran-Mac\n",
      "ð§© Token 3603: donald is not recommended in for short texts as it\n",
      "ð§© Token 3604:  yields sparse results. For example,\n",
      "in the\n",
      "ð§© Token 3605:  example we are considering, it outputs a category distribution\n",
      "ð§© Token 3606:  that only loads on âStrong Modalï¿½\n",
      "ð§© Token 3607: ï¿½, which is not a really\n",
      "useful\n",
      "ð§© Token 3608:  analysis.\n",
      "LM_Scores = {ï¿½\n",
      "ð§© Token 3609: ï¿½Negativeâ: 0, â\n",
      "ð§© Token 3610: Positiveâ: 0, âUn\n",
      "ð§© Token 3611: certaintyâ: 0, âL\n",
      "ð§© Token 3612: itigiousâ: 0, âStrong\n",
      "ð§© Token 3613: _Modalâ: 2,\n",
      "56\n",
      "ð§© Token 3614: Example 3: A news article about TelefÃ³\n",
      "ð§© Token 3615:  nica and Cellnex | Sentiment: Text\n",
      "ð§© Token 3616: Blob\n",
      "Cellnex will face more competition in\n",
      "ð§© Token 3617:  Europe Score: 0.50\n",
      "TelefÃ³n\n",
      "ð§© Token 3618: icaâs (TEF.MC)\n",
      "ð§© Token 3619:  subsidiary, Telxius Telecom, has agreed to\n",
      "ð§© Token 3620:  sell its telecommuni-\n",
      "cations tower\n",
      "ð§© Token 3621:  division in Europe and Latin America to American Tower (\n",
      "ð§© Token 3622: AMT), which\n",
      "will expand the latterï¿½\n",
      "ð§© Token 3623: ï¿½s presence in Europe and increase competition for the\n",
      "ð§© Token 3624:  Spanish\n",
      "wireless telecommunications group Cellnex Telecom (\n",
      "ð§© Token 3625: CLNX.MC), according to Equita\n",
      "ð§© Token 3626: \n",
      "Sim. Score: 0.00 The transaction\n",
      "ð§© Token 3627:  \"represents the entry of a new independent tower\n",
      "ð§© Token 3628: \n",
      "operator into the Spanish market and potentially more competition\n",
      "ð§© Token 3629:  for future growth in\n",
      "the European market as well\n",
      "ð§© Token 3630: ,\" says the brokerage ï¬rm. Score\n",
      "ð§© Token 3631: : 0.06\n",
      "Overall Score: 0.\n",
      "ð§© Token 3632: 085\n",
      "Note: TextBlob is a\n",
      "ð§© Token 3633:  general-purpose sentiment analysis tool that relies on a\n",
      "ð§© Token 3634:  pre-built lexicon\n",
      "to assess the pol\n",
      "ð§© Token 3635: arity of the text. It computes a sentiment\n",
      "ð§© Token 3636:  score ranging from -1 to 1, where\n",
      "\n",
      "ð§© Token 3637: -1 signiï¬es a negative\n",
      "ð§© Token 3638:  sentiment, 1 indicates a positive sentiment, and 0\n",
      "ð§© Token 3639:  represents a neutral\n",
      "sentiment. The methodology behind\n",
      "ð§© Token 3640:  TextBlob focuses on tokenizing the input into\n",
      "ð§© Token 3641:  words and\n",
      "phrases, which are compared\n",
      "ð§© Token 3642:  against its built-in polarity dictionary.\n",
      "\n",
      "ð§© Token 3643: Example 4: A news article about TelefÃ³\n",
      "ð§© Token 3644:  nica and Cellnex | Sentiment: text\n",
      "ð§© Token 3645: 2data\n",
      "Cellnex will face more competition in\n",
      "ð§© Token 3646:  Europe Score: 0.145\n",
      "TelefÃ³n\n",
      "ð§© Token 3647: icaâs (TEF.MC)\n",
      "ð§© Token 3648:  subsidiary, Telxius Telecom, has agreed to\n",
      "ð§© Token 3649:  sell its telecommuni-\n",
      "cations tower\n",
      "ð§© Token 3650:  division in Europe and Latin America to American Tower (\n",
      "ð§© Token 3651: AMT), which\n",
      "will expand the latterï¿½\n",
      "ð§© Token 3652: ï¿½s presence in Europe and increase competition for the\n",
      "ð§© Token 3653:  Spanish\n",
      "wireless telecommunications group Cellnex Telecom (\n",
      "ð§© Token 3654: CLNX.MC), according to Equita\n",
      "ð§© Token 3655: \n",
      "Sim. Score: -0.512 The\n",
      "ð§© Token 3656:  transaction \"represents the entry of a new independent\n",
      "ð§© Token 3657: \n",
      "tower operator into the Spanish market and potentially more\n",
      "ð§© Token 3658:  competition for future\n",
      "growth in the European market as\n",
      "ð§© Token 3659:  well,\" says the brokerage ï¬rm.\n",
      "ð§© Token 3660:  Score: -0.560\n",
      "Overall Score:\n",
      "ð§© Token 3661:  -0.61\n",
      "âWeak_Mod\n",
      "ð§© Token 3662: alâ: 0, âConstr\n",
      "ð§© Token 3663: ainingâ: 0, âComplex\n",
      "ð§© Token 3664: ityâ: 0}\n",
      "57\n",
      "ð§© Token 3665: Note: text2data employs scientiï¿½\n",
      "ð§© Token 3666: ï¿½c deep learning NLP methods to analyze sentiment\n",
      "ð§© Token 3667: . Every\n",
      "sentence is split into smaller chunks\n",
      "ð§© Token 3668:  and represented as a tree structure, capturing the synt\n",
      "ð§© Token 3669: actic\n",
      "relationships between words and phrases. To\n",
      "ð§© Token 3670:  determine the ï¬nal sentiment score,\n",
      "ð§© Token 3671:  text2data\n",
      "uses probabilistic methods based\n",
      "ð§© Token 3672:  on a pre-trained data model, providing an\n",
      "ð§© Token 3673:  output score between\n",
      "-1 and 1, where\n",
      "ð§© Token 3674:  -1 is negative and 1 is positive.\n",
      "\n",
      "ð§© Token 3675: Example 5: A news article about TelefÃ³\n",
      "ð§© Token 3676:  nica and Cellnex | Sentiment: V\n",
      "ð§© Token 3677: ADER\n",
      "Cellnex will face more competition in\n",
      "ð§© Token 3678:  Europe Score: 0.00\n",
      "TelefÃ³n\n",
      "ð§© Token 3679: icaâs (TEF.MC)\n",
      "ð§© Token 3680:  subsidiary, Telxius Telecom, has agreed to\n",
      "ð§© Token 3681:  sell its telecommuni-\n",
      "cations tower\n",
      "ð§© Token 3682:  division in Europe and Latin America to American Tower (\n",
      "ð§© Token 3683: AMT), which\n",
      "will expand the latterï¿½\n",
      "ð§© Token 3684: ï¿½s presence in Europe and increase competition for the\n",
      "ð§© Token 3685:  Spanish\n",
      "wireless telecommunications group Cellnex Telecom (\n",
      "ð§© Token 3686: CLNX.MC), according to Equita\n",
      "ð§© Token 3687: \n",
      "Sim. Score: 0.69 The transaction\n",
      "ð§© Token 3688:  \"represents the entry of a new independent tower\n",
      "ð§© Token 3689: \n",
      "operator into the Spanish market and potentially more competition\n",
      "ð§© Token 3690:  for future growth in\n",
      "the European market as well\n",
      "ð§© Token 3691: ,\" says the brokerage ï¬rm. Score\n",
      "ð§© Token 3692: : 0.57\n",
      "Overall Score: 0.\n",
      "ð§© Token 3693: 81\n",
      "Note: VADER (Valence\n",
      "ð§© Token 3694:  Aware Dictionary and sEntiment Reasoner) is\n",
      "ð§© Token 3695:  a lexicon and rule-based\n",
      "sentiment\n",
      "ð§© Token 3696:  analysis tool uses a combination of lexical features (\n",
      "ð§© Token 3697: i.e., words) that are generally\n",
      "\n",
      "ð§© Token 3698: classiï¬ed as having positive,\n",
      "ð§© Token 3699:  negative, or neutral valence. VADER\n",
      "ð§© Token 3700:  produces four sentiment met-\n",
      "rics: positive,\n",
      "ð§© Token 3701:  negative, neutral, and a compound score. The\n",
      "ð§© Token 3702:  compound score is a normalized,\n",
      "weighted composite\n",
      "ð§© Token 3703:  score that ranges from -1 to 1, indicating\n",
      "ð§© Token 3704:  the overall sentiment of the text.\n",
      "In this\n",
      "ð§© Token 3705:  example, we provide the compound measure sentence by sentence\n",
      "ð§© Token 3706:  and for the whole\n",
      "text.\n",
      "58\n",
      "ð§© Token 3707: Example 6: A news article about TelefÃ³\n",
      "ð§© Token 3708:  nica and Cellnex | Sentiment: Fin\n",
      "ð§© Token 3709: BERT\n",
      "Cellnex will face more competition in\n",
      "ð§© Token 3710:  Europe Negative, 0.75\n",
      "TelefÃ³n\n",
      "ð§© Token 3711: icaâs (TEF.MC)\n",
      "ð§© Token 3712:  subsidiary, Telxius Telecom, has agreed to\n",
      "ð§© Token 3713:  sell its telecommuni-\n",
      "cations tower\n",
      "ð§© Token 3714:  division in Europe and Latin America to American Tower (\n",
      "ð§© Token 3715: AMT), which\n",
      "will expand the latterï¿½\n",
      "ð§© Token 3716: ï¿½s presence in Europe and increase competition for the\n",
      "ð§© Token 3717:  Spanish\n",
      "wireless telecommunications group Cellnex Telecom (\n",
      "ð§© Token 3718: CLNX.MC), according to Equita\n",
      "ð§© Token 3719: \n",
      "Sim. Neutral, 0.98 The transaction\n",
      "ð§© Token 3720:  \"represents the entry of a new independent\n",
      "\n",
      "ð§© Token 3721: tower operator into the Spanish market and potentially more competition\n",
      "ð§© Token 3722:  for future\n",
      "growth in the European market as well\n",
      "ð§© Token 3723: ,\" says the brokerage ï¬rm. Negative\n",
      "ð§© Token 3724: , 0.81\n",
      "Overall Negative, 0.\n",
      "ð§© Token 3725: 94\n",
      "Note: FinBERT is a domain\n",
      "ð§© Token 3726: -speciï¬c transformer-based\n",
      "ð§© Token 3727:  model trained on ï¬nancial texts.\n",
      "ð§© Token 3728:  Unlike\n",
      "the previous models, FinBERT provides\n",
      "ð§© Token 3729:  both a sentiment classiï¬cation (\n",
      "ð§© Token 3730: Positive, Negative,\n",
      "Neutral) and\n",
      "ð§© Token 3731:  a conï¬dence score ranging from\n",
      "ð§© Token 3732:  0 to 1, representing the modelâs\n",
      "ð§© Token 3733:  certainty about\n",
      "the sentiment classiï¬\n",
      "ð§© Token 3734: cation.\n",
      "Why not Topic Modeling as a\n",
      "ð§© Token 3735:  benchmark?\n",
      "Topic modeling, particularly techniques like Lat\n",
      "ð§© Token 3736: ent Dirichlet Allocation (LDA),\n",
      "ð§© Token 3737:  decomposes text into\n",
      "a set of latent topics\n",
      "ð§© Token 3738:  based on word co-occurrences. Topic\n",
      "ð§© Token 3739:  modelling oï¬er a more granular\n",
      "ð§© Token 3740:  approach\n",
      "compared to sentiment analysis and could potentially\n",
      "ð§© Token 3741:  oï¬er a valid benchmark for our\n",
      "ð§© Token 3742:  purpose. However,\n",
      "we argue that transforming news\n",
      "ð§© Token 3743:  articles into vector embeddings and subsequently clustering\n",
      "ð§© Token 3744:  them using\n",
      "KMeans oï¬\n",
      "ð§© Token 3745: ers a more balanced approach than topic modeling.\n",
      "\n",
      "ð§© Token 3746: Topic models rely on bag-of-words representations\n",
      "ð§© Token 3747: , which disregard the order and context of words.\n",
      "ð§© Token 3748: \n",
      "This limitation hampers the modelâ\n",
      "ð§© Token 3749: s ability to capture complex semantic relationships and contextual\n",
      "\n",
      "ð§© Token 3750: nuances essential for accurately identifying economic shocks. Consequently\n",
      "ð§© Token 3751: , topic models may overlook\n",
      "subtle but economically\n",
      "ð§© Token 3752:  signiï¬cant information present in\n",
      "ð§© Token 3753:  the text. On the other hand, vector embed\n",
      "ð§© Token 3754: dings\n",
      "encapsulate rich semantic information by\n",
      "ð§© Token 3755:  capturing the relationships between words in a continuous vector\n",
      "\n",
      "ð§© Token 3756: space. Unlike topic models, which are conï¿½\n",
      "ð§© Token 3757: ï¿½ï¿½ned to word co-occurrences\n",
      "ð§© Token 3758: , embedding models, particularly\n",
      "transformer-\n",
      "ð§© Token 3759: based, generate context-dependent representations, allowing for\n",
      "ð§© Token 3760:  a nuanced understanding of\n",
      "polysemy and context\n",
      "ð§© Token 3761: . This means that the same word can have di\n",
      "ð§© Token 3762: ï¬erent embeddings depending on the\n",
      "ð§© Token 3763: \n",
      "context of the sentence, such as â\n",
      "ð§© Token 3764: Appleâ in âApple is a leading\n",
      "ð§© Token 3765:  tech companyâ versus âApple is a\n",
      "ð§© Token 3766:  type of\n",
      "fruitâ.\n",
      "59\n",
      "ð§© Token 3767: An important advantage of vector embeddings is that\n",
      "ð§© Token 3768:  they scale eï¬ciently with large\n",
      "ð§© Token 3769:  corpora and can\n",
      "be generated at various gran\n",
      "ð§© Token 3770: ularities, including word, sentence, or document\n",
      "ð§© Token 3771:  levels. This scalability makes\n",
      "embeddings\n",
      "ð§© Token 3772:  highly adaptable for diverse downstream tasks such as clust\n",
      "ð§© Token 3773: ering, classiï¬cation, and\n",
      "ð§© Token 3774:  similarity\n",
      "detection. In contrast, topic models\n",
      "ð§© Token 3775:  often require extensive manual tuning and become computationally\n",
      "\n",
      "ð§© Token 3776: expensive with larger datasets, limiting their practicality for\n",
      "ð§© Token 3777:  extensive analyses. This makes embeddings\n",
      "a\n",
      "ð§© Token 3778:  superior choice for grouping news articles and analyzing their economic\n",
      "ð§© Token 3779:  implications, as compared to\n",
      "the relatively rigid and\n",
      "ð§© Token 3780:  broad classiï¬cations produced by\n",
      "ð§© Token 3781:  topic models.\n",
      "It is true, however,\n",
      "ð§© Token 3782:  that topic models excel at grouping articles based on shared\n",
      "ð§© Token 3783:  themes, oï¬ering\n",
      "a straightforward\n",
      "ð§© Token 3784:  way to identify and interpret these themes by examining the\n",
      "ð§© Token 3785:  common content of the\n",
      "grouped articles. This\n",
      "ð§© Token 3786:  interpretability is a key advantage of topic models,\n",
      "ð§© Token 3787:  as it allows for clear labeling\n",
      "of themes.\n",
      "ð§© Token 3788:  In contrast, vector embeddings lack inherent interpret\n",
      "ð§© Token 3789: ability at the dimension level. The\n",
      "individual dimensions\n",
      "ð§© Token 3790:  of an embedding do not have an intuitive meaning\n",
      "ð§© Token 3791: , making it challenging to directly\n",
      "understand the\n",
      "ð§© Token 3792:  relationships they capture. However, this limitation can be\n",
      "ð§© Token 3793:  mitigated by clustering the\n",
      "embeddings\n",
      "ð§© Token 3794:  to then apply a similar interpretive process as with\n",
      "ð§© Token 3795:  topic models: analyzing the articles\n",
      "within each cluster\n",
      "ð§© Token 3796:  to infer the common patterns. As demonstrated in our\n",
      "ð§© Token 3797:  analysis, these clusters often\n",
      "correspond to ï¿½\n",
      "ð§© Token 3798: ï¿½ï¿½rm-speciï¬c\n",
      "ð§© Token 3799:  or industry-speciï¬c topics\n",
      "ð§© Token 3800: , oï¬ering valuable insights into economic\n",
      "ð§© Token 3801:  relationships\n",
      "and forming a valuable benchmark for our LL\n",
      "ð§© Token 3802: Mâs classiï¬cation\n",
      "ð§© Token 3803:  of ï¬rm-speciï¿½\n",
      "ð§© Token 3804: ï¿½c shocks.\n",
      "Lastly, using embedd\n",
      "ð§© Token 3805: ings as a benchmark is particularly compelling because they represent\n",
      "ð§© Token 3806:  the foun-\n",
      "dational layer of an\n",
      "ð§© Token 3807:  LLM. The ï¬rst step\n",
      "ð§© Token 3808:  an LLMâs processing pipeline is to\n",
      "ð§© Token 3809:  transform the text that it is\n",
      "fed into high\n",
      "ð§© Token 3810: -dimensional embeddings for further processing. By\n",
      "ð§© Token 3811:  benchmarking against embeddings, we\n",
      "ens\n",
      "ð§© Token 3812: ure a direct and relevant comparison between the foundational representations\n",
      "ð§© Token 3813:  used by LLMs and our\n",
      "specialized class\n",
      "ð§© Token 3814: iï¬cation methodology. This comparison highlights\n",
      "ð§© Token 3815:  the added value of the LLMâs\n",
      "ð§© Token 3816:  capac-\n",
      "ity to convert these semantic representations (\n",
      "ð§© Token 3817: i.e: the vector embeddings)\n",
      "ð§© Token 3818:  into economically meaningful\n",
      "classiï¬c\n",
      "ð§© Token 3819: ations. (i.e: our news-\n",
      "ð§© Token 3820: implied ï¬rm-speciï¿½\n",
      "ð§© Token 3821: ï¿½ï¿½c shock classiï¬c\n",
      "ð§© Token 3822: ations).\n",
      "In summary, KMeans clust\n",
      "ð§© Token 3823: ering of vector embeddings oï¬\n",
      "ð§© Token 3824: ers a robust and economically relevant bench-\n",
      "mark\n",
      "ð§© Token 3825:  for our LLM-based methodology. It provides\n",
      "ð§© Token 3826:  a rich semantic representation, context-dependent\n",
      "ï¿½\n",
      "ð§© Token 3827: ï¿½ï¿½exibility, and scalability that surpass\n",
      "ð§© Token 3828:  sentiment analysis and topic modeling. Additionally, its alignment\n",
      "ð§© Token 3829: \n",
      "with the underlying architecture of LLMs ensures a\n",
      "ð§© Token 3830:  meaningful comparison. As demonstrated in our\n",
      "analysis,\n",
      "ð§© Token 3831:  the clusters derived through this approach are predominantly ï¿½\n",
      "ð§© Token 3832: ï¿½rm or industry-speciï¬\n",
      "ð§© Token 3833: c, thereby\n",
      "oï¬ering a\n",
      "ð§© Token 3834:  suitable and superior benchmark against which to measure the e\n",
      "ð§© Token 3835: ï¬ectiveness of our granular\n",
      "\n",
      "ð§© Token 3836: classiï¬cation of news-impl\n",
      "ð§© Token 3837: ied ï¬rm-speciï¿½\n",
      "ð§© Token 3838: ï¿½c shocks.\n",
      "A.8 Trading Int\n",
      "ð§© Token 3839: ensity\n",
      "The extraordinary performance of our proposed LLM\n",
      "ð§© Token 3840: -based methodology warrants a careful examina-\n",
      "\n",
      "ð§© Token 3841: tion of its implementation costs and practical viability. While\n",
      "ð§© Token 3842:  our primary objective has been to develop\n",
      "a framework\n",
      "ð§© Token 3843:  that better captures the economic content of news articles and\n",
      "ð§© Token 3844:  their subsequent market\n",
      "60\n",
      "ð§© Token 3845: impact, the practical implementation of such strategies necessarily involves\n",
      "ð§© Token 3846:  trading frictions that could\n",
      "aï¬\n",
      "ð§© Token 3847: ect their real-world eï¬c\n",
      "ð§© Token 3848: acy. In this section, we analyze the trading\n",
      "ð§© Token 3849:  intensity patterns of both method-\n",
      "ologies to provide\n",
      "ð§© Token 3850:  a more complete assessment of their relative merits and to\n",
      "ð§© Token 3851:  understand how transaction\n",
      "costs might inï¿½\n",
      "ð§© Token 3852: ï¿½uence their comparative advantages. We begin by\n",
      "ð§© Token 3853:  examining the temporal evolution of\n",
      "open positions for both\n",
      "ð§© Token 3854:  approaches, which provides insights into their underlying trading dynamics\n",
      "ð§© Token 3855:  and\n",
      "stability characteristics. This analysis is followed\n",
      "ð§© Token 3856:  by detailed trading intensity metrics and concludes with\n",
      "a\n",
      "ð§© Token 3857:  reassessment of portfolio statistics after accounting for transaction costs\n",
      "ð§© Token 3858: .\n",
      "[Insert Figure A6 about here]\n",
      "ð§© Token 3859: \n",
      "The temporal evolution of open positions, as illustrated\n",
      "ð§© Token 3860:  in Figure A6, reveals fundamental diï¿½\n",
      "ð§© Token 3861: ï¿½er-\n",
      "ences in the stability and reliability\n",
      "ð§© Token 3862:  of trading signals generated by KMeans versus LL\n",
      "ð§© Token 3863: M-based clustering\n",
      "approaches. The\n",
      "ð§© Token 3864:  KMeans implementation exhibits pronounced volatility in position management\n",
      "ð§© Token 3865: , partic-\n",
      "ularly evident in the Gre\n",
      "ð§© Token 3866: edy algorithmâs behavior, which shows extreme\n",
      "ð§© Token 3867:  ï¬uctuations ranging from 6 to\n",
      "\n",
      "ð§© Token 3868: 105 positions. This erratic pattern suggests that KMe\n",
      "ð§© Token 3869: ans-detected clusters are highly sensitive to market\n",
      "ð§© Token 3870: \n",
      "noise and potentially capture transient correlations rather than\n",
      "ð§© Token 3871:  fundamental relationships. The substan-\n",
      "tial\n",
      "ð§© Token 3872:  divergence between Greedy and Stable algorithms under K\n",
      "ð§© Token 3873: Means further underscores the methodâs\n",
      "\n",
      "ð§© Token 3874: instability, as even minor variations in cluster selection\n",
      "ð§© Token 3875:  criteria lead to dramatically diï¬erent trading\n",
      "ð§© Token 3876:  de-\n",
      "cisions. In stark contrast,\n",
      "ð§© Token 3877:  the LLM-based approach demonstrates remarkably more coherent\n",
      "ð§© Token 3878:  and stable\n",
      "position management. Both Greedy and\n",
      "ð§© Token 3879:  Stable algorithms maintain more closely aligned position counts,\n",
      "ð§© Token 3880: \n",
      "typically ranging between 20 and 75 positions, with\n",
      "ð§© Token 3881:  highly correlated temporal movements. This conver-\n",
      "gence\n",
      "ð§© Token 3882:  in behavior between algorithms suggests that LLM-ident\n",
      "ð§© Token 3883: iï¬ed clusters capture more fundamental\n",
      "\n",
      "ð§© Token 3884: and persistent market relationships. Particularly telling is the test\n",
      "ð§© Token 3885:  period performance, where KMeans\n",
      "exhib\n",
      "ð§© Token 3886: its increased position volatility and extreme spikes, while the\n",
      "ð§© Token 3887:  LLM approach maintains consistent\n",
      "position patterns across both\n",
      "ð§© Token 3888:  algorithms. This stability in the out-of-\n",
      "ð§© Token 3889: sample period provides strong ev-\n",
      "idence that LL\n",
      "ð§© Token 3890: M-derived signals, grounded in economic analysis of\n",
      "ð§© Token 3891:  ï¬rm-speciï¬\n",
      "ð§© Token 3892: c shocks, generalize more\n",
      "eï¿½\n",
      "ð§© Token 3893: ï¿½ectively to unseen data.\n",
      "[Insert\n",
      "ð§© Token 3894:  Table A3 about here]\n",
      "The trading intensity\n",
      "ð§© Token 3895:  metrics, detailed in Table A3, provide quantitative\n",
      "ð§© Token 3896:  validation of the structural\n",
      "diï¬erences\n",
      "ð§© Token 3897:  between KMeans and LLM clustering approaches\n",
      "ð§© Token 3898: . Under KMeans, the dramatic disparity\n",
      "\n",
      "ð§© Token 3899: between Greedy and Stable algorithms (averaging\n",
      "ð§© Token 3900:  40.1 versus 10.77 positions, with\n",
      "ð§© Token 3901:  standard deviations of\n",
      "18.59 and 6.\n",
      "ð§© Token 3902: 41 respectively) reï¬ects the\n",
      "ð§© Token 3903:  methodâs fundamental instability. More concerning is\n",
      "ð§© Token 3904:  the Stable\n",
      "algorithmâs exceptionally\n",
      "ð§© Token 3905:  high Changes/Position ratio (3.228 versus\n",
      "ð§© Token 3906:  0.798 for Greedy), indicating frequent\n",
      "\n",
      "ð§© Token 3907: position adjustments necessitated by the transient nature of K\n",
      "ð§© Token 3908: Means-identiï¬ed clusters\n",
      "ð§© Token 3909: . The LLM imple-\n",
      "mentation demonstrates\n",
      "ð§© Token 3910:  substantially more balanced and stable metrics across both algorithms.\n",
      "ð§© Token 3911:  Average\n",
      "position counts converge (31.8 for\n",
      "ð§© Token 3912:  Greedy, 26.61 for Stable)\n",
      "ð§© Token 3913:  with more moderate standard deviations\n",
      "61\n",
      "ð§© Token 3914: (14.84 and 12.16), suggesting\n",
      "ð§© Token 3915:  that both aggressive and conservative cluster selection approaches identify\n",
      "\n",
      "ð§© Token 3916: similar, fundamentally-driven trading opportunities. The more\n",
      "ð§© Token 3917:  balanced Changes/Position ratios (1.234\n",
      "\n",
      "ð§© Token 3918: and 1.473) and consistent turnover rates (\n",
      "ð§© Token 3919: approximately 39% for both algorithms) indicate that LL\n",
      "ð§© Token 3920: M-\n",
      "identiï¬ed clusters\n",
      "ð§© Token 3921:  require less frequent rebalancing, supporting the hypothesis\n",
      "ð§© Token 3922:  that they capture more\n",
      "persistent market relationships.\n",
      "ð§© Token 3923: \n",
      "[Insert Table A4 about here]\n",
      "\n",
      "ð§© Token 3924: Finally, the introduction of trading costs impacts the performance\n",
      "ð§© Token 3925:  metrics of both clustering ap-\n",
      "proaches\n",
      "ð§© Token 3926:  (see Table A4), though with notably di\n",
      "ð§© Token 3927: ï¬erent implications for their practical viability.\n",
      "ð§© Token 3928:  The\n",
      "KMeans-based strategy exhibits visible\n",
      "ð§© Token 3929:  performance degradation, particularly evident in the test period\n",
      "\n",
      "ð§© Token 3930: where both algorithms generate losses (Greedy: -\n",
      "ð§© Token 3931: 4.1%, Stable: -6.\n",
      "ð§© Token 3932: 8% average annual returns). This\n",
      "deter\n",
      "ð§© Token 3933: ioration is accompanied by elevated risk metrics, with\n",
      "ð§© Token 3934:  the Stable algorithm showing particularly\n",
      "concerning characteristics\n",
      "ð§© Token 3935:  including high standard deviation (14.2%) and\n",
      "ð§© Token 3936:  extreme kurtosis (14.74) in\n",
      "ð§© Token 3937:  the\n",
      "test period, suggesting frequent occurrence of extreme\n",
      "ð§© Token 3938:  returns. In contrast, the LLM-based\n",
      "ð§© Token 3939:  approach\n",
      "demonstrates superior resilience to trading costs\n",
      "ð§© Token 3940: , maintaining more stable performance characteristics\n",
      "across all\n",
      "ð§© Token 3941:  periods. Most notably, in the test period,\n",
      "ð§© Token 3942:  the strategy maintains its positive performance\n",
      "(Greedy\n",
      "ð§© Token 3943: : 19.0%, Stable: 24.\n",
      "ð§© Token 3944: 7% annual returns) with substantially lower risk metrics\n",
      "ð§© Token 3945:  (standard deviations\n",
      "of 6.2% and\n",
      "ð§© Token 3946:  7.0% respectively). The LLM approach\n",
      "ð§© Token 3947: âs more moderate VaR and CVa\n",
      "ð§© Token 3948: R measures compared\n",
      "to KMeans further underscore\n",
      "ð§© Token 3949:  its superior risk management characteristics under transaction costs. This\n",
      "ð§© Token 3950: \n",
      "stark contrast in net performance can be attributed\n",
      "ð§© Token 3951:  to the fundamentally diï¬erent nature of\n",
      "ð§© Token 3952:  the signals\n",
      "generated by each approach. While K\n",
      "ð§© Token 3953: Meansâstatistically-driven clusters require\n",
      "ð§© Token 3954:  frequent rebalancing that\n",
      "ampliï¿½\n",
      "ð§© Token 3955: ï¿½ï¿½es transaction costs, the LLMï¿½\n",
      "ð§© Token 3956: ï¿½s economically-motivated clusters appear to identify\n",
      "ð§© Token 3957:  more persistent\n",
      "price patterns that remain proï¿½\n",
      "ð§© Token 3958: ï¿½table even after accounting for trading frictions.\n",
      "ð§© Token 3959:  However, it is worth\n",
      "noting that neither\n",
      "ð§© Token 3960:  approach was explicitly optimized for transaction cost eï¿½\n",
      "ð§© Token 3961: ï¿½ciency, suggesting potential\n",
      "for further improvement\n",
      "ð§© Token 3962:  through cost-aware portfolio construction. These results highlight\n",
      "ð§© Token 3963:  that while our\n",
      "LLM-based news parser\n",
      "ð§© Token 3964:  successfully captures predictable market reactions to news articles, practitioners\n",
      "ð§© Token 3965: \n",
      "implementing such strategies would beneï¿½\n",
      "ð§© Token 3966: ï¿½t from incorporating transaction costs into their optimization\n",
      "\n",
      "ð§© Token 3967: framework.\n",
      "62\n",
      "ð§© Token 3968: Table A3: Trading Intensity Analysis: Model\n",
      "ð§© Token 3969:  Comparison\n",
      "(a) Panel A: KMe\n",
      "ð§© Token 3970: ans\n",
      "Split Algorithm # Open Positions Trading\n",
      "ð§© Token 3971:  Activity (%) Trading Costs (%)\n",
      "Avg. Std\n",
      "ð§© Token 3972: . Max Min Turnover Changes/Pos. Cost\n",
      "ð§© Token 3973:  Active\n",
      "All Greedy 40.1 18.\n",
      "ð§© Token 3974: 59 105 6 32.03 0.798 0\n",
      "ð§© Token 3975: .0320 100.0\n",
      "Stable 10\n",
      "ð§© Token 3976: .77 6.41 30 0 34.75\n",
      "ð§© Token 3977:  3.228 0.0347 99.1\n",
      "ð§© Token 3978: \n",
      "Train Greedy 36.4 19.33\n",
      "ð§© Token 3979:  88 7 30.59 0.840 0.\n",
      "ð§© Token 3980: 0306 100.0\n",
      "Stable 9.\n",
      "ð§© Token 3981: 89 5.93 27 0 33.73 3\n",
      "ð§© Token 3982: .412 0.0337 98.2\n",
      "\n",
      "ð§© Token 3983: Validation Greedy 48.4 10.00\n",
      "ð§© Token 3984:  80 30 31.39 0.649 0.\n",
      "ð§© Token 3985: 0314 100.0\n",
      "Stable 12.\n",
      "ð§© Token 3986: 34 6.05 30 1 33.42 2\n",
      "ð§© Token 3987: .708 0.0334 100.0\n",
      "\n",
      "ð§© Token 3988: Test Greedy 38.8 21.74 105\n",
      "ð§© Token 3989:  6 35.86 0.925 0.0\n",
      "ð§© Token 3990: 359 100.0\n",
      "Stable 10.84\n",
      "ð§© Token 3991:  7.47 28 1 39.30 3.\n",
      "ð§© Token 3992: 626 0.0393 100.0\n",
      "(\n",
      "ð§© Token 3993: b) Panel B: LLM\n",
      "Split Al\n",
      "ð§© Token 3994: gorithm # Open Positions Trading Activity (%) Trading Costs\n",
      "ð§© Token 3995:  (%)\n",
      "Avg. Std. Max Min Turn\n",
      "ð§© Token 3996: over Changes/Pos. Cost Active\n",
      "All Gre\n",
      "ð§© Token 3997: edy 31.8 14.84 75 4 39\n",
      "ð§© Token 3998: .21 1.234 0.0392 100\n",
      "ð§© Token 3999: .0\n",
      "Stable 26.61 12.\n",
      "ð§© Token 4000: 16 56 3 39.18 1.473 0\n",
      "ð§© Token 4001: .0392 100.0\n",
      "Train Greedy\n",
      "ð§© Token 4002:  29.9 16.34 75 4 40.\n",
      "ð§© Token 4003: 42 1.351 0.0404 100.\n",
      "ð§© Token 4004: 0\n",
      "Stable 25.54 12.90\n",
      "ð§© Token 4005:  56 3 40.45 1.584 0.\n",
      "ð§© Token 4006: 0404 100.0\n",
      "Validation Greedy\n",
      "ð§© Token 4007:  37.0 7.69 58 24 38.\n",
      "ð§© Token 4008: 43 1.039 0.0384 100\n",
      "ð§© Token 4009: .0\n",
      "Stable 31.38 6.\n",
      "ð§© Token 4010: 82 50 17 37.95 1.209 0\n",
      "ð§© Token 4011: .0379 100.0\n",
      "Test Greedy\n",
      "ð§© Token 4012:  29.7 16.24 75 6 37.\n",
      "ð§© Token 4013: 56 1.264 0.0376 100.\n",
      "ð§© Token 4014: 0\n",
      "Stable 23.43 13.71\n",
      "ð§© Token 4015:  54 3 37.85 1.615 0.\n",
      "ð§© Token 4016: 0378 100.0\n",
      "Note: This table\n",
      "ð§© Token 4017:  presents trading intensity metrics for both Greedy and St\n",
      "ð§© Token 4018: able algorithms across diï¬erent data splits\n",
      "ð§© Token 4019: \n",
      "for two diï¬erent models:\n",
      "ð§© Token 4020:  KMeans (Panel A) and LLM\n",
      "ð§© Token 4021:  (Panel B). The metrics are computed at a\n",
      "ð§© Token 4022:  daily frequency.\n",
      "The â# Open Pos\n",
      "ð§© Token 4023: itionsâ columns report position-related statistics:\n",
      "ð§© Token 4024:  âAvg.â shows the mean number\n",
      "ð§© Token 4025:  of concurrent\n",
      "open positions per day, â\n",
      "ð§© Token 4026: Std.â represents their standard deviation,\n",
      "ð§© Token 4027:  while âMaxâand âMin\n",
      "ð§© Token 4028: âindicate the maximum and\n",
      "minimum number\n",
      "ð§© Token 4029:  of positions held simultaneously. Under âTrading\n",
      "ð§© Token 4030:  Activity (%)â, âTurnoverï¿½\n",
      "ð§© Token 4031: ï¿½ is calculated as the\n",
      "sum of absolute changes\n",
      "ð§© Token 4032:  in position sizes divided by the total portfolio size,\n",
      "ð§© Token 4033:  expressed as a percentage; formally,\n",
      "T \n",
      "ð§© Token 4034: urnovert = 100 Ã (ï¿½\n",
      "ð§© Token 4035: ï¿½\n",
      "i |wi,t â wi,\n",
      "ð§© Token 4036: tâ1|)/(ó°\n",
      "ð§© Token 4037: \n",
      "i |wi,t|), where wi\n",
      "ð§© Token 4038: ,t represents the position size in asset i at\n",
      "ð§© Token 4039:  time t.\n",
      "âChanges/Pos.\n",
      "ð§© Token 4040: â represents the average number of modiï¿½\n",
      "ð§© Token 4041: ï¿½ï¿½cations per position per day, computed\n",
      "ð§© Token 4042:  as the daily turnover\n",
      "divided by the average\n",
      "ð§© Token 4043:  number of positions, providing insight into how actively individual\n",
      "ð§© Token 4044:  positions are managed. The\n",
      "âTrading\n",
      "ð§© Token 4045:  Costs (%)â section reports âCostï¿½\n",
      "ð§© Token 4046: ï¿½ as the average daily implementation shortfall (computed\n",
      "ð§© Token 4047:  as the product\n",
      "of daily turnover and a transaction\n",
      "ð§© Token 4048:  cost parameter of 10 basis points) expressed in percentage\n",
      "ð§© Token 4049:  terms, while âActiveâ\n",
      "shows\n",
      "ð§© Token 4050:  the percentage of trading days with at least one open\n",
      "ð§© Token 4051:  position. All metrics are ï¬rst\n",
      "ð§© Token 4052:  computed daily and then\n",
      "averaged over their respective\n",
      "ð§© Token 4053:  periods, except for Max and Min positions which represent\n",
      "ð§© Token 4054:  the absolute extremes over\n",
      "each period.\n",
      "63\n",
      "ð§© Token 4055: 64\n",
      "ð§© Token 4056: Table A4: Portfolio Statistics Comparison: K\n",
      "ð§© Token 4057: Means vs LLM Clustering (net\n",
      "ð§© Token 4058:  of Trading Costs)\n",
      "(a) Panel A\n",
      "ð§© Token 4059: : Statistics of PKMeans\n",
      "Split Algo\n",
      "ð§© Token 4060: . Cum.\n",
      "Ret.\n",
      "Avg.\n",
      "\n",
      "ð§© Token 4061: Ret.\n",
      "St.\n",
      "Dev.\n",
      "Shar\n",
      "ð§© Token 4062: pe\n",
      "Ra-\n",
      "tio\n",
      "Sortino\n",
      "ð§© Token 4063: \n",
      "Ra-\n",
      "tio\n",
      "Max.\n",
      "\n",
      "ð§© Token 4064: DD\n",
      "Calmar\n",
      "Ratio\n",
      "Ske\n",
      "ð§© Token 4065: w. Exc.\n",
      "Kurt.\n",
      "Va\n",
      "ð§© Token 4066: R\n",
      "95%\n",
      "CVaR\n",
      "95\n",
      "ð§© Token 4067: %\n",
      "All Greedy 0.963 -\n",
      "ð§© Token 4068: 2.9 9.6 -0.3\n",
      "ð§© Token 4069:  -0.3 -9.5 -0\n",
      "ð§© Token 4070: .3 -0.46 4.00 -\n",
      "ð§© Token 4071: 13.7 -23.4\n",
      "Stable\n",
      "ð§© Token 4072:  1.329 24.4 16.8 1\n",
      "ð§© Token 4073: .3 1.5 -8.3 2\n",
      "ð§© Token 4074: .9 0.18 5.08 -23\n",
      "ð§© Token 4075: .0 -36.8\n",
      "Train Greedy\n",
      "ð§© Token 4076:  0.911 -13.2 11.6\n",
      "ð§© Token 4077:  -1.2 -1.1 -9\n",
      "ð§© Token 4078: .5 -1.4 -0.52\n",
      "ð§© Token 4079:  2.72 -18.7 -28.\n",
      "ð§© Token 4080: 9\n",
      "Stable 1.182 28.9\n",
      "ð§© Token 4081:  19.7 1.3 1.4 -\n",
      "ð§© Token 4082: 8.3 3.5 -0.23\n",
      "ð§© Token 4083:  3.24 -30.6 -44.\n",
      "ð§© Token 4084: 0\n",
      "Validation Greedy 1.058\n",
      "ð§© Token 4085:  17.1 7.3 2.2 2\n",
      "ð§© Token 4086: .2 -4.0 4.3 -\n",
      "ð§© Token 4087: 0.48 1.10 -10.2\n",
      "ð§© Token 4088:  -16.6\n",
      "Stable 1.115\n",
      "ð§© Token 4089:  35.7 13.3 2.3 2\n",
      "ð§© Token 4090: .6 -4.2 8.6 -\n",
      "ð§© Token 4091: 0.23 1.85 -19.3\n",
      "ð§© Token 4092:  -29.0\n",
      "Test Greedy 0.\n",
      "ð§© Token 4093: 988 -4.1 6.8 -\n",
      "ð§© Token 4094: 0.6 -0.8 -5.\n",
      "ð§© Token 4095: 3 -0.8 1.76 5.\n",
      "ð§© Token 4096: 10 -8.2 -10.4\n",
      "\n",
      "ð§© Token 4097: Stable 0.979 -6.8\n",
      "ð§© Token 4098:  14.2 -0.5 -0.\n",
      "ð§© Token 4099: 6 -5.6 -1.2 2\n",
      "ð§© Token 4100: .49 14.74 -19.4 -\n",
      "ð§© Token 4101: 27.4\n",
      "(b) Panel B:\n",
      "ð§© Token 4102:  Statistics of PLLM\n",
      "Split Algo.\n",
      "ð§© Token 4103:  Cum.\n",
      "Ret.\n",
      "Avg.\n",
      "Ret\n",
      "ð§© Token 4104: .\n",
      "St.\n",
      "Dev.\n",
      "Sharpe\n",
      "ð§© Token 4105: \n",
      "Ra-\n",
      "tio\n",
      "Sortino\n",
      "\n",
      "ð§© Token 4106: Ra-\n",
      "tio\n",
      "Max.\n",
      "DD\n",
      "ð§© Token 4107: \n",
      "Calmar\n",
      "Ratio\n",
      "Skew\n",
      "ð§© Token 4108: . Exc.\n",
      "Kurt.\n",
      "VaR\n",
      "ð§© Token 4109: \n",
      "95%\n",
      "CVaR\n",
      "95%\n",
      "ð§© Token 4110: \n",
      "All Greedy 1.152 11.5\n",
      "ð§© Token 4111:  9.6 1.1 1.4 -\n",
      "ð§© Token 4112: 7.6 1.5 1.47 9\n",
      "ð§© Token 4113: .93 -14.4 -19.6\n",
      "ð§© Token 4114: \n",
      "Stable 1.200 15.0 8\n",
      "ð§© Token 4115: .6 1.6 1.9 -7\n",
      "ð§© Token 4116: .2 2.1 0.29 2.\n",
      "ð§© Token 4117: 23 -12.1 -17.4\n",
      "\n",
      "ð§© Token 4118: Train Greedy 1.040 6.2 11\n",
      "ð§© Token 4119: .4 0.5 0.7 -7\n",
      "ð§© Token 4120: .6 0.8 1.65 8.\n",
      "ð§© Token 4121: 97 -16.2 -21.6\n",
      "\n",
      "ð§© Token 4122: Stable 1.101 15.9 9.\n",
      "ð§© Token 4123: 9 1.5 1.7 -7.\n",
      "ð§© Token 4124: 2 2.2 0.18 1.68\n",
      "ð§© Token 4125:  -14.1 -20.3\n",
      "Val\n",
      "ð§© Token 4126: idation Greedy 1.054 16.2\n",
      "ð§© Token 4127:  8.2 1.8 2.3 -\n",
      "ð§© Token 4128: 3.3 4.9 0.16 1\n",
      "ð§© Token 4129: .31 -10.9 -17.2\n",
      "ð§© Token 4130: \n",
      "Stable 1.013 3.8 7\n",
      "ð§© Token 4131: .0 0.5 0.6 -2\n",
      "ð§© Token 4132: .2 1.7 0.22 1.\n",
      "ð§© Token 4133: 31 -11.7 -15.3\n",
      "\n",
      "ð§© Token 4134: Test Greedy 1.054 19.0\n",
      "ð§© Token 4135:  6.2 2.8 3.5 -\n",
      "ð§© Token 4136: 1.6 11.9 1.35 7\n",
      "ð§© Token 4137: .85 -7.5 -10.6\n",
      "ð§© Token 4138: \n",
      "Stable 1.069 24.7\n",
      "ð§© Token 4139:  7.0 3.1 4.7 -\n",
      "ð§© Token 4140: 1.3 18.6 0.86 1\n",
      "ð§© Token 4141: .99 -10.1 -11.6\n",
      "ð§© Token 4142: \n",
      "Note: Portfolio statistics of trading strategies based\n",
      "ð§© Token 4143:  on clusters obtained from KMeans (Panel A\n",
      "ð§© Token 4144: ) and LLM (Panel\n",
      "B) approaches\n",
      "ð§© Token 4145: . The statistics provided include performance metrics (Cum\n",
      "ð§© Token 4146: ulative Return, Average Return (%)), risk\n",
      "measures\n",
      "ð§© Token 4147:  (Standard Deviation (%), Maximum Drawdown (\n",
      "ð§© Token 4148: %), Value at Risk (%), Conditional Value at\n",
      "ð§© Token 4149:  Risk (%)),\n",
      "risk-adjusted performance ratios (\n",
      "ð§© Token 4150: Sharpe Ratio, Sortino Ratio, Calmar\n",
      "ð§© Token 4151:  Ratio), and return distribution characteristics\n",
      "(Ske\n",
      "ð§© Token 4152: wness, Excess Kurtosis). These statistics\n",
      "ð§© Token 4153:  are provided for both cluster-selection algorithms: Gre\n",
      "ð§© Token 4154: edy and Stable.\n",
      "Except for the Cum\n",
      "ð§© Token 4155: ulative Return, all returns are annualized. The\n",
      "ð§© Token 4156:  Sharpe Ratio is computed using the daily returns,\n",
      "ð§© Token 4157: \n",
      "assuming 252 trading days in a year. The\n",
      "ð§© Token 4158:  Sortino Ratio is calculated using the daily downside returns\n",
      "ð§© Token 4159: . The\n",
      "Maximum Drawdown is the maximum loss\n",
      "ð§© Token 4160:  from a peak to a trough. The Calmar\n",
      "ð§© Token 4161:  Ratio is the ratio of the annualized\n",
      "return\n",
      "ð§© Token 4162:  to the maximum drawdown. Skewness measures\n",
      "ð§© Token 4163:  the asymmetry of the return distribution, while Kurt\n",
      "ð§© Token 4164: osis\n",
      "quantiï¬es the tails\n",
      "ð§© Token 4165: â thickness. The Value at Risk (Va\n",
      "ð§© Token 4166: R) and Conditional Value at Risk (CV\n",
      "ð§© Token 4167: aR) are calculated\n",
      "at a 95%\n",
      "ð§© Token 4168:  conï¬dence level. All returns\n",
      "ð§© Token 4169:  are calculated net of transaction costs. We implement a\n",
      "ð§© Token 4170:  transaction cost\n",
      "estimate of 10 basis points per\n",
      "ð§© Token 4171:  trade,. The Greedy algorithm longs (sh\n",
      "ð§© Token 4172: orts) clusters that maximize (minimize) the\n",
      "ð§© Token 4173: \n",
      "cluster-average-SR in the validation\n",
      "ð§© Token 4174:  sample subject to a positivity (negativity)\n",
      "ð§© Token 4175:  constraint, while the Stable algorithm\n",
      "longs\n",
      "ð§© Token 4176:  (shorts) clusters that minimize the rank di\n",
      "ð§© Token 4177: ï¬erence between the training and validation rankings\n",
      "ð§© Token 4178:  of the cluster-\n",
      "average-SRâ\n",
      "ð§© Token 4179: s subject to a positivity (negativity)\n",
      "ð§© Token 4180:  constraint, which is now imposed on both sample splits\n",
      "ð§© Token 4181: . In both\n",
      "algorithms, the cardinal\n",
      "ð§© Token 4182: ity of each leg is upper-bounded by\n",
      "ð§© Token 4183:  a hyperparameter Î¸. The holding period\n",
      "ð§© Token 4184:  of the beta-\n",
      "neutral positions is set to\n",
      "ð§© Token 4185:  L = 4 trading days for both approaches. The\n",
      "ð§© Token 4186:  number of traded clusters is Î¸ = 0.\n",
      "ð§© Token 4187: 5k = 13\n",
      "for KMeans (\n",
      "ð§© Token 4188: kâ = 26 clusters) and Î¸\n",
      "ð§© Token 4189:  = 0.5k = 10 for LLM\n",
      "ð§© Token 4190:  (kâ = 20 clusters). The selection\n",
      "ð§© Token 4191:  criteria for these\n",
      "hyperparameters (L,\n",
      "ð§© Token 4192:  Î¸) is based on maximizing the Sharpe\n",
      "ð§© Token 4193:  Ratios of the train and validation samples.\n",
      "\n",
      "ð§© Token 4194: 65\n",
      "ð§© Token 4195: Figure A1: Sharpe Ratios in the\n",
      "ð§© Token 4196:  train and validation splits as a function of L (\n",
      "ð§© Token 4197: KMeans)\n",
      "246810121416\n",
      "ð§© Token 4198: 1820Holding period length (L)Â°\n",
      "ð§© Token 4199: 1â¿0Â°0â¿\n",
      "ð§© Token 4200: 50â¿00â¿51ï¿½\n",
      "ð§© Token 4201: ï¿½ï¿½01â¿52â¿\n",
      "ð§© Token 4202: 02â¿53â¿0Shar\n",
      "ð§© Token 4203: pe Ratio (Train)\n",
      "GreedyStable\n",
      "ð§© Token 4204: \n",
      "(a) Plot of SRPtr\n",
      "(\n",
      "ð§© Token 4205: L) over a grid of L\n",
      "2 4\n",
      "ð§© Token 4206:  6 8101214161820Holding period\n",
      "ð§© Token 4207:  length (L)\n",
      "012345Sharpe\n",
      "ð§© Token 4208:  Ratio (Validation)\n",
      "GreedyStable\n",
      "ð§© Token 4209: \n",
      "(b) Plot of SRPval\n",
      "\n",
      "ð§© Token 4210: (L) over a grid of L\n",
      "Note\n",
      "ð§© Token 4211: : This ï¬gure shows the Shar\n",
      "ð§© Token 4212: pe Ratios (SR) as a function of\n",
      "ð§© Token 4213:  the holding period length (L) for the K\n",
      "ð§© Token 4214: Means\n",
      "clustering method in the training\n",
      "ð§© Token 4215:  (Panel a) and validation (Panel b)\n",
      "ð§© Token 4216:  splits. In Panel (a), the Sharpe\n",
      "ð§© Token 4217:  Ratios in\n",
      "the training set indicate that lower\n",
      "ð§© Token 4218:  values of L (less than 4) maximize performance\n",
      "ð§© Token 4219: . Conversely, in Panel (b), the\n",
      "\n",
      "ð§© Token 4220: validation set shows higher Sharpe Ratios for\n",
      "ð§© Token 4221:  longer holding periods. The choice of L = 4\n",
      "ð§© Token 4222:  represents a balanced\n",
      "compromise, providing a\n",
      "ð§© Token 4223:  stable Sharpe Ratio proï¬le across\n",
      "ð§© Token 4224:  both splits, ensuring consistent in-sample performance\n",
      "\n",
      "ð§© Token 4225: without introducing lookahead bias.\n",
      "66\n",
      "ð§© Token 4226: Figure A2: Sharpe Ratios in the\n",
      "ð§© Token 4227:  train and validation splits as a function of Î¸\n",
      "ð§© Token 4228:  (KMeans)\n",
      "123456789\n",
      "ð§© Token 4229: 1011121314Number of traded clusters (ï¿½\n",
      "ð§© Token 4230: ï¿½)\n",
      "Â°0â¿50ï¿½\n",
      "ð§© Token 4231: ï¿½ï¿½00â¿51â¿\n",
      "ð§© Token 4232: 01â¿52â¿0Shar\n",
      "ð§© Token 4233: pe Ratio (Train)\n",
      "GreedyStable\n",
      "ð§© Token 4234: \n",
      "(a) Plot of SRPtr\n",
      "(\n",
      "ð§© Token 4235: Î¸) over a grid of Î¸\n",
      "\n",
      "ð§© Token 4236: 1234567891011121314Number of\n",
      "ð§© Token 4237:  traded clusters (Âµ)\n",
      "Â°10123\n",
      "ð§© Token 4238: 45Sharpe Ratio (Validation)\n",
      "Gre\n",
      "ð§© Token 4239: edyStable\n",
      "(b) Plot of SR\n",
      "ð§© Token 4240: Pval\n",
      "(Î¸) over a grid\n",
      "ð§© Token 4241:  of Î¸\n",
      "Note: This ï¬\n",
      "ð§© Token 4242: gure illustrates the Sharpe Ratios (SR\n",
      "ð§© Token 4243: ) as a function of Î¸, the upper\n",
      "ð§© Token 4244:  bound on the number of traded\n",
      "clusters,\n",
      "ð§© Token 4245:  for the KMeans clustering method in the\n",
      "ð§© Token 4246:  training (Panel a) and validation (Panel b\n",
      "ð§© Token 4247: ) splits. In Panel\n",
      "(a), the\n",
      "ð§© Token 4248:  Sharpe Ratios in the training set show a\n",
      "ð§© Token 4249:  trend of increasing stability and maximizing performance as Î¸\n",
      "ð§© Token 4250: \n",
      "approaches its upper limit. Similarly, Panel\n",
      "ð§© Token 4251:  (b) displays a consistent pattern in the validation\n",
      "ð§© Token 4252:  set, where higher\n",
      "values of Î¸ lead\n",
      "ð§© Token 4253:  to convergence at the highest and most stable Sharpe\n",
      "ð§© Token 4254:  Ratios. The choice of Î¸ = 13\n",
      "ð§© Token 4255:  (i.e: â0.\n",
      "ð§© Token 4256: 5 Â· 26â)\n",
      "reï¿½\n",
      "ð§© Token 4257: ï¿½ï¿½ects this observed stability and optimization,\n",
      "ð§© Token 4258:  providing a balanced and robust selection for the portfolio strategy\n",
      "ð§© Token 4259: .\n",
      "67\n",
      "ð§© Token 4260: Figure A3: Sharpe Ratios in the\n",
      "ð§© Token 4261:  train and validation splits as a function of hyperparam\n",
      "ð§© Token 4262: eters (LLM)\n",
      "2468101214\n",
      "ð§© Token 4263: 161820Holding period length (L)\n",
      "ð§© Token 4264: Â°1â¿0Â°0ï¿½\n",
      "ð§© Token 4265: ï¿½50â¿00â¿51\n",
      "ð§© Token 4266: â¿01â¿52ï¿½\n",
      "ð§© Token 4267: ï¿½02â¿5Sharpe Ratio (\n",
      "ð§© Token 4268: Train)\n",
      "GreedyStable\n",
      "(a\n",
      "ð§© Token 4269: ) Plot of SRPtr\n",
      "(L) over\n",
      "ð§© Token 4270:  a grid of L\n",
      "246810121416\n",
      "ð§© Token 4271: 1820Holding period length (L)0\n",
      "ð§© Token 4272: â¿00â¿51ï¿½\n",
      "ð§© Token 4273: ï¿½01â¿52â¿02\n",
      "ð§© Token 4274: â¿53â¿03ï¿½\n",
      "ð§© Token 4275: ï¿½5Sharpe Ratio (Validation)\n",
      "\n",
      "ð§© Token 4276: GreedyStable\n",
      "(b) Plot of\n",
      "ð§© Token 4277:  SRPval\n",
      "(L) over a grid\n",
      "ð§© Token 4278:  of L\n",
      "Note: This ï¬g\n",
      "ð§© Token 4279: ure shows the Sharpe Ratios (SR)\n",
      "ð§© Token 4280:  as a function of the holding period length (L\n",
      "ð§© Token 4281: ) for the LLM clustering\n",
      "method,\n",
      "ð§© Token 4282:  across the training (Panel a) and validation (\n",
      "ð§© Token 4283: Panel b) splits. In Panel (a),\n",
      "ð§© Token 4284:  the Sharpe Ratios in the training\n",
      "set\n",
      "ð§© Token 4285:  reach their maximum at L = 4, suggesting shorter\n",
      "ð§© Token 4286:  holding periods are more eï¬ective for\n",
      "ð§© Token 4287:  maximizing performance.\n",
      "Conversely, Panel (b\n",
      "ð§© Token 4288: ) illustrates that longer holding periods yield higher Sharpe\n",
      "ð§© Token 4289:  Ratios in the validation set. The\n",
      "choice\n",
      "ð§© Token 4290:  of L = 4 serves as a compromise, balancing\n",
      "ð§© Token 4291:  the trade-oï¬ between maximizing SR\n",
      "ð§© Token 4292:  in both splits and providing\n",
      "a stable and consistent\n",
      "ð§© Token 4293:  holding period length for the strategy.\n",
      "68\n",
      "ð§© Token 4294: Figure A4: Sharpe Ratios in the\n",
      "ð§© Token 4295:  train and validation splits as a function of Î¸\n",
      "ð§© Token 4296:  (LLM)\n",
      "123456789101\n",
      "ð§© Token 4297: 1121314Number of traded clusters (Âµ\n",
      "ð§© Token 4298: )\n",
      "Â°0â¿50ï¿½\n",
      "ð§© Token 4299: ï¿½00â¿51â¿01\n",
      "ð§© Token 4300: â¿52â¿02ï¿½\n",
      "ð§© Token 4301: ï¿½5Sharpe Ratio (Train)\n",
      "Gre\n",
      "ð§© Token 4302: edyStable\n",
      "(a) Plot of SR\n",
      "ð§© Token 4303: Ptr\n",
      "(Î¸) over a grid of\n",
      "ð§© Token 4304:  Î¸\n",
      "12345678910111213\n",
      "ð§© Token 4305: 14Number of traded clusters (Âµ)0\n",
      "ð§© Token 4306: â¿00â¿51ï¿½\n",
      "ð§© Token 4307: ï¿½01â¿52â¿02\n",
      "ð§© Token 4308: â¿53â¿0Sharpe\n",
      "ð§© Token 4309:  Ratio (Validation)\n",
      "GreedyStable\n",
      "ð§© Token 4310: \n",
      "(b) Plot of SRPval\n",
      "\n",
      "ð§© Token 4311: (Î¸) over a grid of Î¸\n",
      "ð§© Token 4312: \n",
      "Note: This ï¬gure illustrates\n",
      "ð§© Token 4313:  the Sharpe Ratios (SR) as a\n",
      "ð§© Token 4314:  function of Î¸, the upper bound on the\n",
      "ð§© Token 4315:  number of traded\n",
      "clusters, for the LL\n",
      "ð§© Token 4316: M clustering method in the training (Panel a\n",
      "ð§© Token 4317: ) and validation (Panel b) splits. In\n",
      "ð§© Token 4318:  Panel (a),\n",
      "the Sharpe Ratios\n",
      "ð§© Token 4319:  for the training set indicate a temporary dip at ï¿½\n",
      "ð§© Token 4320: ï¿½ = 10 for the Greedy algorithm, yet\n",
      "ð§© Token 4321:  this value\n",
      "still provides a relatively stable outcome.\n",
      "ð§© Token 4322:  In contrast, Panel (b) shows that ï¿½\n",
      "ð§© Token 4323: ï¿½ = 10 leads to a noticeable increase in\n",
      "\n",
      "ð§© Token 4324: Sharpe Ratios for the validation set, particularly\n",
      "ð§© Token 4325:  beneï¬ting the Greedy algorithm.\n",
      "ð§© Token 4326:  The choice of Î¸ = â0\n",
      "ð§© Token 4327: .5kâ = 10\n",
      "st\n",
      "ð§© Token 4328: rikes a balance, conï¬rming\n",
      "ð§© Token 4329:  it as an eï¬ective hyperparam\n",
      "ð§© Token 4330: eter selection for achieving stability in both the training\n",
      "\n",
      "ð§© Token 4331: and validation splits with LLM clustering.\n",
      "\n",
      "ð§© Token 4332: 69\n",
      "ð§© Token 4333: Figure A5: Distribution of Cluster-Average Shar\n",
      "ð§© Token 4334: pe Ratios (SRg) by Split\n",
      "\n",
      "ð§© Token 4335: (a) Panel A: KMeans Cl\n",
      "ð§© Token 4336: ustering\n",
      "Â°15Â°10Â°5 0\n",
      "ð§© Token 4337:  5 10 15 20Cluster-Average Sharpe\n",
      "ð§© Token 4338:  Ratio (SRg)0â¿0000\n",
      "ð§© Token 4339: â¿0250â¿0500\n",
      "ð§© Token 4340: â¿0750â¿1000ï¿½\n",
      "ð§© Token 4341: ï¿½ï¿½1250â¿1500ï¿½\n",
      "ð§© Token 4342: ï¿½1750â¿200Density\n",
      "\n",
      "ð§© Token 4343: SplitTrainValidationTest\n",
      "(b) Panel\n",
      "ð§© Token 4344:  B: LLM Clustering\n",
      "Â°20\n",
      "ð§© Token 4345: Â°15Â°10Â°5 0 5 101520\n",
      "ð§© Token 4346: Cluster-Average Sharpe Ratio (SRg\n",
      "ð§© Token 4347: )0â¿0000â¿02\n",
      "ð§© Token 4348: 50â¿0500â¿07\n",
      "ð§© Token 4349: 50â¿1000â¿1250\n",
      "ð§© Token 4350: â¿1500â¿1750ï¿½\n",
      "ð§© Token 4351: ï¿½ï¿½200Density\n",
      "SplitTrainValidation\n",
      "ð§© Token 4352: Test\n",
      "Note: This ï¬gure\n",
      "ð§© Token 4353:  presents the distribution of cluster-average Sharpe Rat\n",
      "ð§© Token 4354: ios (SRg) across training, validation,\n",
      "ð§© Token 4355:  and test data\n",
      "splits for both KMe\n",
      "ð§© Token 4356: ans clustering (Panel A) and LLM\n",
      "ð§© Token 4357:  clustering (Panel B). Each Sharpe Ratio\n",
      "ð§© Token 4358:  is computed as the average\n",
      "of beta-neutral\n",
      "ð§© Token 4359:  positions associated with articles in a given cluster. The\n",
      "ð§© Token 4360:  KMeans approach (Panel A) shows distributions\n",
      "ð§© Token 4361: \n",
      "centered around 0 in the validation set, with\n",
      "ð§© Token 4362:  some outliers exhibiting unusually high or low Sharpe\n",
      "ð§© Token 4363:  Ratios. The training and\n",
      "test set distributions\n",
      "ð§© Token 4364:  are slightly right-skewed, suggesting better\n",
      "ð§© Token 4365:  performance in certain clusters, with no signiï¿½\n",
      "ð§© Token 4366: ï¿½ï¿½cant outliers.\n",
      "In contrast\n",
      "ð§© Token 4367: , the LLM clustering (Panel B)\n",
      "ð§© Token 4368:  exhibits left-skewed distributions across all splits\n",
      "ð§© Token 4369: , indicating a higher frequency\n",
      "of lower Sharpe\n",
      "ð§© Token 4370:  Ratios. The training data shows fat tails,\n",
      "ð§© Token 4371:  suggesting extreme values, while the validation data has lighter\n",
      "ð§© Token 4372: \n",
      "tails. The test data distribution is more bell\n",
      "ð§© Token 4373: -shaped, with Sharpe Ratios concentrated between\n",
      "ð§© Token 4374:  5 and 15, indicating stronger\n",
      "performance in some\n",
      "ð§© Token 4375:  clusters.\n",
      "70\n",
      "ð§© Token 4376: Figure A6: Evolution of Open Positions:\n",
      "ð§© Token 4377:  KMeans vs LLM Clustering\n",
      "\n",
      "ð§© Token 4378: (a) Panel A: KMeans Cl\n",
      "ð§© Token 4379: ustering\n",
      "Jul2020 Sep Nov Jan2021\n",
      "ð§© Token 4380: Mar May Jul Sep\n",
      "Time (trading days\n",
      "ð§© Token 4381: )\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "\n",
      "ð§© Token 4382: 80\n",
      "100# Open Positions\n",
      "Train Val\n",
      "ð§© Token 4383: idationTestGreedyStable\n",
      "(b)\n",
      "ð§© Token 4384:  Panel B: LLM Clustering\n",
      "Jul\n",
      "ð§© Token 4385: 2020 Sep Nov Jan2021Mar May Jul Sep\n",
      "ð§© Token 4386: \n",
      "Time (trading days)\n",
      "0\n",
      "\n",
      "ð§© Token 4387: 20\n",
      "40\n",
      "60\n",
      "80\n",
      "100#\n",
      "ð§© Token 4388:  Open Positions\n",
      "Train ValidationTest Greedy\n",
      "ð§© Token 4389: Stable\n",
      "Note: This ï¬g\n",
      "ð§© Token 4390: ure shows the daily evolution of the number of open\n",
      "ð§© Token 4391:  positions for both Greedy (blue) and St\n",
      "ð§© Token 4392: able (green) algorithms\n",
      "across diï¿½\n",
      "ð§© Token 4393: ï¿½ï¿½erent data splits (Train, Validation\n",
      "ð§© Token 4394: , Test) using KMeans clustering (\n",
      "ð§© Token 4395: Panel A) and LLM clustering (Panel\n",
      "ð§© Token 4396:  B).\n",
      "The time period spans from July 2020\n",
      "ð§© Token 4397:  to September 2021. Vertical dashed lines separate the di\n",
      "ð§© Token 4398: ï¬erent data splits. The\n",
      "Gre\n",
      "ð§© Token 4399: edy algorithm selects clusters that maximize (minimize)\n",
      "ð§© Token 4400:  the cluster-average-SR for long (short\n",
      "ð§© Token 4401: ) positions, while the Stable\n",
      "algorithm\n",
      "ð§© Token 4402:  minimizes the rank diï¬erence between\n",
      "ð§© Token 4403:  training and validation rankings. The number of traded clusters\n",
      "ð§© Token 4404:  is Î¸ =\n",
      "0.5k =\n",
      "ð§© Token 4405:  13 for KMeans (kâ =\n",
      "ð§© Token 4406:  26 clusters) and Î¸ = 0.5\n",
      "ð§© Token 4407: k = 10 for LLM (kâ\n",
      "ð§© Token 4408:  = 20 clusters).71\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(tokens):\n",
    "    print(f\"ð§© Token {i}: {token.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae24e3",
   "metadata": {},
   "source": [
    "Looking at the first document after token-based splitting, we can see that the chunk contains a portion of the document content, and importantly, it preserves the metadata (source and page number) from the original document.\n",
    "\n",
    "This metadata preservation is crucial - it ensures that even after splitting a document into many smaller chunks, we can still trace each chunk back to its source. Let's verify that the metadata matches the original document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "917f7abc",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'macOS Version 15.5 (Build 24F74) Quartz PDFContext',\n",
       " 'creator': 'TexpadTeX CoreGraphicsOutputContext backend: 839',\n",
       " 'creationdate': \"D:20250604103256Z00'00'\",\n",
       " 'moddate': \"D:20250604103256Z00'00'\",\n",
       " 'source': 'docs/paper.pdf',\n",
       " 'total_pages': 72,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d9bfa",
   "metadata": {},
   "source": [
    "## Context aware splitting\n",
    "\n",
    "Beyond preserving existing metadata, sometimes we want to add additional context to our chunks based on the document's structure. This is where context-aware splitting becomes valuable.\n",
    "\n",
    "The goal of chunking is to keep semantically related text together. While basic text splitters use delimiters like newlines or spaces, many documents (such as Markdown) have explicit structure through headers that can be leveraged for smarter splitting.\n",
    "\n",
    "`MarkdownHeaderTextSplitter` not only splits text based on headers but also adds those headers as metadata to each chunk. This is particularly useful because:\n",
    "\n",
    "1. Headers provide critical context about the content's topic\n",
    "2. They establish hierarchical relationships between chunks\n",
    "3. This metadata can be used later for more targeted retrieval\n",
    "\n",
    "Let's see how this works with a sample Markdown document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa3b93d9",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2c73a9c",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "markdown_document = \"\"\"# Title\\n\\n \\\n",
    "## Chapter 1\\n\\n \\\n",
    "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
    "### Section \\n\\n \\\n",
    "Hi this is Lance \\n\\n \n",
    "## Chapter 2\\n\\n \\\n",
    "Hi this is Molly\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eadb7740",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a789eede",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3cb9b",
   "metadata": {},
   "source": [
    "The `MarkdownHeaderTextSplitter` requires us to define which headers to look for and what metadata field names to use for them. In this case, we're identifying three levels of headers:\n",
    "- `#` (Header 1): Top-level headers\n",
    "- `##` (Header 2): Second-level headers \n",
    "- `###` (Header 3): Third-level headers\n",
    "\n",
    "This allows the splitter to recognize the hierarchical structure of the document and preserve that structure in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27053c17",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Markdown Header 0: Hi this is Jim  \n",
      "Hi this is Joe\n",
      "\n",
      " Markdown Header 1: Hi this is Lance\n",
      "\n",
      " Markdown Header 2: Hi this is Molly\n"
     ]
    }
   ],
   "source": [
    "for i, md_header in enumerate(md_header_splits):\n",
    "    print(f\"\\n Markdown Header {i}: {md_header.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7044ae19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1'}, page_content='Hi this is Jim  \\nHi this is Joe')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f007e85",
   "metadata": {},
   "source": [
    "Looking at the first split, we can see it contains the content \"Hi this is Jim\" and \"Hi this is Joe\" from the Chapter 1 section. Most importantly, look at the metadata - it contains:\n",
    "\n",
    "- `Header 1: \"Title\"` - This comes from the top-level header\n",
    "- `Header 2: \"Chapter 1\"` - This comes from the second-level header\n",
    "\n",
    "This metadata provides crucial context about where this content appears in the document hierarchy, which can be extremely valuable when retrieving information later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "css-datascience-2025-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
