{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38758df9",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jesusvillota/CSS_DataScience_2025/blob/main/Session3/3_2_RAG_V_Full_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeabd5b4",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 880px; margin: 20px auto 22px; padding: 0px; border-radius: 18px; border: 1px solid #e5e7eb; background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%); box-shadow: 0 8px 26px rgba(0,0,0,0.06); overflow: hidden;\">\n",
    "\n",
    "  <!-- Banner Header -->\n",
    "  <div style=\"padding: 34px 32px 14px; text-align: center; line-height: 1.38;\">\n",
    "    <div style=\"font-size: 13px; letter-spacing: 0.14em; text-transform: uppercase; color: #6b7280; font-weight: bold; margin-bottom: 5px;\">\n",
    "      Session #2\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      RAG with LangChain\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      Part V: Chat with your data\n",
    "    </div>\n",
    "    <div style=\"font-size: 16.5px; color: #374151; font-style: italic; margin-bottom: 0;\">\n",
    "      Data Science for Economics: Mastering Unstructured Data\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Logo Section -->\n",
    "  <div style=\"background: none; text-align: center; margin: 30px 0 10px;\">\n",
    "    <img src=\"https://www.cemfi.es/images/Logo-Azul.png\" alt=\"CEMFI Logo\" style=\"width: 158px; filter: drop-shadow(0 2px 12px rgba(56,84,156,0.05)); margin-bottom: 0;\">\n",
    "  </div>\n",
    "\n",
    "  <!-- Name -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1.22em; font-weight: bold; margin-bottom: 0px;\">\n",
    "    Jesus Villota Miranda ¬© 2025\n",
    "  </div>\n",
    "\n",
    "  <!-- Contact info -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1em; margin-top: 7px; margin-bottom: 20px;\">\n",
    "    <a href=\"mailto:jesus.villota@cemfi.edu.es\" style=\"color: #38549c; text-decoration: none; margin-right:8px;\" title=\"Email\">\n",
    "      <!-- Email logo -->\n",
    "      <!-- <img src=\"https://cdn-icons-png.flaticon.com/512/11679/11679732.png\" alt=\"Email\" style=\"width:18px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      jesus.villota@cemfi.edu.es\n",
    "    </a>\n",
    "    <span style=\"color:#9fa7bd;\">|</span>\n",
    "    <a href=\"https://www.linkedin.com/in/jesusvillotamiranda/\" target=\"_blank\" style=\"color: #38549c; text-decoration: none; margin-left:7px;\" title=\"LinkedIn\">\n",
    "      <!-- LinkedIn logo -->\n",
    "      <!-- <img src=\"https://1.bp.blogspot.com/-onvhHUdW1Us/YI52e9j4eKI/AAAAAAAAE4c/6s9wzOpIDYcAo4YmTX1Qg51OlwMFmilFACLcBGAsYHQ/s1600/Logo%2BLinkedin.png\" alt=\"LinkedIn\" style=\"width:17px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      LinkedIn\n",
    "    </a>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3862c",
   "metadata": {},
   "source": [
    "**IMPORTANT**: **Are you running this notebook in Google Colab?**\n",
    "\n",
    "- If so, please make sure that in the cell below `running_in_colab` is set to `True`\n",
    "\n",
    "- And, of course,  make sure to **run the cell**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d3974b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_in_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running_in_colab: \n",
    "    ! pip install langchain_huggingface openai pypdf\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    folder_dir = '/content/drive/My Drive/docs/'\n",
    "else: \n",
    "    folder_dir = 'docs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef480fe",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Recall the overall workflow for retrieval augmented generation (RAG):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa2dab",
   "metadata": {},
   "source": [
    "![](images/rag_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8825752",
   "metadata": {},
   "source": [
    "The final step in RAG is to merge the retrieved documents and the original query to produce a final answer. \n",
    "This process is intermediated by an LLM, which sees both your prompt and the retrieved documents as context and then generates an informed response.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/RAG.png\" alt=\"RAG Final Step\" width=\"320\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c9e7e",
   "metadata": {},
   "source": [
    "In this notebook we will complete the full RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec929ccd",
   "metadata": {},
   "source": [
    "# **RAG Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0779ed6a",
   "metadata": {},
   "source": [
    "1) **Document Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d849626",
   "metadata": {},
   "source": [
    "- Let's load some example PDFs. \n",
    "- For this illustration, we will use the transcripts from the first three lectures of the CS229 Machine Learning course (Stanford).\n",
    "- https://see.stanford.edu/Course/CS229\n",
    "- Make sure to download the pdfs to your Drive to be able to load the documents (I uploaded them to `Session3/docs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b997cff1",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    PyPDFLoader(folder_dir + \"MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(folder_dir + \"MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(folder_dir + \"MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4743c4",
   "metadata": {},
   "source": [
    "2) **Splitting**\n",
    "\n",
    "We use the RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "73225ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cce27e",
   "metadata": {},
   "source": [
    "3) **Embeddings**\n",
    "\n",
    "- Here I give you the option to do it with a free open-source model from HuggingFace, or with the more sophisticated OpenAI embeddings.\n",
    "- Note that, to use the OpenAI embeddings, you need need an OPENAI_API_KEY and credit in your OpenAI account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "use_free_embeddings = False\n",
    "\n",
    "if use_free_embeddings:\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "else:\n",
    "    # You need an OPENAI_API_KEY and credit in your OpenAI account\n",
    "    embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106b060",
   "metadata": {},
   "source": [
    "4) **Vectorstore**\n",
    "- As we saw in the previous notebook, we can store our embeddings in a vectorstore \n",
    "- We use Chroma, but there are other alternatives you can explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a9977d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "persist_directory = 'chroma_db/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7524d36",
   "metadata": {},
   "source": [
    "5) **Define the user question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ac6058bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is probability a class topic?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cbba3",
   "metadata": {},
   "source": [
    "6) **Context retrieval**\n",
    "\n",
    "- This is the context that is retrieved when we call `retriever=vectordb.as_retriever()` on the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4fc56058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================[ üìÑ Relevant Chunk 1 ]========================================\n",
      "statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \n",
      "refresher for those of you that want one.  \n",
      "Later in this quarter, we'll also use the discussion sections to go over extensions for the \n",
      "material that I'm teaching in the main lectures. So machine learning is a huge field, and \n",
      "there are a few extensions that we really want to teach but didn't have time in the main \n",
      "lectures for.\n",
      "\n",
      "========================================[ üìÑ Relevant Chunk 2 ]========================================\n",
      "of this class will not be very programming intensive, although we will do some \n",
      "programming, mostly in either MATLAB or Octave. I'll say a bit more about that later.  \n",
      "I also assume familiarity with basic probability and statistics. So most undergraduate \n",
      "statistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \n",
      "assume all of you know what random variables are, that all of you know what expectation \n",
      "is, what a variance or a random variable is. And in case of some of you, it's been a while \n",
      "since you've seen some of this material. At some of the discussion sections, we'll actually \n",
      "go over some of the prerequisites, sort of as a refresher course under prerequisite class. \n",
      "I'll say a bit more about that later as well.  \n",
      "Lastly, I also assume familiarity with basic linear algebra. And again, most undergraduate \n",
      "linear algebra courses are more than enough. So if you've taken courses like Math 51, \n",
      "103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \n",
      "gonna assume that all of you know what matrixes and vectors are, that you know how to \n",
      "multiply matrices and vectors and multiply matrix and matrices, that you know what a \n",
      "matrix inverse is. If you know what an eigenvector of a matrix is, that'd be even better. \n",
      "But if you don't quite know or if you're not quite sure, that's fine, too. We'll go over it in \n",
      "the review sections.\n",
      "\n",
      "========================================[ üìÑ Relevant Chunk 3 ]========================================\n",
      "So, for example, what a learning algorithm may do is maybe come in and decide that a \n",
      "straight line like that separates the two classes of tumors really well, and so if you have a \n",
      "new patient who's age and tumor size fall over there, then the algorithm may predict that \n",
      "the tumor is benign rather than malignant, okay? So this is just another example of \n",
      "another supervised learning problem and another classification problem.  \n",
      "And so it turns out that one of the issues we'll talk about later in this class is in this \n",
      "specific example, we're going to try to predict whether a tumor is malignant or benign \n",
      "based on two features or based on two inputs, namely the age of the patient and the tumor \n",
      "size. It turns out that when you look at a real data set, you find that learning algorithms \n",
      "often use other sets of features. In the breast cancer data example, you also use properties \n",
      "of the tumors, like clump thickness, uniformity of cell size, uniformity of cell shape, \n",
      "[inaudible] adhesion and so on, so various other medical properties.  \n",
      "And one of the most interesting things we'll talk about later this quarter is what if your \n",
      "data doesn't lie in a two-dimensional or three-dimensional or sort of even a finite \n",
      "dimensional space, but is it possible ‚Äî what if your data actually lies in an infinite \n",
      "dimensional space? Our plots here are two-dimensional space. I can't plot you an infinite \n",
      "dimensional space, right? And so it turns out that one of the most successful classes of\n",
      "\n",
      "========================================[ üìÑ Relevant Chunk 4 ]========================================\n",
      "assume what we‚Äôre trying to prove. Instructor? \n",
      "That‚Äôs the [inaudible] but, yes. You are assuming that the error has zero mean. Which is, \n",
      "yeah, right. I think later this quarter we get to some of the other things, but for now just \n",
      "think of this as a mathematically ‚Äì it‚Äôs actually not an unreasonable assumption. I guess, \n",
      "in machine learning all the assumptions we make are almost never true in the absence \n",
      "sense, right? Because, for instance, housing prices are priced to dollars and cents, so the \n",
      "error will be ‚Äì errors in prices are not continued as value random variables, because \n",
      "houses can only be priced at a certain number of dollars and a certain number of cents \n",
      "and you never have fractions of cents in housing prices. Whereas a Gaussian random\n",
      "\n",
      "========================================[ üìÑ Relevant Chunk 5 ]========================================\n",
      "variable would. So in that sense, assumptions we make are never ‚Äúabsolutely true,‚Äù but \n",
      "for practical purposes this is a accurate enough assumption that it‚Äôll be useful to make. \n",
      "Okay? I think in a week or two, we‚Äôll actually come back to selected more about the \n",
      "assumptions we make and when they help our learning algorithms and when they hurt our \n",
      "learning algorithms. We‚Äôll say a bit more about it when we talk about generative and \n",
      "discriminative learning algorithms, like, in a week or two. Okay?  \n",
      "So let‚Äôs point out one bit of notation, which is that when I wrote this down I actually \n",
      "wrote P of YI given XI and then semicolon theta and I‚Äôm going to use this notation when \n",
      "we are not thinking of theta as a random variable. So in statistics, though, sometimes it‚Äôs \n",
      "called the frequentist‚Äôs point of view, where you think of there as being some, sort of, \n",
      "true value of theta that‚Äôs out there that‚Äôs generating the data say, but we don‚Äôt know what \n",
      "theta is, but theta is not a random vehicle, right? So it‚Äôs not like there‚Äôs some random \n",
      "value of theta out there. It‚Äôs that theta is ‚Äì there‚Äôs some true value of theta out there. It‚Äôs \n",
      "just that we don‚Äôt know what the true value of theta is. So if theta is not a random \n",
      "variable, then I‚Äôm going to avoid writing P of YI given XI comma theta, because this \n",
      "would mean that probably of YI conditioned on X and theta and you can only condition \n",
      "on random variables. So at this part of the class where we‚Äôre taking sort of frequentist‚Äôs\n"
     ]
    }
   ],
   "source": [
    "docs = vectordb.similarity_search(question,k=5)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"\\n\" + \"=\"*40 + f\"[ üìÑ Relevant Chunk {i+1} ]\" + \"=\"*40)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0c13b",
   "metadata": {},
   "source": [
    "7) **Pass the template and context to the LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2cc6895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running_in_colab:\n",
    "    from google.colab import secrets\n",
    "    api_key = secrets[\"OPENAI_API_KEY\"]\n",
    "else:\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a11701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2b37519f",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# Build prompt\n",
    "template = \"\"\"\n",
    "            Use the following pieces of context to answer the question at the end. \\\n",
    "            If you don't know the answer, just say that you don't know, don't try to make up an answer. \\\n",
    "            Use three sentences maximum. Keep the answer as concise as possible. \\\n",
    "            Always say \"thanks for asking!\" at the end of the answer. \\\n",
    "            Context: {context} \\\n",
    "            Question: {question} \\\n",
    "            Helpful Answer:\n",
    "            \"\"\"\n",
    "            \n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fbe0f",
   "metadata": {},
   "source": [
    "8) **Build the QA prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fcb5817c",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813803e",
   "metadata": {},
   "source": [
    "9) **Build the RetrievalQA chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f3a21b52",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872efa9",
   "metadata": {},
   "source": [
    "10) **Run the chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9ab4c517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Is probability a class topic?',\n",
       " 'result': 'Probability is assumed to be a prerequisite for this class, so it will not be a main topic covered in lectures. Thanks for asking!',\n",
       " 'source_documents': [Document(metadata={'creator': 'PScript5.dll Version 5.2.2', 'source': 'docs/MachineLearning-Lecture01.pdf', 'title': '', 'page_label': '9', 'page': 8, 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'total_pages': 22, 'author': '', 'creationdate': '2008-07-11T11:25:23-07:00', 'moddate': '2008-07-11T11:25:23-07:00'}, page_content=\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the discussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectures. So machine learning is a huge field, and \\nthere are a few extensions that we really want to teach but didn't have time in the main \\nlectures for.\"),\n",
       "  Document(metadata={'title': '', 'total_pages': 22, 'author': '', 'creationdate': '2008-07-11T11:25:23-07:00', 'page': 4, 'page_label': '5', 'moddate': '2008-07-11T11:25:23-07:00', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'PScript5.dll Version 5.2.2', 'source': 'docs/MachineLearning-Lecture01.pdf'}, page_content=\"of this class will not be very programming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octave. I'll say a bit more about that later.  \\nI also assume familiarity with basic probability and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what random variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basic linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrixes and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a \\nmatrix inverse is. If you know what an eigenvector of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not quite sure, that's fine, too. We'll go over it in \\nthe review sections.\"),\n",
       "  Document(metadata={'creator': 'PScript5.dll Version 5.2.2', 'page': 13, 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'title': '', 'source': 'docs/MachineLearning-Lecture01.pdf', 'moddate': '2008-07-11T11:25:23-07:00', 'author': '', 'creationdate': '2008-07-11T11:25:23-07:00', 'page_label': '14', 'total_pages': 22}, page_content=\"So, for example, what a learning algorithm may do is maybe come in and decide that a \\nstraight line like that separates the two classes of tumors really well, and so if you have a \\nnew patient who's age and tumor size fall over there, then the algorithm may predict that \\nthe tumor is benign rather than malignant, okay? So this is just another example of \\nanother supervised learning problem and another classification problem.  \\nAnd so it turns out that one of the issues we'll talk about later in this class is in this \\nspecific example, we're going to try to predict whether a tumor is malignant or benign \\nbased on two features or based on two inputs, namely the age of the patient and the tumor \\nsize. It turns out that when you look at a real data set, you find that learning algorithms \\noften use other sets of features. In the breast cancer data example, you also use properties \\nof the tumors, like clump thickness, uniformity of cell size, uniformity of cell shape, \\n[inaudible] adhesion and so on, so various other medical properties.  \\nAnd one of the most interesting things we'll talk about later this quarter is what if your \\ndata doesn't lie in a two-dimensional or three-dimensional or sort of even a finite \\ndimensional space, but is it possible ‚Äî what if your data actually lies in an infinite \\ndimensional space? Our plots here are two-dimensional space. I can't plot you an infinite \\ndimensional space, right? And so it turns out that one of the most successful classes of\"),\n",
       "  Document(metadata={'page': 7, 'moddate': '2008-07-11T11:25:03-07:00', 'author': '', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'title': '', 'source': 'docs/MachineLearning-Lecture03.pdf', 'page_label': '8', 'total_pages': 16, 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': '2008-07-11T11:25:03-07:00'}, page_content='assume what we‚Äôre trying to prove. Instructor? \\nThat‚Äôs the [inaudible] but, yes. You are assuming that the error has zero mean. Which is, \\nyeah, right. I think later this quarter we get to some of the other things, but for now just \\nthink of this as a mathematically ‚Äì it‚Äôs actually not an unreasonable assumption. I guess, \\nin machine learning all the assumptions we make are almost never true in the absence \\nsense, right? Because, for instance, housing prices are priced to dollars and cents, so the \\nerror will be ‚Äì errors in prices are not continued as value random variables, because \\nhouses can only be priced at a certain number of dollars and a certain number of cents \\nand you never have fractions of cents in housing prices. Whereas a Gaussian random')]}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "74e2f6cc",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Probability is assumed to be a prerequisite for this class, so it will not be a main topic covered in lectures. Thanks for asking!'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4a2531ba",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'creator': 'PScript5.dll Version 5.2.2', 'source': 'docs/MachineLearning-Lecture01.pdf', 'title': '', 'page_label': '9', 'page': 8, 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'total_pages': 22, 'author': '', 'creationdate': '2008-07-11T11:25:23-07:00', 'moddate': '2008-07-11T11:25:23-07:00'}, page_content=\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the discussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectures. So machine learning is a huge field, and \\nthere are a few extensions that we really want to teach but didn't have time in the main \\nlectures for.\")"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_course_cemfi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
