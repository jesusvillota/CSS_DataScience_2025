{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60450c7",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 880px; margin: 20px auto 22px; padding: 0px; border-radius: 18px; border: 1px solid #e5e7eb; background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%); box-shadow: 0 8px 26px rgba(0,0,0,0.06); overflow: hidden;\">\n",
    "\n",
    "  <!-- Banner Header -->\n",
    "  <div style=\"padding: 34px 32px 14px; text-align: center; line-height: 1.38;\">\n",
    "    <div style=\"font-size: 13px; letter-spacing: 0.14em; text-transform: uppercase; color: #6b7280; font-weight: bold; margin-bottom: 5px;\">\n",
    "      Session #2\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      LLMs (Part I)\n",
    "    </div>\n",
    "    <div style=\"font-size: 26px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      Part I: Downloading LLMs locally\n",
    "    </div>\n",
    "    <div style=\"font-size: 16.5px; color: #374151; font-style: italic; margin-bottom: 0;\">\n",
    "      Data Science for Economics: Mastering Unstructured Data\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Logo Section -->\n",
    "  <div style=\"background: none; text-align: center; margin: 30px 0 10px;\">\n",
    "    <img src=\"https://www.cemfi.es/images/Logo-Azul.png\" alt=\"CEMFI Logo\" style=\"width: 158px; filter: drop-shadow(0 2px 12px rgba(56,84,156,0.05)); margin-bottom: 0;\">\n",
    "  </div>\n",
    "\n",
    "  <!-- Name -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1.22em; font-weight: bold; margin-bottom: 0px;\">\n",
    "    Jesus Villota Miranda © 2025\n",
    "  </div>\n",
    "\n",
    "  <!-- Contact info -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1em; margin-top: 7px; margin-bottom: 20px;\">\n",
    "    <a href=\"mailto:jesus.villota@cemfi.edu.es\" style=\"color: #38549c; text-decoration: none; margin-right:8px;\" title=\"Email\">\n",
    "      <img src=\"https://cdn-icons-png.flaticon.com/512/11679/11679732.png\" alt=\"Email\" style=\"width:18px; vertical-align:middle; margin-right:5px;\">\n",
    "      jesus.villota@cemfi.edu.es\n",
    "    </a>\n",
    "    <span style=\"color:#9fa7bd;\">|</span>\n",
    "    <a href=\"https://www.linkedin.com/in/jesusvillotamiranda/\" target=\"_blank\" style=\"color: #38549c; text-decoration: none; margin-left:7px;\" title=\"LinkedIn\">\n",
    "      <img src=\"https://1.bp.blogspot.com/-onvhHUdW1Us/YI52e9j4eKI/AAAAAAAAE4c/6s9wzOpIDYcAo4YmTX1Qg51OlwMFmilFACLcBGAsYHQ/s1600/Logo%2BLinkedin.png\" alt=\"LinkedIn\" style=\"width:17px; vertical-align:middle; margin-right:5px;\">\n",
    "      LinkedIn\n",
    "    </a>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f819f6",
   "metadata": {},
   "source": [
    "<div style=\"height:10px; background: linear-gradient(90deg,#dbeafe, #f5f3ff); border-radius: 8px; margin: 6px 0 14px;\"></div>\n",
    "## Why run LLMs locally?\n",
    "\n",
    "> Running small Large Language Models (LLMs) on your own machine is great for:\n",
    "> - Privacy: your data stays on device.\n",
    "> - Cost control: no API bills while experimenting.\n",
    "> - Offline/edge use: works without internet.\n",
    "> - Reproducibility & customization: full control over versions and files.\n",
    "\n",
    "> In this short, hands-on lesson you will:\n",
    "> - Download a compact, CPU-friendly model: Google’s `gemma-3-270m` from Hugging Face.\n",
    "> - Locate the downloaded files on disk using Terminal and Finder.\n",
    "> - Understand the typical files in a model repository and what they do.\n",
    "\n",
    "Hugging Face model card: https://huggingface.co/google/gemma-3-270m\n",
    "\n",
    "### Prerequisites\n",
    "> - Python installed (3.9+ recommended) and a working internet connection.\n",
    "> - The package `huggingface_hub` installed. If not, install from Terminal:\n",
    ">   - macOS/Linux: `pip install -U huggingface_hub`\n",
    "> - A few hundred MB of free disk space.\n",
    "> - Optional but recommended: a free Hugging Face account and CLI login (`huggingface-cli login`) if a model requires access/terms acceptance.\n",
    "\n",
    "> Note: `gemma-3-270m` is intentionally small so it downloads fast and runs on typical laptops. It’s ideal for learning how local LLM tooling works. Some people call these type of \"small\" models Small Language Models (SLMs) as opposed to the typical monsters with billions of parameters, which are often referred to as Large Language Models (LLMs).\n",
    "<div style=\"height:8px; background: linear-gradient(90deg,#f5f3ff, #dbeafe); border-radius: 8px; margin: 14px 0 6px;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9553266",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_in_colab = False\n",
    "\n",
    "if running_in_colab:\n",
    "    ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2add6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model = False\n",
    "\n",
    "if download_model:\n",
    "    # Download a small, CPU-friendly model locally using Hugging Face Hub\n",
    "    # Note: Some models require accepting terms or logging in via `huggingface-cli login`.\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    model_id = \"google/gemma-3-270m\"\n",
    "\n",
    "    # This downloads the full model repo to a local cache folder and returns the path\n",
    "    local_dir = snapshot_download(model_id)\n",
    "\n",
    "    print(f\"Model downloaded at: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69918f5f",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px 14px; border: 1px solid #e5e7eb; border-radius: 12px; background: linear-gradient(180deg,#ffffff, #f9fafb); box-shadow: 0 4px 16px rgba(0,0,0,0.04);\">\n",
    "<div style=\"font-size:20px; font-weight:800; color:#14276c; margin:0 0 6px;\">DeepSeek‑R1‑Distill‑Qwen‑1.5B — distilled, in plain English</div>\n",
    "\n",
    " \n",
    "\n",
    "### What is model distillation?\n",
    "\n",
    "- A large “teacher” model trains a smaller “student” model to mimic its outputs.\n",
    "\n",
    "- The student keeps most of the teacher’s skill but is lighter, faster, and cheaper to run.\n",
    "\n",
    "- Why it matters: near‑teacher quality without the hardware and cost of huge models.\n",
    "\n",
    " \n",
    "\n",
    "### How it applies here\n",
    "\n",
    "- DeepSeek‑R1‑Distill‑Qwen‑1.5B is a 1.5B‑parameter student distilled from DeepSeek‑R1’s reasoning models, built on the Qwen‑2.5 family.\n",
    "\n",
    "- You get R1‑style reasoning patterns (step‑by‑step, self‑checking) in a compact model that runs on typical laptops.\n",
    "\n",
    " \n",
    "\n",
    "### The only details you really need\n",
    "\n",
    "- Size: ~1.5B params → practical for local demos, teaching, and prototypes.\n",
    "\n",
    "- Context: long context support helps with longer prompts/doc chunks.\n",
    "\n",
    "- Usage: chat‑style prompting; ask it to “think step by step” for math/logic tasks.\n",
    "\n",
    "- Sampling: temperature ~0.5–0.7 often yields clearer, less repetitive answers.\n",
    "\n",
    "- License: permissive for classroom and projects (still review the model card).\n",
    "\n",
    " \n",
    "\n",
    "### When to use it\n",
    "\n",
    "- On‑device experiments, classroom exercises, quick reasoning prototypes, or anytime big models are overkill.\n",
    "\n",
    " \n",
    "\n",
    "### TL;DR\n",
    "\n",
    "Distillation compresses a strong teacher into a small student that keeps most of the capability. DeepSeek‑R1‑Distill‑Qwen‑1.5B brings R1‑style reasoning to a laptop‑friendly model.\n",
    "\n",
    " \n",
    "\n",
    "Below you’ll download it from the Hub and locate it on disk for local use.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef5481a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024ffbd0650e402bbc28fa77bca7dfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce21da9ab144449a5c208d1f1673c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39324c7e4a7f4e2ca885549e4a43ccb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32adc0714f034785a145faa6f849ecc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc2a684dd5948059932ae43b5d87efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18436b682814a12826e3caef3d0b779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1560049aef334b8ab36d1c715a6547ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d089e56bb347228cc295973788fd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "benchmark.jpg:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a182505caf7f4dd690155f660d6b3bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac04193d00aa4bf188f36a9c4228211f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded at: /Users/jesusvillotamiranda/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562\n"
     ]
    }
   ],
   "source": [
    "download_model = False\n",
    "\n",
    "if download_model:\n",
    "    from huggingface_hub import snapshot_download\n",
    "    model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    local_dir = snapshot_download(model_id)\n",
    "    print(f\"Model downloaded at: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b320e",
   "metadata": {},
   "source": [
    "<div style=\"height:10px; background: linear-gradient(90deg,#fef9c3,#dbeafe); border-radius: 8px; margin: 8px 0 14px;\"></div>\n",
    "## Where did the model download to?\n",
    "\n",
    "The function `snapshot_download` saves the full Hugging Face repository to your local cache. On macOS the default path is inside your user cache directory, for example:\n",
    "\n",
    "> /Users/<**your-username**>/.cache/huggingface/hub/models--<**model**>/snapshots/<**commit-hash**>\n",
    "\n",
    "In my case:\n",
    "\n",
    "> /Users/jesusvillotamiranda/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/9b0cfec892e2bc2afd938c98eabe4e4a7b1e0ca1\n",
    ">\n",
    "> /Users/jesusvillotamiranda/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562\n",
    "\n",
    "> **Important: Your exact path will be printed by the previous code cell. Use that path in the steps below.**\n",
    "\n",
    "### Option A: Use Terminal (macOS/Linux)\n",
    "1. Open Terminal.\n",
    "2. Change directory to your printed path. Example:\n",
    "   ```bash\n",
    "   cd /Users/yourname/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/<commit-hash>\n",
    "   ```\n",
    "3. List files with sizes:\n",
    "   ```bash\n",
    "   ls -lh\n",
    "   ```\n",
    "\n",
    "Tip: press Tab to auto-complete long paths. If you get “No such file or directory,” double-check spaces and the exact hash.\n",
    "\n",
    "### Option B: Use Finder (macOS) or File Explorer (Windows)\n",
    "1. In Finder, press Cmd + Shift + G (Go to Folder...).\n",
    "2. Paste the path you saw printed (the snapshot folder).\n",
    "3. Press Enter to open the folder with the downloaded model files.\n",
    "\n",
    "Below are screenshots showing the Hugging Face cache structure and the model folder contents.\n",
    "<div style=\"height:8px; background: linear-gradient(90deg,#dbeafe,#fef9c3); border-radius: 8px; margin: 14px 0 6px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064fdb4",
   "metadata": {},
   "source": [
    "### Exploring your local Hugging Face cache\n",
    "\n",
    "In your Hugging Face cache (`~/.cache/huggingface/hub`), you’ll find all models you’ve downloaded. For example:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/finder_1.png\" alt=\"Finder view 1\" style=\"max-width:860px; width:100%; border: 1px solid #e5e7eb; border-radius: 10px; box-shadow: 0 6px 18px rgba(0,0,0,0.06);\"/>\n",
    "</div>\n",
    "\n",
    "Now, drilling down into the `gemma-3-270m` snapshot folder, you’ll see the actual model artifacts:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/finder_2.png\" alt=\"Finder view 2\" style=\"max-width:860px; width:100%; border: 1px solid #e5e7eb; border-radius: 10px; box-shadow: 0 6px 18px rgba(0,0,0,0.06);\"/>\n",
    "</div>\n",
    "\n",
    "Common files you’ll encounter in model repos:\n",
    "| File Name                       | Purpose/Description                                                                                      |\n",
    "|----------------------------------|---------------------------------------------------------------------------------------------------------|\n",
    "| `config.json`                    | Model architecture and hyperparameters used by libraries like Transformers.                             |\n",
    "| `tokenizer.json` / `tokenizer.model` / `tokenizer_config.json` | Vocabulary and rules for turning text into tokens.                                 |\n",
    "| `model.safetensors` or `pytorch_model.bin` | The neural network weights. `safetensors` is preferred for safety and speed.         |\n",
    "| `generation_config.json`         | Default generation parameters (temperature, max_new_tokens, etc.) used by convenience APIs.             |\n",
    "| `README.md` and license files    | Model card and licensing terms. Always review licensing before redistribution or commercial use.         |\n",
    "\n",
    "These files together are what frameworks load to run the model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99ca7c",
   "metadata": {},
   "source": [
    "<div style=\"padding: 12px 14px; background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 10px;\">\n",
    "## A quick mental model: how all the pieces fit\n",
    "\n",
    "- Hugging Face Hub is like Git for models: each model repo has versions (commits) and files.\n",
    "\n",
    "- `snapshot_download` fetches a specific snapshot (commit) to your local cache.\n",
    "\n",
    "- Loading a model later only needs the local path; no re-download unless you change versions.\n",
    "\n",
    "- `tokenizer` turns text into tokens; the model turns tokens into the next-token probabilities; a generation loop produces text.\n",
    "\n",
    "- `config` and `generation_config` tell libraries how to reconstruct the model and generate text sensibly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d27487",
   "metadata": {},
   "source": [
    "<div style=\"padding: 12px 14px; border: 1px solid #e5e7eb; border-radius: 12px; background: linear-gradient(180deg,#ffffff, #f8fafc); box-shadow: 0 4px 16px rgba(0,0,0,0.04);\">\n",
    "## Optional: Run the model with Transformers (CPU-friendly)\n",
    "\n",
    "Once downloaded, you can load the model from the local path using `transformers`.\n",
    "\n",
    "Each model has its own way of interacting with transformers. For example, some models may require specific input formats or additional preprocessing steps. Always refer to the model's documentation for details on how to use it effectively.\n",
    "\n",
    "1) Go to the model page, and click on \"Use this model\"\n",
    "<div align=\"center\"><img src=\"images/model_page.png\" alt=\"Model page\" style=\"max-width:860px; width:100%; border: 1px solid #e5e7eb; border-radius: 10px; box-shadow: 0 6px 18px rgba(0,0,0,0.06);\"/></div>\n",
    "\n",
    "2) Follow the instructions provided on the model page to integrate it into your application.\n",
    "<div align=\"center\"><img src=\"images/pipeline.png\" alt=\"Pipeline\" style=\"max-width:860px; width:100%; border: 1px solid #e5e7eb; border-radius: 10px; box-shadow: 0 6px 18px rgba(0,0,0,0.06);\"/></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Set parameters for comparison --\n",
    "PROMPT = \"What do economists do?\"\n",
    "MAX_TOKENS = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "527ca4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do economists do? They study economics and the principles of economics and how they affect and make decisions in business, politics and government.Economics is the study of human behavior and value in the natural world.\n",
      "\n",
      "The way that human behavior works. Human beings will choose a business and will want a profit from it. A business may have a bad idea and will make a few mistakes while trying to be profitable. It may decide to pay a low price for a good and then make a profit. It also decides to make a bad decision in the future and then makes a wrong choice.\n",
      "\n",
      "Economics will help us understand human behavior and how to make better decisions. \n",
      "\n",
      "What are the reasons for different economic problems?\n",
      "\n",
      "The way that economic problems have different causes. Economic problems are most likely due to economic factors such as inflation, recessions and wars.\n",
      "\n",
      "Causes of economic problems:\n",
      "\n",
      "Inflation: inflation is when the value of money in a country increases. This will impact the purchasing power of money and the value of goods and services.\n",
      "\n",
      "Deflation: this is when the value of money is not affected by price changes.\n",
      "\n",
      "Deflation is a result of government intervention and the financial institutions that they work with.\n",
      "\n",
      "The consequences of bad decisions for the economy:\n",
      "\n",
      "A decrease in the economy: when businesses make mistakes by creating bad decisions or making poor decisions that would affect their profits.\n",
      "\n",
      "A decrease in production: when businesses make mistakes by not considering the impacts of bad decisions that affect the business.\n",
      "\n",
      "Economic decisions should be fair and right: businesses should make informed decisions that benefit their companies and the community.\n",
      "\n",
      "Some of the basic rules of economic decision-making are:\n",
      "\n",
      "Reasoning: people must have a good reason for making a decision.\n",
      "\n",
      "Making decisions that will benefit the company or business: people must have good reasons to make a decision.\n",
      "\n",
      "Making decisions based on logic and facts: people must be able to understand facts that prove them wrong.\n",
      "\n",
      "The best decision\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gemma_model_path = \"/Users/jesusvillotamiranda/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/9b0cfec892e2bc2afd938c98eabe4e4a7b1e0ca1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(gemma_model_path, trust_remote_code=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=MAX_TOKENS)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a086310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out what economists do. I'm not super familiar with economics, but I know it's a field that deals with how people, businesses, governments, and institutions make decisions regarding the allocation of resources. I remember hearing terms like supply and demand, supply-side economics, and maybe even some concepts related to policy making. Let me try to break this down.\n",
      "\n",
      "First, I think economists study how markets work. Markets are places where people buy and sell goods and services. They look at things like prices, quantities, and how decisions are made in these markets. Maybe they look at supply, which is how much a good can be produced, and demand, which is how much people want to buy at a certain price. I've heard terms like equilibrium, which is where supply meets demand, and maybe there are things like surpluses or deficits when there's a mismatch between what's produced and what's demanded.\n",
      "\n",
      "I also remember hearing about different types of economics, like\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "deepseek_model = \"/Users/jesusvillotamiranda/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(deepseek_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(deepseek_model)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": PROMPT},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=MAX_TOKENS)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
