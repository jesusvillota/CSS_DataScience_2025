{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesusvillota/CSS_DataScience_2025/blob/main/Session3/3_5_LLM_I_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 880px; margin: 20px auto 22px; padding: 0px; border-radius: 18px; border: 1px solid #e5e7eb; background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%); box-shadow: 0 8px 26px rgba(0,0,0,0.06); overflow: hidden;\">\n",
        "\n",
        "  <!-- Banner Header -->\n",
        "  <div style=\"padding: 34px 32px 14px; text-align: center; line-height: 1.38;\">\n",
        "    <div style=\"font-size: 13px; letter-spacing: 0.14em; text-transform: uppercase; color: #6b7280; font-weight: bold; margin-bottom: 5px;\">\n",
        "      Session #3\n",
        "    </div>\n",
        "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
        "      LLMs\n",
        "    </div>\n",
        "    <div style=\"font-size: 26px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
        "      Part I: Introduction\n",
        "    </div>\n",
        "    <div style=\"font-size: 16.5px; color: #374151; font-style: italic; margin-bottom: 0;\">\n",
        "      Data Science for Economics: Mastering Unstructured Data\n",
        "    </div>\n",
        "  </div>\n",
        "\n",
        "  <!-- Logo Section -->\n",
        "  <div style=\"background: none; text-align: center; margin: 30px 0 10px;\">\n",
        "    <img src=\"https://www.cemfi.es/images/Logo-Azul.png\" alt=\"CEMFI Logo\" style=\"width: 158px; filter: drop-shadow(0 2px 12px rgba(56,84,156,0.05)); margin-bottom: 0;\">\n",
        "  </div>\n",
        "\n",
        "  <!-- Name -->\n",
        "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1.22em; font-weight: bold; margin-bottom: 0px;\">\n",
        "    Jesus Villota Miranda © 2025\n",
        "  </div>\n",
        "\n",
        "  <!-- Contact info -->\n",
        "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1em; margin-top: 7px; margin-bottom: 20px;\">\n",
        "    <a href=\"mailto:jesus.villota@cemfi.edu.es\" style=\"color: #38549c; text-decoration: none; margin-right:8px;\" title=\"Email\">\n",
        "      <!-- <img src=\"https://cdn-icons-png.flaticon.com/512/11679/11679732.png\" alt=\"Email\" style=\"width:18px; vertical-align:middle; margin-right:5px;\"> -->\n",
        "      jesus.villota@cemfi.edu.es\n",
        "    </a>\n",
        "    <span style=\"color:#9fa7bd;\">|</span>\n",
        "    <a href=\"https://www.linkedin.com/in/jesusvillotamiranda/\" target=\"_blank\" style=\"color: #38549c; text-decoration: none; margin-left:7px;\" title=\"LinkedIn\">\n",
        "      <!-- <img src=\"https://1.bp.blogspot.com/-onvhHUdW1Us/YI52e9j4eKI/AAAAAAAAE4c/6s9wzOpIDYcAo4YmTX1Qg51OlwMFmilFACLcBGAsYHQ/s1600/Logo%2BLinkedin.png\" alt=\"LinkedIn\" style=\"width:17px; vertical-align:middle; margin-right:5px;\"> -->\n",
        "      LinkedIn\n",
        "    </a>\n",
        "  </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1FnHx9gE-15"
      },
      "source": [
        "\n",
        "🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
        "\n",
        "**FIRST THING: Run the cell below!!!**\n",
        "\n",
        "*This will save us time later (this way we can look at the theory while packages get installed in the background)*\n",
        "\n",
        "Note that `transformers` take a long time to install!\n",
        "\n",
        "🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF6paI_aE98D",
        "outputId": "c64bcdb2-85d3-4be6-d877-ec5712218ee3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (4.55.2)\n",
            "Requirement already satisfied: filelock in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
            "Requirement already satisfied: requests in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: groq in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (0.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from groq) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bertviz in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (1.4.1)\n",
            "Requirement already satisfied: transformers>=2.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from bertviz) (4.55.2)\n",
            "Requirement already satisfied: torch>=1.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from bertviz) (2.4.1)\n",
            "Requirement already satisfied: tqdm in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from bertviz) (4.66.5)\n",
            "Requirement already satisfied: boto3 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from bertviz) (1.36.7)\n",
            "Requirement already satisfied: requests in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from bertviz) (2.32.3)\n",
            "Requirement already satisfied: regex in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from bertviz) (2024.7.24)\n",
            "Requirement already satisfied: sentencepiece in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from bertviz) (0.2.0)\n",
            "Requirement already satisfied: IPython>=7.14 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from bertviz) (8.27.0)\n",
            "Requirement already satisfied: decorator in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from IPython>=7.14->bertviz) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from IPython>=7.14->bertviz) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from IPython>=7.14->bertviz) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from IPython>=7.14->bertviz) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from IPython>=7.14->bertviz) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from IPython>=7.14->bertviz) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from IPython>=7.14->bertviz) (5.14.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from IPython>=7.14->bertviz) (4.14.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from IPython>=7.14->bertviz) (4.8.0)\n",
            "Requirement already satisfied: filelock in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.13.1)\n",
            "Requirement already satisfied: sympy in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (1.13.2)\n",
            "Requirement already satisfied: networkx in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from torch>=1.0->bertviz) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (0.4.4)\n",
            "Requirement already satisfied: botocore<1.37.0,>=1.36.7 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from boto3->bertviz) (1.36.7)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from boto3->bertviz) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from boto3->bertviz) (0.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from requests->bertviz) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from requests->bertviz) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from requests->bertviz) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from requests->bertviz) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.7->boto3->bertviz) (2.9.0.post0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=2.0->bertviz) (1.1.8)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from jedi>=0.16->IPython>=7.14->bertviz) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from pexpect>4.3->IPython>=7.14->bertviz) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython>=7.14->bertviz) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.0->bertviz) (2.1.3)\n",
            "Requirement already satisfied: executing in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from stack-data->IPython>=7.14->bertviz) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from stack-data->IPython>=7.14->bertviz) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from stack-data->IPython>=7.14->bertviz) (0.2.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.7->boto3->bertviz) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install groq\n",
        "!pip install bertviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L3CRb5EwYqW"
      },
      "source": [
        "## **What are LLMs?**\n",
        "\n",
        "- In natural language processing (NLP), **Large Language Models (LLMs)** are designed to \" understand\" and generate human-like text. These models utilize the **transformer architecture**, which excels in modeling complex language tasks by capturing **long-range dependencies and contextual relationships**.\n",
        "\n",
        "\n",
        "- At the heart of LLMs lies the concept of **tokens**, which serve as the elemental units of text. **Tokens can be individual words, subword units, or characters**. Let $x_{1: n}:=\\left\\{x_1, x_2, \\ldots, x_n\\right\\}$ represent a sequence of tokens. *The goal of an LLM is to estimate the probability distribution of the next token $x_{n+1}$ conditioned on the previous tokens $x_{1: n}$*\n",
        "\n",
        "$$\n",
        "\\mathbb{P}\\left[x_{n+1} \\mid\\left\\{x_1, x_2, \\ldots, x_n\\right\\}\\right]\n",
        "$$\n",
        "\n",
        "\n",
        "- An **LLM** is a **neural network** architecture designed to learn and **approximate** this **conditional probability distribution** over sequences of tokens with a large number of parameters $\\Theta$. Namely, we can formulate an LLM as a parameterized function $f_{\\Theta}$ that maps a sequence of tokens $\\left\\{x_1, x_2, \\ldots, x_n\\right\\}$ to a probability distribution over the vocabulary, where the parameters $\\Theta$ are learned from a large corpus of text training data.\n",
        "\n",
        "$$\n",
        "f_{\\Theta}:\\left\\{x_1, x_2, \\ldots, x_n\\right\\} \\rightarrow \\mathbb{P}\\left[x_{n+1} \\mid\\left\\{x_1, x_2, \\ldots, x_n\\right\\} ; \\Theta\\right]\n",
        "$$\n",
        "\n",
        "\n",
        "- **Interacting with an LLM** involves specifying a prefix sequence $x_{1: n}$, termed the \"**prompt**\", and sampling the subsequent tokens $x_{n+1: z}$, known as the \"**completion**\". This process enables users to guide and control the generation of text according to desired contexts and constraints.\n",
        "\n",
        "\n",
        "$$\n",
        "\\underbrace{\\left\\{x_1, \\ldots, x_n\\right\\}}_{\\text {prompt }} \\longrightarrow \\underbrace{\\left\\{x_{n+1}, \\ldots, x_z\\right\\}}_{\\text {completion }}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQRRj7Niznur"
      },
      "source": [
        "## **Exploring Tokenization**\n",
        "\n",
        "Let's explore how tokenization works, which is a crucial step before feeding text into an LLM. We will use Python and some NLP libraries to visualize tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SEL29CoyXQ-",
        "outputId": "9c0b6953-5221-445c-a219-3023db598e05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['Understanding', 'Ġthe', 'Ġeffects', 'Ġof', 'Ġmonetary', 'Ġpolicy', 'Ġon', 'Ġeconomic', 'Ġgrowth', 'Ġis', 'Ġcrucial', 'Ġfor', 'Ġcentral', 'Ġbanks', '.']\n",
            "Token IDs: [43467, 262, 3048, 286, 15331, 2450, 319, 3034, 3349, 318, 8780, 329, 4318, 6341, 13]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Select a pre-trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Sample text from economics\n",
        "sample_text = \"Understanding the effects of monetary policy on economic growth is crucial for central banks.\"\n",
        "\n",
        "# Tokenize the sample text\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Display tokens and their corresponding IDs\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2yj99j_00V-"
      },
      "source": [
        "## **Visualizing Self-Attention**\n",
        "\n",
        "#### **What is Attention in Transformers?**\n",
        "\n",
        "Before diving into the code, it's important to understand what attention means in the context of transformer models.\n",
        "\n",
        "- **Attention Mechanism**: In transformers, the attention mechanism allows the model to *weigh the importance of different words (tokens) in a sentence* when making predictions. Instead of processing words in isolation, attention helps the model understand context by focusing on relevant words in a sequence.\n",
        "\n",
        "- **Self-Attention**: In self-attention, *every word in a sentence can attend to every other word*, including itself. This allows the model to capture dependencies regardless of their distance in the sequence.\n",
        "\n",
        "\n",
        "#### **Understanding the Head View Plot**\n",
        "The head view plot in bertviz shows the attention weights for each attention head in a given layer of the transformer model. Here’s how to interpret the plot:\n",
        "\n",
        "- **Layers and Heads:**\n",
        "  - Transformers like BERT consist of multiple layers (e.g., 12 layers for BERT-base). Each layer contains several attention heads (e.g., 12 heads per layer for BERT-base).\n",
        "  - The head view plot displays a **grid** where each cell corresponds to an **attention head** in a specific layer. For example, the top-left cell represents the attention for head 0 in layer 0, the next cell to the right represents head 1 in layer 0, and so on.\n",
        "\n",
        "- **Tokens Displayed on Axes:**\n",
        "The tokens of the input sentence are displayed along both the x-axis and y-axis of each attention head’s plot.\n",
        "The y-axis represents the \"query\" tokens (the token that is paying attention), while the x-axis represents the \"key\" tokens (the tokens being paid attention to).\n",
        "\n",
        "- **Color Intensity:**\n",
        "The color intensity of each square in a plot indicates the attention weight between the corresponding pair of tokens.\n",
        "Darker colors represent higher attention weights, meaning the model is focusing more on these tokens in the context of the sentence.\n",
        "Lighter colors represent lower attention weights, meaning less focus.\n",
        "\n",
        "- **Interpreting Attention Patterns:**\n",
        "\n",
        "  - **Diagonal Lines**: If you see dark squares along the diagonal, it indicates that the model is paying attention to the current word itself. This is common, as each token usually attends to itself to maintain information.\n",
        "\n",
        "  - **Off-Diagonal Patterns**: When attention weights are darker off the diagonal, this indicates that the model is focusing more on other tokens. For example, in a sentence like \"The central bank raised interest rates to control inflation,\" the word \"interest\" might have high attention weights with \"rates\" in a financial context.\n",
        "\n",
        "- **Global Attention vs. Local Attention:**\n",
        "Some heads might show more uniform attention (darker squares across multiple tokens), indicating a global context understanding.\n",
        "Others might focus only on nearby tokens (localized clusters of darker squares), indicating a local context understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/jesusvillotamiranda/anaconda3/bin/python\n",
            "['ASTConfig', 'ASTFeatureExtractor', 'ASTForAudioClassification', 'ASTModel', 'ASTPreTrainedModel', 'Adafactor', 'AdamWeightDecay', 'AdaptiveEmbedding', 'AddedToken', 'Aimv2Config', 'Aimv2Model', 'Aimv2PreTrainedModel', 'Aimv2TextConfig', 'Aimv2TextModel', 'Aimv2VisionConfig', 'Aimv2VisionModel', 'AlbertConfig', 'AlbertForMaskedLM', 'AlbertForMultipleChoice', 'AlbertForPreTraining', 'AlbertForQuestionAnswering', 'AlbertForSequenceClassification', 'AlbertForTokenClassification', 'AlbertModel', 'AlbertOnnxConfig', 'AlbertPreTrainedModel', 'AlbertTokenizer', 'AlbertTokenizerFast', 'AlignConfig', 'AlignModel', 'AlignPreTrainedModel', 'AlignProcessor', 'AlignTextConfig', 'AlignTextModel', 'AlignVisionConfig', 'AlignVisionModel', 'AltCLIPConfig', 'AltCLIPModel', 'AltCLIPPreTrainedModel', 'AltCLIPProcessor', 'AltCLIPTextConfig', 'AltCLIPTextModel', 'AltCLIPVisionConfig', 'AltCLIPVisionModel', 'AlternatingCodebooksLogitsProcessor', 'AqlmConfig', 'ArceeConfig', 'ArceeForCausalLM', 'ArceeForQuestionAnswering', 'ArceeForSequenceClassification', 'ArceeForTokenClassification', 'ArceeModel', 'ArceePreTrainedModel', 'AriaConfig', 'AriaForConditionalGeneration', 'AriaImageProcessor', 'AriaModel', 'AriaPreTrainedModel', 'AriaProcessor', 'AriaTextConfig', 'AriaTextForCausalLM', 'AriaTextModel', 'AriaTextPreTrainedModel', 'AsyncTextIteratorStreamer', 'AttentionInterface', 'AttentionMaskInterface', 'AudioClassificationPipeline', 'AutoBackbone', 'AutoConfig', 'AutoFeatureExtractor', 'AutoImageProcessor', 'AutoModel', 'AutoModelForAudioClassification', 'AutoModelForAudioFrameClassification', 'AutoModelForAudioTokenization', 'AutoModelForAudioXVector', 'AutoModelForCTC', 'AutoModelForCausalLM', 'AutoModelForDepthEstimation', 'AutoModelForDocumentQuestionAnswering', 'AutoModelForImageClassification', 'AutoModelForImageSegmentation', 'AutoModelForImageTextToText', 'AutoModelForImageToImage', 'AutoModelForInstanceSegmentation', 'AutoModelForKeypointDetection', 'AutoModelForKeypointMatching', 'AutoModelForMaskGeneration', 'AutoModelForMaskedImageModeling', 'AutoModelForMaskedLM', 'AutoModelForMultipleChoice', 'AutoModelForNextSentencePrediction', 'AutoModelForObjectDetection', 'AutoModelForPreTraining', 'AutoModelForQuestionAnswering', 'AutoModelForSemanticSegmentation', 'AutoModelForSeq2SeqLM', 'AutoModelForSequenceClassification', 'AutoModelForSpeechSeq2Seq', 'AutoModelForTableQuestionAnswering', 'AutoModelForTextEncoding', 'AutoModelForTextToSpectrogram', 'AutoModelForTextToWaveform', 'AutoModelForTimeSeriesPrediction', 'AutoModelForTokenClassification', 'AutoModelForUniversalSegmentation', 'AutoModelForVideoClassification', 'AutoModelForVision2Seq', 'AutoModelForVisualQuestionAnswering', 'AutoModelForZeroShotImageClassification', 'AutoModelForZeroShotObjectDetection', 'AutoModelWithLMHead', 'AutoProcessor', 'AutoRoundConfig', 'AutoTokenizer', 'AutoVideoProcessor', 'AutoformerConfig', 'AutoformerForPrediction', 'AutoformerModel', 'AutoformerPreTrainedModel', 'AutomaticSpeechRecognitionPipeline', 'AwqConfig', 'AyaVisionConfig', 'AyaVisionForConditionalGeneration', 'AyaVisionModel', 'AyaVisionPreTrainedModel', 'AyaVisionProcessor', 'BambaConfig', 'BambaForCausalLM', 'BambaModel', 'BambaPreTrainedModel', 'BarkCausalModel', 'BarkCoarseConfig', 'BarkCoarseModel', 'BarkConfig', 'BarkFineConfig', 'BarkFineModel', 'BarkModel', 'BarkPreTrainedModel', 'BarkProcessor', 'BarkSemanticConfig', 'BarkSemanticModel', 'BartConfig', 'BartForCausalLM', 'BartForConditionalGeneration', 'BartForQuestionAnswering', 'BartForSequenceClassification', 'BartModel', 'BartOnnxConfig', 'BartPreTrainedModel', 'BartPretrainedModel', 'BartTokenizer', 'BartTokenizerFast', 'BarthezTokenizer', 'BarthezTokenizerFast', 'BartphoTokenizer', 'BaseImageProcessor', 'BaseImageProcessorFast', 'BaseVideoProcessor', 'BasicTokenizer', 'BatchEncoding', 'BatchFeature', 'BayesianDetectorConfig', 'BayesianDetectorModel', 'BeamScorer', 'BeamSearchScorer', 'BeitBackbone', 'BeitConfig', 'BeitFeatureExtractor', 'BeitForImageClassification', 'BeitForMaskedImageModeling', 'BeitForSemanticSegmentation', 'BeitImageProcessor', 'BeitImageProcessorFast', 'BeitModel', 'BeitOnnxConfig', 'BeitPreTrainedModel', 'BertConfig', 'BertForMaskedLM', 'BertForMultipleChoice', 'BertForNextSentencePrediction', 'BertForPreTraining', 'BertForQuestionAnswering', 'BertForSequenceClassification', 'BertForTokenClassification', 'BertGenerationConfig', 'BertGenerationDecoder', 'BertGenerationEncoder', 'BertGenerationPreTrainedModel', 'BertGenerationTokenizer', 'BertJapaneseTokenizer', 'BertLMHeadModel', 'BertLayer', 'BertModel', 'BertOnnxConfig', 'BertPreTrainedModel', 'BertTokenizer', 'BertTokenizerFast', 'BertweetTokenizer', 'BigBirdConfig', 'BigBirdForCausalLM', 'BigBirdForMaskedLM', 'BigBirdForMultipleChoice', 'BigBirdForPreTraining', 'BigBirdForQuestionAnswering', 'BigBirdForSequenceClassification', 'BigBirdForTokenClassification', 'BigBirdLayer', 'BigBirdModel', 'BigBirdOnnxConfig', 'BigBirdPegasusConfig', 'BigBirdPegasusForCausalLM', 'BigBirdPegasusForConditionalGeneration', 'BigBirdPegasusForQuestionAnswering', 'BigBirdPegasusForSequenceClassification', 'BigBirdPegasusModel', 'BigBirdPegasusOnnxConfig', 'BigBirdPegasusPreTrainedModel', 'BigBirdPreTrainedModel', 'BigBirdTokenizer', 'BigBirdTokenizerFast', 'BioGptConfig', 'BioGptForCausalLM', 'BioGptForSequenceClassification', 'BioGptForTokenClassification', 'BioGptModel', 'BioGptPreTrainedModel', 'BioGptTokenizer', 'BitBackbone', 'BitConfig', 'BitForImageClassification', 'BitImageProcessor', 'BitImageProcessorFast', 'BitModel', 'BitNetConfig', 'BitNetForCausalLM', 'BitNetModel', 'BitNetPreTrainedModel', 'BitNetQuantConfig', 'BitPreTrainedModel', 'BitsAndBytesConfig', 'BlenderbotConfig', 'BlenderbotForCausalLM', 'BlenderbotForConditionalGeneration', 'BlenderbotModel', 'BlenderbotOnnxConfig', 'BlenderbotPreTrainedModel', 'BlenderbotSmallConfig', 'BlenderbotSmallForCausalLM', 'BlenderbotSmallForConditionalGeneration', 'BlenderbotSmallModel', 'BlenderbotSmallOnnxConfig', 'BlenderbotSmallPreTrainedModel', 'BlenderbotSmallTokenizer', 'BlenderbotSmallTokenizerFast', 'BlenderbotTokenizer', 'BlenderbotTokenizerFast', 'Blip2Config', 'Blip2ForConditionalGeneration', 'Blip2ForImageTextRetrieval', 'Blip2Model', 'Blip2PreTrainedModel', 'Blip2Processor', 'Blip2QFormerConfig', 'Blip2QFormerModel', 'Blip2TextModelWithProjection', 'Blip2VisionConfig', 'Blip2VisionModel', 'Blip2VisionModelWithProjection', 'BlipConfig', 'BlipForConditionalGeneration', 'BlipForImageTextRetrieval', 'BlipForQuestionAnswering', 'BlipImageProcessor', 'BlipImageProcessorFast', 'BlipModel', 'BlipPreTrainedModel', 'BlipProcessor', 'BlipTextConfig', 'BlipTextLMHeadModel', 'BlipTextModel', 'BlipTextPreTrainedModel', 'BlipVisionConfig', 'BlipVisionModel', 'BloomConfig', 'BloomForCausalLM', 'BloomForQuestionAnswering', 'BloomForSequenceClassification', 'BloomForTokenClassification', 'BloomModel', 'BloomOnnxConfig', 'BloomPreTrainedModel', 'BloomTokenizerFast', 'BridgeTowerConfig', 'BridgeTowerForContrastiveLearning', 'BridgeTowerForImageAndTextRetrieval', 'BridgeTowerForMaskedLM', 'BridgeTowerImageProcessor', 'BridgeTowerImageProcessorFast', 'BridgeTowerModel', 'BridgeTowerPreTrainedModel', 'BridgeTowerProcessor', 'BridgeTowerTextConfig', 'BridgeTowerVisionConfig', 'BrosConfig', 'BrosForTokenClassification', 'BrosModel', 'BrosPreTrainedModel', 'BrosProcessor', 'BrosSpadeEEForTokenClassification', 'BrosSpadeELForTokenClassification', 'ByT5Tokenizer', 'CLIPConfig', 'CLIPFeatureExtractor', 'CLIPForImageClassification', 'CLIPImageProcessor', 'CLIPImageProcessorFast', 'CLIPModel', 'CLIPOnnxConfig', 'CLIPPreTrainedModel', 'CLIPProcessor', 'CLIPSegConfig', 'CLIPSegForImageSegmentation', 'CLIPSegModel', 'CLIPSegPreTrainedModel', 'CLIPSegProcessor', 'CLIPSegTextConfig', 'CLIPSegTextModel', 'CLIPSegVisionConfig', 'CLIPSegVisionModel', 'CLIPTextConfig', 'CLIPTextModel', 'CLIPTextModelWithProjection', 'CLIPTokenizer', 'CLIPTokenizerFast', 'CLIPVisionConfig', 'CLIPVisionModel', 'CLIPVisionModelWithProjection', 'CONFIG_MAPPING', 'CONFIG_NAME', 'CTRLConfig', 'CTRLForSequenceClassification', 'CTRLLMHeadModel', 'CTRLModel', 'CTRLPreTrainedModel', 'CTRLTokenizer', 'Cache', 'CacheConfig', 'CacheLayerMixin', 'CacheProcessor', 'CamembertConfig', 'CamembertForCausalLM', 'CamembertForMaskedLM', 'CamembertForMultipleChoice', 'CamembertForQuestionAnswering', 'CamembertForSequenceClassification', 'CamembertForTokenClassification', 'CamembertModel', 'CamembertOnnxConfig', 'CamembertPreTrainedModel', 'CamembertTokenizer', 'CamembertTokenizerFast', 'CanineConfig', 'CanineForMultipleChoice', 'CanineForQuestionAnswering', 'CanineForSequenceClassification', 'CanineForTokenClassification', 'CanineLayer', 'CanineModel', 'CaninePreTrainedModel', 'CanineTokenizer', 'ChameleonConfig', 'ChameleonForConditionalGeneration', 'ChameleonImageProcessor', 'ChameleonImageProcessorFast', 'ChameleonModel', 'ChameleonPreTrainedModel', 'ChameleonProcessor', 'ChameleonVQVAE', 'ChameleonVQVAEConfig', 'CharSpan', 'CharacterTokenizer', 'ChineseCLIPConfig', 'ChineseCLIPFeatureExtractor', 'ChineseCLIPImageProcessor', 'ChineseCLIPImageProcessorFast', 'ChineseCLIPModel', 'ChineseCLIPOnnxConfig', 'ChineseCLIPPreTrainedModel', 'ChineseCLIPProcessor', 'ChineseCLIPTextConfig', 'ChineseCLIPTextModel', 'ChineseCLIPVisionConfig', 'ChineseCLIPVisionModel', 'ChunkedSlidingLayer', 'ClapAudioConfig', 'ClapAudioModel', 'ClapAudioModelWithProjection', 'ClapConfig', 'ClapFeatureExtractor', 'ClapModel', 'ClapPreTrainedModel', 'ClapProcessor', 'ClapTextConfig', 'ClapTextModel', 'ClapTextModelWithProjection', 'ClassifierFreeGuidanceLogitsProcessor', 'ClvpConfig', 'ClvpDecoder', 'ClvpDecoderConfig', 'ClvpEncoder', 'ClvpEncoderConfig', 'ClvpFeatureExtractor', 'ClvpForCausalLM', 'ClvpModel', 'ClvpModelForConditionalGeneration', 'ClvpPreTrainedModel', 'ClvpProcessor', 'ClvpTokenizer', 'CodeGenConfig', 'CodeGenForCausalLM', 'CodeGenModel', 'CodeGenOnnxConfig', 'CodeGenPreTrainedModel', 'CodeGenTokenizer', 'CodeGenTokenizerFast', 'CodeLlamaTokenizer', 'CodeLlamaTokenizerFast', 'Cohere2Config', 'Cohere2ForCausalLM', 'Cohere2Model', 'Cohere2PreTrainedModel', 'Cohere2VisionConfig', 'Cohere2VisionForConditionalGeneration', 'Cohere2VisionImageProcessorFast', 'Cohere2VisionModel', 'Cohere2VisionPreTrainedModel', 'Cohere2VisionProcessor', 'CohereConfig', 'CohereForCausalLM', 'CohereModel', 'CoherePreTrainedModel', 'CohereTokenizerFast', 'ColPaliConfig', 'ColPaliForRetrieval', 'ColPaliPreTrainedModel', 'ColPaliProcessor', 'ColQwen2Config', 'ColQwen2ForRetrieval', 'ColQwen2PreTrainedModel', 'ColQwen2Processor', 'CompileConfig', 'CompressedTensorsConfig', 'ConditionalDetrConfig', 'ConditionalDetrFeatureExtractor', 'ConditionalDetrForObjectDetection', 'ConditionalDetrForSegmentation', 'ConditionalDetrImageProcessor', 'ConditionalDetrImageProcessorFast', 'ConditionalDetrModel', 'ConditionalDetrOnnxConfig', 'ConditionalDetrPreTrainedModel', 'ConstrainedBeamSearchScorer', 'Constraint', 'ConstraintListState', 'Conv1D', 'ConvBertConfig', 'ConvBertForMaskedLM', 'ConvBertForMultipleChoice', 'ConvBertForQuestionAnswering', 'ConvBertForSequenceClassification', 'ConvBertForTokenClassification', 'ConvBertLayer', 'ConvBertModel', 'ConvBertOnnxConfig', 'ConvBertPreTrainedModel', 'ConvBertTokenizer', 'ConvBertTokenizerFast', 'ConvNextBackbone', 'ConvNextConfig', 'ConvNextFeatureExtractor', 'ConvNextForImageClassification', 'ConvNextImageProcessor', 'ConvNextImageProcessorFast', 'ConvNextModel', 'ConvNextOnnxConfig', 'ConvNextPreTrainedModel', 'ConvNextV2Backbone', 'ConvNextV2Config', 'ConvNextV2ForImageClassification', 'ConvNextV2Model', 'ConvNextV2PreTrainedModel', 'CpmAntConfig', 'CpmAntForCausalLM', 'CpmAntModel', 'CpmAntPreTrainedModel', 'CpmAntTokenizer', 'CpmTokenizer', 'CpmTokenizerFast', 'CsmBackboneModel', 'CsmConfig', 'CsmDepthDecoderConfig', 'CsmDepthDecoderForCausalLM', 'CsmDepthDecoderModel', 'CsmForConditionalGeneration', 'CsmPreTrainedModel', 'CsmProcessor', 'CsvPipelineDataFormat', 'CvtConfig', 'CvtForImageClassification', 'CvtModel', 'CvtPreTrainedModel', 'DFineConfig', 'DFineForObjectDetection', 'DFineModel', 'DFinePreTrainedModel', 'DPRConfig', 'DPRContextEncoder', 'DPRContextEncoderTokenizer', 'DPRContextEncoderTokenizerFast', 'DPRPreTrainedModel', 'DPRPretrainedContextEncoder', 'DPRPretrainedQuestionEncoder', 'DPRPretrainedReader', 'DPRQuestionEncoder', 'DPRQuestionEncoderTokenizer', 'DPRQuestionEncoderTokenizerFast', 'DPRReader', 'DPRReaderOutput', 'DPRReaderTokenizer', 'DPRReaderTokenizerFast', 'DPTConfig', 'DPTFeatureExtractor', 'DPTForDepthEstimation', 'DPTForSemanticSegmentation', 'DPTImageProcessor', 'DPTImageProcessorFast', 'DPTModel', 'DPTPreTrainedModel', 'DabDetrConfig', 'DabDetrForObjectDetection', 'DabDetrModel', 'DabDetrPreTrainedModel', 'DacConfig', 'DacFeatureExtractor', 'DacModel', 'DacPreTrainedModel', 'Data2VecAudioConfig', 'Data2VecAudioForAudioFrameClassification', 'Data2VecAudioForCTC', 'Data2VecAudioForSequenceClassification', 'Data2VecAudioForXVector', 'Data2VecAudioModel', 'Data2VecAudioPreTrainedModel', 'Data2VecTextConfig', 'Data2VecTextForCausalLM', 'Data2VecTextForMaskedLM', 'Data2VecTextForMultipleChoice', 'Data2VecTextForQuestionAnswering', 'Data2VecTextForSequenceClassification', 'Data2VecTextForTokenClassification', 'Data2VecTextModel', 'Data2VecTextOnnxConfig', 'Data2VecTextPreTrainedModel', 'Data2VecVisionConfig', 'Data2VecVisionForImageClassification', 'Data2VecVisionForSemanticSegmentation', 'Data2VecVisionModel', 'Data2VecVisionOnnxConfig', 'Data2VecVisionPreTrainedModel', 'DataCollator', 'DataCollatorForLanguageModeling', 'DataCollatorForMultipleChoice', 'DataCollatorForPermutationLanguageModeling', 'DataCollatorForSOP', 'DataCollatorForSeq2Seq', 'DataCollatorForTokenClassification', 'DataCollatorForWholeWordMask', 'DataCollatorWithFlattening', 'DataCollatorWithPadding', 'DataProcessor', 'DbrxConfig', 'DbrxForCausalLM', 'DbrxModel', 'DbrxPreTrainedModel', 'DebertaConfig', 'DebertaForMaskedLM', 'DebertaForQuestionAnswering', 'DebertaForSequenceClassification', 'DebertaForTokenClassification', 'DebertaModel', 'DebertaOnnxConfig', 'DebertaPreTrainedModel', 'DebertaTokenizer', 'DebertaTokenizerFast', 'DebertaV2Config', 'DebertaV2ForMaskedLM', 'DebertaV2ForMultipleChoice', 'DebertaV2ForQuestionAnswering', 'DebertaV2ForSequenceClassification', 'DebertaV2ForTokenClassification', 'DebertaV2Model', 'DebertaV2OnnxConfig', 'DebertaV2PreTrainedModel', 'DebertaV2Tokenizer', 'DebertaV2TokenizerFast', 'DecisionTransformerConfig', 'DecisionTransformerGPT2Model', 'DecisionTransformerGPT2PreTrainedModel', 'DecisionTransformerModel', 'DecisionTransformerPreTrainedModel', 'DeepseekV2Config', 'DeepseekV2ForCausalLM', 'DeepseekV2ForSequenceClassification', 'DeepseekV2Model', 'DeepseekV2PreTrainedModel', 'DeepseekV3Config', 'DeepseekV3ForCausalLM', 'DeepseekV3Model', 'DeepseekV3PreTrainedModel', 'DeepseekVLConfig', 'DeepseekVLForConditionalGeneration', 'DeepseekVLHybridConfig', 'DeepseekVLHybridForConditionalGeneration', 'DeepseekVLHybridImageProcessor', 'DeepseekVLHybridImageProcessorFast', 'DeepseekVLHybridModel', 'DeepseekVLHybridPreTrainedModel', 'DeepseekVLHybridProcessor', 'DeepseekVLImageProcessor', 'DeepseekVLImageProcessorFast', 'DeepseekVLModel', 'DeepseekVLPreTrainedModel', 'DeepseekVLProcessor', 'DefaultDataCollator', 'DefaultFlowCallback', 'DeformableDetrConfig', 'DeformableDetrFeatureExtractor', 'DeformableDetrForObjectDetection', 'DeformableDetrImageProcessor', 'DeformableDetrImageProcessorFast', 'DeformableDetrModel', 'DeformableDetrPreTrainedModel', 'DeiTConfig', 'DeiTFeatureExtractor', 'DeiTForImageClassification', 'DeiTForImageClassificationWithTeacher', 'DeiTForMaskedImageModeling', 'DeiTImageProcessor', 'DeiTImageProcessorFast', 'DeiTModel', 'DeiTOnnxConfig', 'DeiTPreTrainedModel', 'DepthAnythingConfig', 'DepthAnythingForDepthEstimation', 'DepthAnythingPreTrainedModel', 'DepthEstimationPipeline', 'DepthProConfig', 'DepthProForDepthEstimation', 'DepthProImageProcessor', 'DepthProImageProcessorFast', 'DepthProModel', 'DepthProPreTrainedModel', 'DetaConfig', 'DetaForObjectDetection', 'DetaImageProcessor', 'DetaModel', 'DetaPreTrainedModel', 'DetrConfig', 'DetrFeatureExtractor', 'DetrForObjectDetection', 'DetrForSegmentation', 'DetrImageProcessor', 'DetrImageProcessorFast', 'DetrModel', 'DetrOnnxConfig', 'DetrPreTrainedModel', 'DiaConfig', 'DiaDecoderConfig', 'DiaEncoderConfig', 'DiaFeatureExtractor', 'DiaForConditionalGeneration', 'DiaModel', 'DiaPreTrainedModel', 'DiaProcessor', 'DiaTokenizer', 'DiffLlamaConfig', 'DiffLlamaForCausalLM', 'DiffLlamaForQuestionAnswering', 'DiffLlamaForSequenceClassification', 'DiffLlamaForTokenClassification', 'DiffLlamaModel', 'DiffLlamaPreTrainedModel', 'DinatBackbone', 'DinatConfig', 'DinatForImageClassification', 'DinatModel', 'DinatPreTrainedModel', 'Dinov2Backbone', 'Dinov2Config', 'Dinov2ForImageClassification', 'Dinov2Model', 'Dinov2OnnxConfig', 'Dinov2PreTrainedModel', 'Dinov2WithRegistersBackbone', 'Dinov2WithRegistersConfig', 'Dinov2WithRegistersForImageClassification', 'Dinov2WithRegistersModel', 'Dinov2WithRegistersPreTrainedModel', 'DisjunctiveConstraint', 'DistilBertConfig', 'DistilBertForMaskedLM', 'DistilBertForMultipleChoice', 'DistilBertForQuestionAnswering', 'DistilBertForSequenceClassification', 'DistilBertForTokenClassification', 'DistilBertModel', 'DistilBertOnnxConfig', 'DistilBertPreTrainedModel', 'DistilBertTokenizer', 'DistilBertTokenizerFast', 'DocumentQuestionAnsweringPipeline', 'DogeConfig', 'DogeForCausalLM', 'DogeForSequenceClassification', 'DogeModel', 'DogePreTrainedModel', 'DonutFeatureExtractor', 'DonutImageProcessor', 'DonutImageProcessorFast', 'DonutProcessor', 'DonutSwinConfig', 'DonutSwinForImageClassification', 'DonutSwinModel', 'DonutSwinPreTrainedModel', 'Dots1Config', 'Dots1ForCausalLM', 'Dots1Model', 'Dots1PreTrainedModel', 'DummyObject', 'DynamicCache', 'DynamicLayer', 'EarlyStoppingCallback', 'EetqConfig', 'EfficientFormerConfig', 'EfficientFormerForImageClassification', 'EfficientFormerForImageClassificationWithTeacher', 'EfficientFormerImageProcessor', 'EfficientFormerModel', 'EfficientFormerPreTrainedModel', 'EfficientLoFTRConfig', 'EfficientLoFTRForKeypointMatching', 'EfficientLoFTRImageProcessor', 'EfficientLoFTRModel', 'EfficientLoFTRPreTrainedModel', 'EfficientNetConfig', 'EfficientNetForImageClassification', 'EfficientNetImageProcessor', 'EfficientNetImageProcessorFast', 'EfficientNetModel', 'EfficientNetOnnxConfig', 'EfficientNetPreTrainedModel', 'ElectraConfig', 'ElectraForCausalLM', 'ElectraForMaskedLM', 'ElectraForMultipleChoice', 'ElectraForPreTraining', 'ElectraForQuestionAnswering', 'ElectraForSequenceClassification', 'ElectraForTokenClassification', 'ElectraModel', 'ElectraOnnxConfig', 'ElectraPreTrainedModel', 'ElectraTokenizer', 'ElectraTokenizerFast', 'Emu3Config', 'Emu3ForCausalLM', 'Emu3ForConditionalGeneration', 'Emu3ImageProcessor', 'Emu3Model', 'Emu3PreTrainedModel', 'Emu3Processor', 'Emu3TextConfig', 'Emu3TextModel', 'Emu3VQVAE', 'Emu3VQVAEConfig', 'EncodecConfig', 'EncodecFeatureExtractor', 'EncodecModel', 'EncodecPreTrainedModel', 'EncoderDecoderCache', 'EncoderDecoderConfig', 'EncoderDecoderModel', 'EncoderNoRepeatNGramLogitsProcessor', 'EncoderRepetitionPenaltyLogitsProcessor', 'EomtConfig', 'EomtForUniversalSegmentation', 'EomtImageProcessor', 'EomtImageProcessorFast', 'EomtPreTrainedModel', 'EosTokenCriteria', 'EpsilonLogitsWarper', 'Ernie4_5Config', 'Ernie4_5ForCausalLM', 'Ernie4_5Model', 'Ernie4_5PreTrainedModel', 'Ernie4_5_MoeConfig', 'Ernie4_5_MoeForCausalLM', 'Ernie4_5_MoeModel', 'Ernie4_5_MoePreTrainedModel', 'ErnieConfig', 'ErnieForCausalLM', 'ErnieForMaskedLM', 'ErnieForMultipleChoice', 'ErnieForNextSentencePrediction', 'ErnieForPreTraining', 'ErnieForQuestionAnswering', 'ErnieForSequenceClassification', 'ErnieForTokenClassification', 'ErnieMConfig', 'ErnieMForInformationExtraction', 'ErnieMForMultipleChoice', 'ErnieMForQuestionAnswering', 'ErnieMForSequenceClassification', 'ErnieMForTokenClassification', 'ErnieMModel', 'ErnieMPreTrainedModel', 'ErnieMTokenizer', 'ErnieModel', 'ErnieOnnxConfig', 'ErniePreTrainedModel', 'EsmConfig', 'EsmFoldPreTrainedModel', 'EsmForMaskedLM', 'EsmForProteinFolding', 'EsmForSequenceClassification', 'EsmForTokenClassification', 'EsmModel', 'EsmPreTrainedModel', 'EsmTokenizer', 'EtaLogitsWarper', 'EvalPrediction', 'EvollaConfig', 'EvollaForProteinText2Text', 'EvollaModel', 'EvollaPreTrainedModel', 'EvollaProcessor', 'Exaone4Config', 'Exaone4ForCausalLM', 'Exaone4ForQuestionAnswering', 'Exaone4ForSequenceClassification', 'Exaone4ForTokenClassification', 'Exaone4Model', 'Exaone4PreTrainedModel', 'ExponentialDecayLengthPenalty', 'FEATURE_EXTRACTOR_MAPPING', 'FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_CAUSAL_LM_MAPPING', 'FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_MASKED_LM_MAPPING', 'FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'FLAX_MODEL_FOR_PRETRAINING_MAPPING', 'FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING', 'FLAX_MODEL_MAPPING', 'FNetConfig', 'FNetForMaskedLM', 'FNetForMultipleChoice', 'FNetForNextSentencePrediction', 'FNetForPreTraining', 'FNetForQuestionAnswering', 'FNetForSequenceClassification', 'FNetForTokenClassification', 'FNetLayer', 'FNetModel', 'FNetPreTrainedModel', 'FNetTokenizer', 'FNetTokenizerFast', 'FPQuantConfig', 'FSMTConfig', 'FSMTForConditionalGeneration', 'FSMTModel', 'FSMTTokenizer', 'FalconConfig', 'FalconForCausalLM', 'FalconForQuestionAnswering', 'FalconForSequenceClassification', 'FalconForTokenClassification', 'FalconH1Config', 'FalconH1ForCausalLM', 'FalconH1Model', 'FalconH1PreTrainedModel', 'FalconMambaCache', 'FalconMambaConfig', 'FalconMambaForCausalLM', 'FalconMambaModel', 'FalconMambaPreTrainedModel', 'FalconModel', 'FalconPreTrainedModel', 'FastSpeech2ConformerConfig', 'FastSpeech2ConformerHifiGan', 'FastSpeech2ConformerHifiGanConfig', 'FastSpeech2ConformerModel', 'FastSpeech2ConformerPreTrainedModel', 'FastSpeech2ConformerTokenizer', 'FastSpeech2ConformerWithHifiGan', 'FastSpeech2ConformerWithHifiGanConfig', 'FbgemmFp8Config', 'FeatureExtractionMixin', 'FeatureExtractionPipeline', 'FillMaskPipeline', 'FineGrainedFP8Config', 'FlaubertConfig', 'FlaubertForMultipleChoice', 'FlaubertForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FlaubertForSequenceClassification', 'FlaubertForTokenClassification', 'FlaubertModel', 'FlaubertOnnxConfig', 'FlaubertPreTrainedModel', 'FlaubertTokenizer', 'FlaubertWithLMHeadModel', 'FlavaConfig', 'FlavaFeatureExtractor', 'FlavaForPreTraining', 'FlavaImageCodebook', 'FlavaImageCodebookConfig', 'FlavaImageConfig', 'FlavaImageModel', 'FlavaImageProcessor', 'FlavaImageProcessorFast', 'FlavaModel', 'FlavaMultimodalConfig', 'FlavaMultimodalModel', 'FlavaPreTrainedModel', 'FlavaProcessor', 'FlavaTextConfig', 'FlavaTextModel', 'FlaxAlbertForMaskedLM', 'FlaxAlbertForMultipleChoice', 'FlaxAlbertForPreTraining', 'FlaxAlbertForQuestionAnswering', 'FlaxAlbertForSequenceClassification', 'FlaxAlbertForTokenClassification', 'FlaxAlbertModel', 'FlaxAlbertPreTrainedModel', 'FlaxAutoModel', 'FlaxAutoModelForCausalLM', 'FlaxAutoModelForImageClassification', 'FlaxAutoModelForMaskedLM', 'FlaxAutoModelForMultipleChoice', 'FlaxAutoModelForNextSentencePrediction', 'FlaxAutoModelForPreTraining', 'FlaxAutoModelForQuestionAnswering', 'FlaxAutoModelForSeq2SeqLM', 'FlaxAutoModelForSequenceClassification', 'FlaxAutoModelForSpeechSeq2Seq', 'FlaxAutoModelForTokenClassification', 'FlaxAutoModelForVision2Seq', 'FlaxBartDecoderPreTrainedModel', 'FlaxBartForCausalLM', 'FlaxBartForConditionalGeneration', 'FlaxBartForQuestionAnswering', 'FlaxBartForSequenceClassification', 'FlaxBartModel', 'FlaxBartPreTrainedModel', 'FlaxBeitForImageClassification', 'FlaxBeitForMaskedImageModeling', 'FlaxBeitModel', 'FlaxBeitPreTrainedModel', 'FlaxBertForCausalLM', 'FlaxBertForMaskedLM', 'FlaxBertForMultipleChoice', 'FlaxBertForNextSentencePrediction', 'FlaxBertForPreTraining', 'FlaxBertForQuestionAnswering', 'FlaxBertForSequenceClassification', 'FlaxBertForTokenClassification', 'FlaxBertModel', 'FlaxBertPreTrainedModel', 'FlaxBigBirdForCausalLM', 'FlaxBigBirdForMaskedLM', 'FlaxBigBirdForMultipleChoice', 'FlaxBigBirdForPreTraining', 'FlaxBigBirdForQuestionAnswering', 'FlaxBigBirdForSequenceClassification', 'FlaxBigBirdForTokenClassification', 'FlaxBigBirdModel', 'FlaxBigBirdPreTrainedModel', 'FlaxBlenderbotForConditionalGeneration', 'FlaxBlenderbotModel', 'FlaxBlenderbotPreTrainedModel', 'FlaxBlenderbotSmallForConditionalGeneration', 'FlaxBlenderbotSmallModel', 'FlaxBlenderbotSmallPreTrainedModel', 'FlaxBloomForCausalLM', 'FlaxBloomModel', 'FlaxBloomPreTrainedModel', 'FlaxCLIPModel', 'FlaxCLIPPreTrainedModel', 'FlaxCLIPTextModel', 'FlaxCLIPTextModelWithProjection', 'FlaxCLIPTextPreTrainedModel', 'FlaxCLIPVisionModel', 'FlaxCLIPVisionPreTrainedModel', 'FlaxDinov2ForImageClassification', 'FlaxDinov2Model', 'FlaxDinov2PreTrainedModel', 'FlaxDistilBertForMaskedLM', 'FlaxDistilBertForMultipleChoice', 'FlaxDistilBertForQuestionAnswering', 'FlaxDistilBertForSequenceClassification', 'FlaxDistilBertForTokenClassification', 'FlaxDistilBertModel', 'FlaxDistilBertPreTrainedModel', 'FlaxElectraForCausalLM', 'FlaxElectraForMaskedLM', 'FlaxElectraForMultipleChoice', 'FlaxElectraForPreTraining', 'FlaxElectraForQuestionAnswering', 'FlaxElectraForSequenceClassification', 'FlaxElectraForTokenClassification', 'FlaxElectraModel', 'FlaxElectraPreTrainedModel', 'FlaxEncoderDecoderModel', 'FlaxForceTokensLogitsProcessor', 'FlaxForcedBOSTokenLogitsProcessor', 'FlaxForcedEOSTokenLogitsProcessor', 'FlaxGPT2LMHeadModel', 'FlaxGPT2Model', 'FlaxGPT2PreTrainedModel', 'FlaxGPTJForCausalLM', 'FlaxGPTJModel', 'FlaxGPTJPreTrainedModel', 'FlaxGPTNeoForCausalLM', 'FlaxGPTNeoModel', 'FlaxGPTNeoPreTrainedModel', 'FlaxGemmaForCausalLM', 'FlaxGemmaModel', 'FlaxGemmaPreTrainedModel', 'FlaxGenerationMixin', 'FlaxLlamaForCausalLM', 'FlaxLlamaModel', 'FlaxLlamaPreTrainedModel', 'FlaxLogitsProcessor', 'FlaxLogitsProcessorList', 'FlaxLogitsWarper', 'FlaxLongT5ForConditionalGeneration', 'FlaxLongT5Model', 'FlaxLongT5PreTrainedModel', 'FlaxMBartForConditionalGeneration', 'FlaxMBartForQuestionAnswering', 'FlaxMBartForSequenceClassification', 'FlaxMBartModel', 'FlaxMBartPreTrainedModel', 'FlaxMT5EncoderModel', 'FlaxMT5ForConditionalGeneration', 'FlaxMT5Model', 'FlaxMarianMTModel', 'FlaxMarianModel', 'FlaxMarianPreTrainedModel', 'FlaxMinLengthLogitsProcessor', 'FlaxMistralForCausalLM', 'FlaxMistralModel', 'FlaxMistralPreTrainedModel', 'FlaxOPTForCausalLM', 'FlaxOPTModel', 'FlaxOPTPreTrainedModel', 'FlaxPegasusForConditionalGeneration', 'FlaxPegasusModel', 'FlaxPegasusPreTrainedModel', 'FlaxPreTrainedModel', 'FlaxRegNetForImageClassification', 'FlaxRegNetModel', 'FlaxRegNetPreTrainedModel', 'FlaxResNetForImageClassification', 'FlaxResNetModel', 'FlaxResNetPreTrainedModel', 'FlaxRoFormerForMaskedLM', 'FlaxRoFormerForMultipleChoice', 'FlaxRoFormerForQuestionAnswering', 'FlaxRoFormerForSequenceClassification', 'FlaxRoFormerForTokenClassification', 'FlaxRoFormerModel', 'FlaxRoFormerPreTrainedModel', 'FlaxRobertaForCausalLM', 'FlaxRobertaForMaskedLM', 'FlaxRobertaForMultipleChoice', 'FlaxRobertaForQuestionAnswering', 'FlaxRobertaForSequenceClassification', 'FlaxRobertaForTokenClassification', 'FlaxRobertaModel', 'FlaxRobertaPreLayerNormForCausalLM', 'FlaxRobertaPreLayerNormForMaskedLM', 'FlaxRobertaPreLayerNormForMultipleChoice', 'FlaxRobertaPreLayerNormForQuestionAnswering', 'FlaxRobertaPreLayerNormForSequenceClassification', 'FlaxRobertaPreLayerNormForTokenClassification', 'FlaxRobertaPreLayerNormModel', 'FlaxRobertaPreLayerNormPreTrainedModel', 'FlaxRobertaPreTrainedModel', 'FlaxSpeechEncoderDecoderModel', 'FlaxSuppressTokensAtBeginLogitsProcessor', 'FlaxSuppressTokensLogitsProcessor', 'FlaxT5EncoderModel', 'FlaxT5ForConditionalGeneration', 'FlaxT5Model', 'FlaxT5PreTrainedModel', 'FlaxTemperatureLogitsWarper', 'FlaxTopKLogitsWarper', 'FlaxTopPLogitsWarper', 'FlaxViTForImageClassification', 'FlaxViTModel', 'FlaxViTPreTrainedModel', 'FlaxVisionEncoderDecoderModel', 'FlaxVisionTextDualEncoderModel', 'FlaxWav2Vec2ForCTC', 'FlaxWav2Vec2ForPreTraining', 'FlaxWav2Vec2Model', 'FlaxWav2Vec2PreTrainedModel', 'FlaxWhisperForAudioClassification', 'FlaxWhisperForConditionalGeneration', 'FlaxWhisperModel', 'FlaxWhisperPreTrainedModel', 'FlaxWhisperTimeStampLogitsProcessor', 'FlaxXGLMForCausalLM', 'FlaxXGLMModel', 'FlaxXGLMPreTrainedModel', 'FlaxXLMRobertaForCausalLM', 'FlaxXLMRobertaForMaskedLM', 'FlaxXLMRobertaForMultipleChoice', 'FlaxXLMRobertaForQuestionAnswering', 'FlaxXLMRobertaForSequenceClassification', 'FlaxXLMRobertaForTokenClassification', 'FlaxXLMRobertaModel', 'FlaxXLMRobertaPreTrainedModel', 'FocalNetBackbone', 'FocalNetConfig', 'FocalNetForImageClassification', 'FocalNetForMaskedImageModeling', 'FocalNetModel', 'FocalNetPreTrainedModel', 'ForcedBOSTokenLogitsProcessor', 'ForcedEOSTokenLogitsProcessor', 'FunnelBaseModel', 'FunnelConfig', 'FunnelForMaskedLM', 'FunnelForMultipleChoice', 'FunnelForPreTraining', 'FunnelForQuestionAnswering', 'FunnelForSequenceClassification', 'FunnelForTokenClassification', 'FunnelModel', 'FunnelPreTrainedModel', 'FunnelTokenizer', 'FunnelTokenizerFast', 'FuyuConfig', 'FuyuForCausalLM', 'FuyuImageProcessor', 'FuyuModel', 'FuyuPreTrainedModel', 'FuyuProcessor', 'GLPNConfig', 'GLPNFeatureExtractor', 'GLPNForDepthEstimation', 'GLPNImageProcessor', 'GLPNLayer', 'GLPNModel', 'GLPNPreTrainedModel', 'GPT2Config', 'GPT2DoubleHeadsModel', 'GPT2ForQuestionAnswering', 'GPT2ForSequenceClassification', 'GPT2ForTokenClassification', 'GPT2LMHeadModel', 'GPT2Model', 'GPT2OnnxConfig', 'GPT2PreTrainedModel', 'GPT2Tokenizer', 'GPT2TokenizerFast', 'GPTBigCodeConfig', 'GPTBigCodeForCausalLM', 'GPTBigCodeForSequenceClassification', 'GPTBigCodeForTokenClassification', 'GPTBigCodeModel', 'GPTBigCodePreTrainedModel', 'GPTJConfig', 'GPTJForCausalLM', 'GPTJForQuestionAnswering', 'GPTJForSequenceClassification', 'GPTJModel', 'GPTJOnnxConfig', 'GPTJPreTrainedModel', 'GPTNeoConfig', 'GPTNeoForCausalLM', 'GPTNeoForQuestionAnswering', 'GPTNeoForSequenceClassification', 'GPTNeoForTokenClassification', 'GPTNeoModel', 'GPTNeoOnnxConfig', 'GPTNeoPreTrainedModel', 'GPTNeoXConfig', 'GPTNeoXForCausalLM', 'GPTNeoXForQuestionAnswering', 'GPTNeoXForSequenceClassification', 'GPTNeoXForTokenClassification', 'GPTNeoXJapaneseConfig', 'GPTNeoXJapaneseForCausalLM', 'GPTNeoXJapaneseLayer', 'GPTNeoXJapaneseModel', 'GPTNeoXJapanesePreTrainedModel', 'GPTNeoXJapaneseTokenizer', 'GPTNeoXLayer', 'GPTNeoXModel', 'GPTNeoXPreTrainedModel', 'GPTNeoXTokenizerFast', 'GPTQConfig', 'GPTSanJapaneseConfig', 'GPTSanJapaneseForConditionalGeneration', 'GPTSanJapaneseModel', 'GPTSanJapanesePreTrainedModel', 'GPTSanJapaneseTokenizer', 'GPTSw3Tokenizer', 'Gemma2Config', 'Gemma2ForCausalLM', 'Gemma2ForSequenceClassification', 'Gemma2ForTokenClassification', 'Gemma2Model', 'Gemma2PreTrainedModel', 'Gemma3Config', 'Gemma3ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForSequenceClassification', 'Gemma3ImageProcessor', 'Gemma3ImageProcessorFast', 'Gemma3Model', 'Gemma3PreTrainedModel', 'Gemma3Processor', 'Gemma3TextConfig', 'Gemma3TextModel', 'Gemma3nAudioConfig', 'Gemma3nAudioEncoder', 'Gemma3nAudioFeatureExtractor', 'Gemma3nConfig', 'Gemma3nForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nModel', 'Gemma3nPreTrainedModel', 'Gemma3nProcessor', 'Gemma3nTextConfig', 'Gemma3nTextModel', 'Gemma3nVisionConfig', 'GemmaConfig', 'GemmaForCausalLM', 'GemmaForSequenceClassification', 'GemmaForTokenClassification', 'GemmaModel', 'GemmaPreTrainedModel', 'GemmaTokenizer', 'GemmaTokenizerFast', 'GenerationConfig', 'GenerationMixin', 'GitConfig', 'GitForCausalLM', 'GitModel', 'GitPreTrainedModel', 'GitProcessor', 'GitVisionConfig', 'GitVisionModel', 'Glm4Config', 'Glm4ForCausalLM', 'Glm4ForSequenceClassification', 'Glm4ForTokenClassification', 'Glm4Model', 'Glm4MoeConfig', 'Glm4MoeForCausalLM', 'Glm4MoeModel', 'Glm4MoePreTrainedModel', 'Glm4PreTrainedModel', 'Glm4vConfig', 'Glm4vForConditionalGeneration', 'Glm4vImageProcessor', 'Glm4vImageProcessorFast', 'Glm4vModel', 'Glm4vPreTrainedModel', 'Glm4vProcessor', 'Glm4vTextConfig', 'Glm4vTextModel', 'Glm4vVideoProcessor', 'GlmConfig', 'GlmForCausalLM', 'GlmForSequenceClassification', 'GlmForTokenClassification', 'GlmModel', 'GlmPreTrainedModel', 'GlueDataTrainingArguments', 'GlueDataset', 'GotOcr2Config', 'GotOcr2ForConditionalGeneration', 'GotOcr2ImageProcessor', 'GotOcr2ImageProcessorFast', 'GotOcr2Model', 'GotOcr2PreTrainedModel', 'GotOcr2Processor', 'GotOcr2VisionConfig', 'GptOssConfig', 'GptOssForCausalLM', 'GptOssModel', 'GptOssPreTrainedModel', 'GradientAccumulator', 'GradientCheckpointingLayer', 'GraniteConfig', 'GraniteForCausalLM', 'GraniteModel', 'GraniteMoeConfig', 'GraniteMoeForCausalLM', 'GraniteMoeHybridConfig', 'GraniteMoeHybridForCausalLM', 'GraniteMoeHybridModel', 'GraniteMoeHybridPreTrainedModel', 'GraniteMoeModel', 'GraniteMoePreTrainedModel', 'GraniteMoeSharedConfig', 'GraniteMoeSharedForCausalLM', 'GraniteMoeSharedModel', 'GraniteMoeSharedPreTrainedModel', 'GranitePreTrainedModel', 'GraniteSpeechCTCEncoder', 'GraniteSpeechConfig', 'GraniteSpeechEncoderConfig', 'GraniteSpeechFeatureExtractor', 'GraniteSpeechForConditionalGeneration', 'GraniteSpeechPreTrainedModel', 'GraniteSpeechProcessor', 'GraphormerConfig', 'GraphormerForGraphClassification', 'GraphormerModel', 'GraphormerPreTrainedModel', 'GroundingDinoConfig', 'GroundingDinoForObjectDetection', 'GroundingDinoImageProcessor', 'GroundingDinoImageProcessorFast', 'GroundingDinoModel', 'GroundingDinoPreTrainedModel', 'GroundingDinoProcessor', 'GroupViTConfig', 'GroupViTModel', 'GroupViTOnnxConfig', 'GroupViTPreTrainedModel', 'GroupViTTextConfig', 'GroupViTTextModel', 'GroupViTVisionConfig', 'GroupViTVisionModel', 'HGNetV2Backbone', 'HGNetV2Config', 'HGNetV2ForImageClassification', 'HGNetV2PreTrainedModel', 'HQQQuantizedCache', 'HQQQuantizedCacheProcessor', 'HammingDiversityLogitsProcessor', 'HeliumConfig', 'HeliumForCausalLM', 'HeliumForSequenceClassification', 'HeliumForTokenClassification', 'HeliumModel', 'HeliumPreTrainedModel', 'HerbertTokenizer', 'HerbertTokenizerFast', 'HfArgumentParser', 'HieraBackbone', 'HieraConfig', 'HieraForImageClassification', 'HieraForPreTraining', 'HieraModel', 'HieraPreTrainedModel', 'HiggsConfig', 'HqqConfig', 'HubertConfig', 'HubertForCTC', 'HubertForSequenceClassification', 'HubertModel', 'HubertPreTrainedModel', 'HybridCache', 'HybridChunkedCache', 'IBertConfig', 'IBertForMaskedLM', 'IBertForMultipleChoice', 'IBertForQuestionAnswering', 'IBertForSequenceClassification', 'IBertForTokenClassification', 'IBertModel', 'IBertOnnxConfig', 'IBertPreTrainedModel', 'IJepaConfig', 'IJepaForImageClassification', 'IJepaModel', 'IJepaPreTrainedModel', 'IMAGE_PROCESSOR_MAPPING', 'Idefics2Config', 'Idefics2ForConditionalGeneration', 'Idefics2ImageProcessor', 'Idefics2ImageProcessorFast', 'Idefics2Model', 'Idefics2PreTrainedModel', 'Idefics2Processor', 'Idefics3Config', 'Idefics3ForConditionalGeneration', 'Idefics3ImageProcessor', 'Idefics3ImageProcessorFast', 'Idefics3Model', 'Idefics3PreTrainedModel', 'Idefics3Processor', 'Idefics3VisionConfig', 'Idefics3VisionTransformer', 'IdeficsConfig', 'IdeficsForVisionText2Text', 'IdeficsImageProcessor', 'IdeficsModel', 'IdeficsPreTrainedModel', 'IdeficsProcessor', 'ImageClassificationPipeline', 'ImageFeatureExtractionMixin', 'ImageFeatureExtractionPipeline', 'ImageGPTConfig', 'ImageGPTFeatureExtractor', 'ImageGPTForCausalImageModeling', 'ImageGPTForImageClassification', 'ImageGPTImageProcessor', 'ImageGPTModel', 'ImageGPTOnnxConfig', 'ImageGPTPreTrainedModel', 'ImageProcessingMixin', 'ImageSegmentationPipeline', 'ImageTextToTextPipeline', 'ImageToImagePipeline', 'ImageToTextPipeline', 'InfNanRemoveLogitsProcessor', 'InformerConfig', 'InformerForPrediction', 'InformerModel', 'InformerPreTrainedModel', 'InputExample', 'InputFeatures', 'InstructBlipConfig', 'InstructBlipForConditionalGeneration', 'InstructBlipModel', 'InstructBlipPreTrainedModel', 'InstructBlipProcessor', 'InstructBlipQFormerConfig', 'InstructBlipQFormerModel', 'InstructBlipVideoConfig', 'InstructBlipVideoForConditionalGeneration', 'InstructBlipVideoImageProcessor', 'InstructBlipVideoModel', 'InstructBlipVideoPreTrainedModel', 'InstructBlipVideoProcessor', 'InstructBlipVideoQFormerConfig', 'InstructBlipVideoQFormerModel', 'InstructBlipVideoVideoProcessor', 'InstructBlipVideoVisionConfig', 'InstructBlipVideoVisionModel', 'InstructBlipVisionConfig', 'InstructBlipVisionModel', 'InternVLConfig', 'InternVLForConditionalGeneration', 'InternVLModel', 'InternVLPreTrainedModel', 'InternVLProcessor', 'InternVLVideoProcessor', 'InternVLVisionConfig', 'InternVLVisionModel', 'InternVLVisionPreTrainedModel', 'IntervalStrategy', 'JambaConfig', 'JambaForCausalLM', 'JambaForSequenceClassification', 'JambaModel', 'JambaPreTrainedModel', 'JanusConfig', 'JanusForConditionalGeneration', 'JanusImageProcessor', 'JanusImageProcessorFast', 'JanusModel', 'JanusPreTrainedModel', 'JanusProcessor', 'JanusVQVAE', 'JanusVQVAEConfig', 'JanusVisionConfig', 'JanusVisionModel', 'JetMoeConfig', 'JetMoeForCausalLM', 'JetMoeForSequenceClassification', 'JetMoeModel', 'JetMoePreTrainedModel', 'JsonPipelineDataFormat', 'JukeboxConfig', 'JukeboxModel', 'JukeboxPreTrainedModel', 'JukeboxPrior', 'JukeboxPriorConfig', 'JukeboxTokenizer', 'JukeboxVQVAE', 'JukeboxVQVAEConfig', 'KerasMetricCallback', 'Kosmos2Config', 'Kosmos2ForConditionalGeneration', 'Kosmos2Model', 'Kosmos2PreTrainedModel', 'Kosmos2Processor', 'KyutaiSpeechToTextConfig', 'KyutaiSpeechToTextFeatureExtractor', 'KyutaiSpeechToTextForConditionalGeneration', 'KyutaiSpeechToTextModel', 'KyutaiSpeechToTextPreTrainedModel', 'KyutaiSpeechToTextProcessor', 'LEDConfig', 'LEDForConditionalGeneration', 'LEDForQuestionAnswering', 'LEDForSequenceClassification', 'LEDModel', 'LEDPreTrainedModel', 'LEDTokenizer', 'LEDTokenizerFast', 'LayoutLMConfig', 'LayoutLMForMaskedLM', 'LayoutLMForQuestionAnswering', 'LayoutLMForSequenceClassification', 'LayoutLMForTokenClassification', 'LayoutLMModel', 'LayoutLMOnnxConfig', 'LayoutLMPreTrainedModel', 'LayoutLMTokenizer', 'LayoutLMTokenizerFast', 'LayoutLMv2Config', 'LayoutLMv2FeatureExtractor', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv2ImageProcessor', 'LayoutLMv2ImageProcessorFast', 'LayoutLMv2Layer', 'LayoutLMv2Model', 'LayoutLMv2PreTrainedModel', 'LayoutLMv2Processor', 'LayoutLMv2Tokenizer', 'LayoutLMv2TokenizerFast', 'LayoutLMv3Config', 'LayoutLMv3FeatureExtractor', 'LayoutLMv3ForQuestionAnswering', 'LayoutLMv3ForSequenceClassification', 'LayoutLMv3ForTokenClassification', 'LayoutLMv3ImageProcessor', 'LayoutLMv3ImageProcessorFast', 'LayoutLMv3Model', 'LayoutLMv3OnnxConfig', 'LayoutLMv3PreTrainedModel', 'LayoutLMv3Processor', 'LayoutLMv3Tokenizer', 'LayoutLMv3TokenizerFast', 'LayoutXLMProcessor', 'LayoutXLMTokenizer', 'LayoutXLMTokenizerFast', 'LevitConfig', 'LevitFeatureExtractor', 'LevitForImageClassification', 'LevitForImageClassificationWithTeacher', 'LevitImageProcessor', 'LevitImageProcessorFast', 'LevitModel', 'LevitOnnxConfig', 'LevitPreTrainedModel', 'Lfm2Config', 'Lfm2ForCausalLM', 'Lfm2Model', 'Lfm2PreTrainedModel', 'LightGlueConfig', 'LightGlueForKeypointMatching', 'LightGlueImageProcessor', 'LightGluePreTrainedModel', 'LiltConfig', 'LiltForQuestionAnswering', 'LiltForSequenceClassification', 'LiltForTokenClassification', 'LiltModel', 'LiltPreTrainedModel', 'LineByLineTextDataset', 'LineByLineWithRefDataset', 'LineByLineWithSOPTextDataset', 'Llama4Config', 'Llama4ForCausalLM', 'Llama4ForConditionalGeneration', 'Llama4ImageProcessorFast', 'Llama4PreTrainedModel', 'Llama4Processor', 'Llama4TextConfig', 'Llama4TextModel', 'Llama4VisionConfig', 'Llama4VisionModel', 'LlamaConfig', 'LlamaForCausalLM', 'LlamaForQuestionAnswering', 'LlamaForSequenceClassification', 'LlamaForTokenClassification', 'LlamaModel', 'LlamaPreTrainedModel', 'LlamaTokenizer', 'LlamaTokenizerFast', 'LlavaConfig', 'LlavaForConditionalGeneration', 'LlavaImageProcessor', 'LlavaImageProcessorFast', 'LlavaModel', 'LlavaNextConfig', 'LlavaNextForConditionalGeneration', 'LlavaNextImageProcessor', 'LlavaNextImageProcessorFast', 'LlavaNextModel', 'LlavaNextPreTrainedModel', 'LlavaNextProcessor', 'LlavaNextVideoConfig', 'LlavaNextVideoForConditionalGeneration', 'LlavaNextVideoImageProcessor', 'LlavaNextVideoModel', 'LlavaNextVideoPreTrainedModel', 'LlavaNextVideoProcessor', 'LlavaNextVideoVideoProcessor', 'LlavaOnevisionConfig', 'LlavaOnevisionForConditionalGeneration', 'LlavaOnevisionImageProcessor', 'LlavaOnevisionImageProcessorFast', 'LlavaOnevisionModel', 'LlavaOnevisionPreTrainedModel', 'LlavaOnevisionProcessor', 'LlavaOnevisionVideoProcessor', 'LlavaPreTrainedModel', 'LlavaProcessor', 'LogitNormalization', 'LogitsProcessor', 'LogitsProcessorList', 'LongT5Config', 'LongT5EncoderModel', 'LongT5ForConditionalGeneration', 'LongT5Model', 'LongT5OnnxConfig', 'LongT5PreTrainedModel', 'LongformerConfig', 'LongformerForMaskedLM', 'LongformerForMultipleChoice', 'LongformerForQuestionAnswering', 'LongformerForSequenceClassification', 'LongformerForTokenClassification', 'LongformerModel', 'LongformerOnnxConfig', 'LongformerPreTrainedModel', 'LongformerSelfAttention', 'LongformerTokenizer', 'LongformerTokenizerFast', 'LukeConfig', 'LukeForEntityClassification', 'LukeForEntityPairClassification', 'LukeForEntitySpanClassification', 'LukeForMaskedLM', 'LukeForMultipleChoice', 'LukeForQuestionAnswering', 'LukeForSequenceClassification', 'LukeForTokenClassification', 'LukeModel', 'LukePreTrainedModel', 'LukeTokenizer', 'LxmertConfig', 'LxmertEncoder', 'LxmertForPreTraining', 'LxmertForQuestionAnswering', 'LxmertModel', 'LxmertPreTrainedModel', 'LxmertTokenizer', 'LxmertTokenizerFast', 'LxmertVisualFeatureEncoder', 'LxmertXLayer', 'M2M100Config', 'M2M100ForConditionalGeneration', 'M2M100Model', 'M2M100OnnxConfig', 'M2M100PreTrainedModel', 'M2M100Tokenizer', 'MBart50Tokenizer', 'MBart50TokenizerFast', 'MBartConfig', 'MBartForCausalLM', 'MBartForConditionalGeneration', 'MBartForQuestionAnswering', 'MBartForSequenceClassification', 'MBartModel', 'MBartOnnxConfig', 'MBartPreTrainedModel', 'MBartTokenizer', 'MBartTokenizerFast', 'MCTCTConfig', 'MCTCTFeatureExtractor', 'MCTCTForCTC', 'MCTCTModel', 'MCTCTPreTrainedModel', 'MCTCTProcessor', 'MLCDPreTrainedModel', 'MLCDVisionConfig', 'MLCDVisionModel', 'MLukeTokenizer', 'MMBTConfig', 'MMBTForClassification', 'MMBTModel', 'MMGroundingDinoConfig', 'MMGroundingDinoForObjectDetection', 'MMGroundingDinoModel', 'MMGroundingDinoPreTrainedModel', 'MODEL_CARD_NAME', 'MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING', 'MODEL_FOR_AUDIO_TOKENIZATION_MAPPING', 'MODEL_FOR_AUDIO_XVECTOR_MAPPING', 'MODEL_FOR_BACKBONE_MAPPING', 'MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING', 'MODEL_FOR_CAUSAL_LM_MAPPING', 'MODEL_FOR_CTC_MAPPING', 'MODEL_FOR_DEPTH_ESTIMATION_MAPPING', 'MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'MODEL_FOR_IMAGE_MAPPING', 'MODEL_FOR_IMAGE_SEGMENTATION_MAPPING', 'MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING', 'MODEL_FOR_IMAGE_TO_IMAGE_MAPPING', 'MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING', 'MODEL_FOR_KEYPOINT_DETECTION_MAPPING', 'MODEL_FOR_KEYPOINT_MATCHING_MAPPING', 'MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING', 'MODEL_FOR_MASKED_LM_MAPPING', 'MODEL_FOR_MASK_GENERATION_MAPPING', 'MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'MODEL_FOR_OBJECT_DETECTION_MAPPING', 'MODEL_FOR_PRETRAINING_MAPPING', 'MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_RETRIEVAL_MAPPING', 'MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING', 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_TEXT_ENCODING_MAPPING', 'MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING', 'MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING', 'MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING', 'MODEL_FOR_TIME_SERIES_PREDICTION_MAPPING', 'MODEL_FOR_TIME_SERIES_REGRESSION_MAPPING', 'MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING', 'MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING', 'MODEL_FOR_VISION_2_SEQ_MAPPING', 'MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING', 'MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING', 'MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING', 'MODEL_MAPPING', 'MODEL_NAMES_MAPPING', 'MODEL_WITH_LM_HEAD_MAPPING', 'MPNetConfig', 'MPNetForMaskedLM', 'MPNetForMultipleChoice', 'MPNetForQuestionAnswering', 'MPNetForSequenceClassification', 'MPNetForTokenClassification', 'MPNetLayer', 'MPNetModel', 'MPNetPreTrainedModel', 'MPNetTokenizer', 'MPNetTokenizerFast', 'MT5Config', 'MT5EncoderModel', 'MT5ForConditionalGeneration', 'MT5ForQuestionAnswering', 'MT5ForSequenceClassification', 'MT5ForTokenClassification', 'MT5Model', 'MT5OnnxConfig', 'MT5PreTrainedModel', 'MT5Tokenizer', 'MT5TokenizerFast', 'Mamba2Config', 'Mamba2ForCausalLM', 'Mamba2Model', 'Mamba2PreTrainedModel', 'MambaCache', 'MambaConfig', 'MambaForCausalLM', 'MambaModel', 'MambaPreTrainedModel', 'MarianConfig', 'MarianForCausalLM', 'MarianMTModel', 'MarianModel', 'MarianOnnxConfig', 'MarianPreTrainedModel', 'MarianTokenizer', 'MarkupLMConfig', 'MarkupLMFeatureExtractor', 'MarkupLMForQuestionAnswering', 'MarkupLMForSequenceClassification', 'MarkupLMForTokenClassification', 'MarkupLMModel', 'MarkupLMPreTrainedModel', 'MarkupLMProcessor', 'MarkupLMTokenizer', 'MarkupLMTokenizerFast', 'Mask2FormerConfig', 'Mask2FormerForUniversalSegmentation', 'Mask2FormerImageProcessor', 'Mask2FormerImageProcessorFast', 'Mask2FormerModel', 'Mask2FormerPreTrainedModel', 'MaskFormerConfig', 'MaskFormerFeatureExtractor', 'MaskFormerForInstanceSegmentation', 'MaskFormerImageProcessor', 'MaskFormerImageProcessorFast', 'MaskFormerModel', 'MaskFormerPreTrainedModel', 'MaskFormerSwinBackbone', 'MaskFormerSwinConfig', 'MaskFormerSwinModel', 'MaskFormerSwinPreTrainedModel', 'MaskGenerationPipeline', 'MaxLengthCriteria', 'MaxTimeCriteria', 'MecabTokenizer', 'MegaConfig', 'MegaForCausalLM', 'MegaForMaskedLM', 'MegaForMultipleChoice', 'MegaForQuestionAnswering', 'MegaForSequenceClassification', 'MegaForTokenClassification', 'MegaModel', 'MegaOnnxConfig', 'MegaPreTrainedModel', 'MegatronBertConfig', 'MegatronBertForCausalLM', 'MegatronBertForMaskedLM', 'MegatronBertForMultipleChoice', 'MegatronBertForNextSentencePrediction', 'MegatronBertForPreTraining', 'MegatronBertForQuestionAnswering', 'MegatronBertForSequenceClassification', 'MegatronBertForTokenClassification', 'MegatronBertModel', 'MegatronBertPreTrainedModel', 'MgpstrConfig', 'MgpstrForSceneTextRecognition', 'MgpstrModel', 'MgpstrPreTrainedModel', 'MgpstrProcessor', 'MgpstrTokenizer', 'MimiConfig', 'MimiModel', 'MimiPreTrainedModel', 'MinLengthLogitsProcessor', 'MinNewTokensLengthLogitsProcessor', 'MinPLogitsWarper', 'MiniMaxConfig', 'MiniMaxForCausalLM', 'MiniMaxForQuestionAnswering', 'MiniMaxForSequenceClassification', 'MiniMaxForTokenClassification', 'MiniMaxModel', 'MiniMaxPreTrainedModel', 'Mistral3Config', 'Mistral3ForConditionalGeneration', 'Mistral3Model', 'Mistral3PreTrainedModel', 'MistralCommonTokenizer', 'MistralConfig', 'MistralForCausalLM', 'MistralForQuestionAnswering', 'MistralForSequenceClassification', 'MistralForTokenClassification', 'MistralModel', 'MistralPreTrainedModel', 'MixtralConfig', 'MixtralForCausalLM', 'MixtralForQuestionAnswering', 'MixtralForSequenceClassification', 'MixtralForTokenClassification', 'MixtralModel', 'MixtralPreTrainedModel', 'MllamaConfig', 'MllamaForCausalLM', 'MllamaForConditionalGeneration', 'MllamaImageProcessor', 'MllamaModel', 'MllamaPreTrainedModel', 'MllamaProcessor', 'MllamaTextModel', 'MllamaVisionModel', 'MobileBertConfig', 'MobileBertForMaskedLM', 'MobileBertForMultipleChoice', 'MobileBertForNextSentencePrediction', 'MobileBertForPreTraining', 'MobileBertForQuestionAnswering', 'MobileBertForSequenceClassification', 'MobileBertForTokenClassification', 'MobileBertLayer', 'MobileBertModel', 'MobileBertOnnxConfig', 'MobileBertPreTrainedModel', 'MobileBertTokenizer', 'MobileBertTokenizerFast', 'MobileNetV1Config', 'MobileNetV1FeatureExtractor', 'MobileNetV1ForImageClassification', 'MobileNetV1ImageProcessor', 'MobileNetV1ImageProcessorFast', 'MobileNetV1Model', 'MobileNetV1OnnxConfig', 'MobileNetV1PreTrainedModel', 'MobileNetV2Config', 'MobileNetV2FeatureExtractor', 'MobileNetV2ForImageClassification', 'MobileNetV2ForSemanticSegmentation', 'MobileNetV2ImageProcessor', 'MobileNetV2ImageProcessorFast', 'MobileNetV2Model', 'MobileNetV2OnnxConfig', 'MobileNetV2PreTrainedModel', 'MobileViTConfig', 'MobileViTFeatureExtractor', 'MobileViTForImageClassification', 'MobileViTForSemanticSegmentation', 'MobileViTImageProcessor', 'MobileViTImageProcessorFast', 'MobileViTModel', 'MobileViTOnnxConfig', 'MobileViTPreTrainedModel', 'MobileViTV2Config', 'MobileViTV2ForImageClassification', 'MobileViTV2ForSemanticSegmentation', 'MobileViTV2Model', 'MobileViTV2OnnxConfig', 'MobileViTV2PreTrainedModel', 'ModalEmbeddings', 'ModelCard', 'ModernBertConfig', 'ModernBertDecoderConfig', 'ModernBertDecoderForCausalLM', 'ModernBertDecoderForSequenceClassification', 'ModernBertDecoderModel', 'ModernBertDecoderPreTrainedModel', 'ModernBertForMaskedLM', 'ModernBertForMultipleChoice', 'ModernBertForQuestionAnswering', 'ModernBertForSequenceClassification', 'ModernBertForTokenClassification', 'ModernBertModel', 'ModernBertPreTrainedModel', 'MoonshineConfig', 'MoonshineForConditionalGeneration', 'MoonshineModel', 'MoonshinePreTrainedModel', 'MoshiConfig', 'MoshiDepthConfig', 'MoshiForCausalLM', 'MoshiForConditionalGeneration', 'MoshiModel', 'MoshiPreTrainedModel', 'MptConfig', 'MptForCausalLM', 'MptForQuestionAnswering', 'MptForSequenceClassification', 'MptForTokenClassification', 'MptModel', 'MptPreTrainedModel', 'MraConfig', 'MraForMaskedLM', 'MraForMultipleChoice', 'MraForQuestionAnswering', 'MraForSequenceClassification', 'MraForTokenClassification', 'MraLayer', 'MraModel', 'MraPreTrainedModel', 'MusicgenConfig', 'MusicgenDecoderConfig', 'MusicgenForCausalLM', 'MusicgenForConditionalGeneration', 'MusicgenMelodyConfig', 'MusicgenMelodyDecoderConfig', 'MusicgenMelodyFeatureExtractor', 'MusicgenMelodyForCausalLM', 'MusicgenMelodyForConditionalGeneration', 'MusicgenMelodyModel', 'MusicgenMelodyPreTrainedModel', 'MusicgenMelodyProcessor', 'MusicgenModel', 'MusicgenPreTrainedModel', 'MusicgenProcessor', 'MvpConfig', 'MvpForCausalLM', 'MvpForConditionalGeneration', 'MvpForQuestionAnswering', 'MvpForSequenceClassification', 'MvpModel', 'MvpPreTrainedModel', 'MvpTokenizer', 'MvpTokenizerFast', 'Mxfp4Config', 'MyT5Tokenizer', 'NatBackbone', 'NatConfig', 'NatForImageClassification', 'NatModel', 'NatPreTrainedModel', 'NemotronConfig', 'NemotronForCausalLM', 'NemotronForQuestionAnswering', 'NemotronForSequenceClassification', 'NemotronForTokenClassification', 'NemotronModel', 'NemotronPreTrainedModel', 'NerPipeline', 'NezhaConfig', 'NezhaForMaskedLM', 'NezhaForMultipleChoice', 'NezhaForNextSentencePrediction', 'NezhaForPreTraining', 'NezhaForQuestionAnswering', 'NezhaForSequenceClassification', 'NezhaForTokenClassification', 'NezhaModel', 'NezhaPreTrainedModel', 'NllbMoeConfig', 'NllbMoeForConditionalGeneration', 'NllbMoeModel', 'NllbMoePreTrainedModel', 'NllbMoeSparseMLP', 'NllbMoeTop2Router', 'NllbTokenizer', 'NllbTokenizerFast', 'NoBadWordsLogitsProcessor', 'NoRepeatNGramLogitsProcessor', 'NougatImageProcessor', 'NougatImageProcessorFast', 'NougatProcessor', 'NougatTokenizerFast', 'NystromformerConfig', 'NystromformerForMaskedLM', 'NystromformerForMultipleChoice', 'NystromformerForQuestionAnswering', 'NystromformerForSequenceClassification', 'NystromformerForTokenClassification', 'NystromformerLayer', 'NystromformerModel', 'NystromformerPreTrainedModel', 'OPTConfig', 'OPTForCausalLM', 'OPTForQuestionAnswering', 'OPTForSequenceClassification', 'OPTModel', 'OPTPreTrainedModel', 'ObjectDetectionPipeline', 'OffloadedCache', 'OffloadedCacheProcessor', 'OffloadedStaticCache', 'Olmo2Config', 'Olmo2ForCausalLM', 'Olmo2Model', 'Olmo2PreTrainedModel', 'OlmoConfig', 'OlmoForCausalLM', 'OlmoModel', 'OlmoPreTrainedModel', 'OlmoeConfig', 'OlmoeForCausalLM', 'OlmoeModel', 'OlmoePreTrainedModel', 'OmDetTurboConfig', 'OmDetTurboForObjectDetection', 'OmDetTurboPreTrainedModel', 'OmDetTurboProcessor', 'OneFormerConfig', 'OneFormerForUniversalSegmentation', 'OneFormerImageProcessor', 'OneFormerImageProcessorFast', 'OneFormerModel', 'OneFormerPreTrainedModel', 'OneFormerProcessor', 'OpenAIGPTConfig', 'OpenAIGPTDoubleHeadsModel', 'OpenAIGPTForSequenceClassification', 'OpenAIGPTLMHeadModel', 'OpenAIGPTModel', 'OpenAIGPTPreTrainedModel', 'OpenAIGPTTokenizer', 'OpenAIGPTTokenizerFast', 'OpenLlamaConfig', 'OpenLlamaForCausalLM', 'OpenLlamaForSequenceClassification', 'OpenLlamaModel', 'OpenLlamaPreTrainedModel', 'OwlViTConfig', 'OwlViTFeatureExtractor', 'OwlViTForObjectDetection', 'OwlViTImageProcessor', 'OwlViTImageProcessorFast', 'OwlViTModel', 'OwlViTOnnxConfig', 'OwlViTPreTrainedModel', 'OwlViTProcessor', 'OwlViTTextConfig', 'OwlViTTextModel', 'OwlViTVisionConfig', 'OwlViTVisionModel', 'Owlv2Config', 'Owlv2ForObjectDetection', 'Owlv2ImageProcessor', 'Owlv2ImageProcessorFast', 'Owlv2Model', 'Owlv2PreTrainedModel', 'Owlv2Processor', 'Owlv2TextConfig', 'Owlv2TextModel', 'Owlv2VisionConfig', 'Owlv2VisionModel', 'PLBartConfig', 'PLBartForCausalLM', 'PLBartForConditionalGeneration', 'PLBartForSequenceClassification', 'PLBartModel', 'PLBartPreTrainedModel', 'PLBartTokenizer', 'PROCESSOR_MAPPING', 'PYTORCH_PRETRAINED_BERT_CACHE', 'PYTORCH_TRANSFORMERS_CACHE', 'PaliGemmaConfig', 'PaliGemmaForConditionalGeneration', 'PaliGemmaModel', 'PaliGemmaPreTrainedModel', 'PaliGemmaProcessor', 'PatchTSMixerConfig', 'PatchTSMixerForPrediction', 'PatchTSMixerForPretraining', 'PatchTSMixerForRegression', 'PatchTSMixerForTimeSeriesClassification', 'PatchTSMixerModel', 'PatchTSMixerPreTrainedModel', 'PatchTSTConfig', 'PatchTSTForClassification', 'PatchTSTForPrediction', 'PatchTSTForPretraining', 'PatchTSTForRegression', 'PatchTSTModel', 'PatchTSTPreTrainedModel', 'PegasusConfig', 'PegasusForCausalLM', 'PegasusForConditionalGeneration', 'PegasusModel', 'PegasusPreTrainedModel', 'PegasusTokenizer', 'PegasusTokenizerFast', 'PegasusXConfig', 'PegasusXForConditionalGeneration', 'PegasusXModel', 'PegasusXPreTrainedModel', 'PerceiverConfig', 'PerceiverFeatureExtractor', 'PerceiverForImageClassificationConvProcessing', 'PerceiverForImageClassificationFourier', 'PerceiverForImageClassificationLearned', 'PerceiverForMaskedLM', 'PerceiverForMultimodalAutoencoding', 'PerceiverForOpticalFlow', 'PerceiverForSequenceClassification', 'PerceiverImageProcessor', 'PerceiverImageProcessorFast', 'PerceiverLayer', 'PerceiverModel', 'PerceiverOnnxConfig', 'PerceiverPreTrainedModel', 'PerceiverTokenizer', 'PerceptionLMConfig', 'PerceptionLMForConditionalGeneration', 'PerceptionLMImageProcessorFast', 'PerceptionLMModel', 'PerceptionLMPreTrainedModel', 'PerceptionLMProcessor', 'PerceptionLMVideoProcessor', 'PersimmonConfig', 'PersimmonForCausalLM', 'PersimmonForSequenceClassification', 'PersimmonForTokenClassification', 'PersimmonModel', 'PersimmonPreTrainedModel', 'Phi3Config', 'Phi3ForCausalLM', 'Phi3ForSequenceClassification', 'Phi3ForTokenClassification', 'Phi3Model', 'Phi3PreTrainedModel', 'Phi4MultimodalAudioConfig', 'Phi4MultimodalAudioModel', 'Phi4MultimodalAudioPreTrainedModel', 'Phi4MultimodalConfig', 'Phi4MultimodalFeatureExtractor', 'Phi4MultimodalForCausalLM', 'Phi4MultimodalImageProcessorFast', 'Phi4MultimodalModel', 'Phi4MultimodalPreTrainedModel', 'Phi4MultimodalProcessor', 'Phi4MultimodalVisionConfig', 'Phi4MultimodalVisionModel', 'Phi4MultimodalVisionPreTrainedModel', 'PhiConfig', 'PhiForCausalLM', 'PhiForSequenceClassification', 'PhiForTokenClassification', 'PhiModel', 'PhiPreTrainedModel', 'PhimoeConfig', 'PhimoeForCausalLM', 'PhimoeForSequenceClassification', 'PhimoeModel', 'PhimoePreTrainedModel', 'PhobertTokenizer', 'PhrasalConstraint', 'PipedPipelineDataFormat', 'Pipeline', 'PipelineDataFormat', 'Pix2StructConfig', 'Pix2StructForConditionalGeneration', 'Pix2StructImageProcessor', 'Pix2StructPreTrainedModel', 'Pix2StructProcessor', 'Pix2StructTextConfig', 'Pix2StructTextModel', 'Pix2StructVisionConfig', 'Pix2StructVisionModel', 'PixtralImageProcessor', 'PixtralImageProcessorFast', 'PixtralPreTrainedModel', 'PixtralProcessor', 'PixtralVisionConfig', 'PixtralVisionModel', 'PoolFormerConfig', 'PoolFormerFeatureExtractor', 'PoolFormerForImageClassification', 'PoolFormerImageProcessor', 'PoolFormerImageProcessorFast', 'PoolFormerModel', 'PoolFormerOnnxConfig', 'PoolFormerPreTrainedModel', 'Pop2PianoConfig', 'Pop2PianoFeatureExtractor', 'Pop2PianoForConditionalGeneration', 'Pop2PianoPreTrainedModel', 'Pop2PianoProcessor', 'Pop2PianoTokenizer', 'PreTrainedModel', 'PreTrainedTokenizer', 'PreTrainedTokenizerBase', 'PreTrainedTokenizerFast', 'PrefixConstrainedLogitsProcessor', 'PretrainedBartModel', 'PretrainedConfig', 'PretrainedFSMTModel', 'PrinterCallback', 'ProcessorMixin', 'ProgressCallback', 'PromptDepthAnythingConfig', 'PromptDepthAnythingForDepthEstimation', 'PromptDepthAnythingImageProcessor', 'PromptDepthAnythingPreTrainedModel', 'ProphetNetConfig', 'ProphetNetDecoder', 'ProphetNetEncoder', 'ProphetNetForCausalLM', 'ProphetNetForConditionalGeneration', 'ProphetNetModel', 'ProphetNetPreTrainedModel', 'ProphetNetTokenizer', 'PushToHubCallback', 'PvtConfig', 'PvtForImageClassification', 'PvtImageProcessor', 'PvtImageProcessorFast', 'PvtModel', 'PvtOnnxConfig', 'PvtPreTrainedModel', 'PvtV2Backbone', 'PvtV2Config', 'PvtV2ForImageClassification', 'PvtV2Model', 'PvtV2PreTrainedModel', 'QDQBertConfig', 'QDQBertForMaskedLM', 'QDQBertForMultipleChoice', 'QDQBertForNextSentencePrediction', 'QDQBertForQuestionAnswering', 'QDQBertForSequenceClassification', 'QDQBertForTokenClassification', 'QDQBertLMHeadModel', 'QDQBertLayer', 'QDQBertModel', 'QDQBertPreTrainedModel', 'QuantizedCache', 'QuantizedCacheConfig', 'QuantizedCacheProcessor', 'QuantoConfig', 'QuantoQuantizedCache', 'QuantoQuantizedCacheProcessor', 'QuarkConfig', 'QuestionAnsweringPipeline', 'Qwen2AudioConfig', 'Qwen2AudioEncoder', 'Qwen2AudioEncoderConfig', 'Qwen2AudioForConditionalGeneration', 'Qwen2AudioPreTrainedModel', 'Qwen2AudioProcessor', 'Qwen2Config', 'Qwen2ForCausalLM', 'Qwen2ForQuestionAnswering', 'Qwen2ForSequenceClassification', 'Qwen2ForTokenClassification', 'Qwen2Model', 'Qwen2MoeConfig', 'Qwen2MoeForCausalLM', 'Qwen2MoeForQuestionAnswering', 'Qwen2MoeForSequenceClassification', 'Qwen2MoeForTokenClassification', 'Qwen2MoeModel', 'Qwen2MoePreTrainedModel', 'Qwen2PreTrainedModel', 'Qwen2Tokenizer', 'Qwen2TokenizerFast', 'Qwen2VLConfig', 'Qwen2VLForConditionalGeneration', 'Qwen2VLImageProcessor', 'Qwen2VLImageProcessorFast', 'Qwen2VLModel', 'Qwen2VLPreTrainedModel', 'Qwen2VLProcessor', 'Qwen2VLTextConfig', 'Qwen2VLTextModel', 'Qwen2VLVideoProcessor', 'Qwen2_5OmniConfig', 'Qwen2_5OmniForConditionalGeneration', 'Qwen2_5OmniPreTrainedModel', 'Qwen2_5OmniPreTrainedModelForConditionalGeneration', 'Qwen2_5OmniProcessor', 'Qwen2_5OmniTalkerConfig', 'Qwen2_5OmniTalkerForConditionalGeneration', 'Qwen2_5OmniTalkerModel', 'Qwen2_5OmniThinkerConfig', 'Qwen2_5OmniThinkerForConditionalGeneration', 'Qwen2_5OmniThinkerTextModel', 'Qwen2_5OmniToken2WavBigVGANModel', 'Qwen2_5OmniToken2WavConfig', 'Qwen2_5OmniToken2WavDiTModel', 'Qwen2_5OmniToken2WavModel', 'Qwen2_5_VLConfig', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2_5_VLModel', 'Qwen2_5_VLPreTrainedModel', 'Qwen2_5_VLProcessor', 'Qwen2_5_VLTextConfig', 'Qwen2_5_VLTextModel', 'Qwen3Config', 'Qwen3ForCausalLM', 'Qwen3ForQuestionAnswering', 'Qwen3ForSequenceClassification', 'Qwen3ForTokenClassification', 'Qwen3Model', 'Qwen3MoeConfig', 'Qwen3MoeForCausalLM', 'Qwen3MoeForQuestionAnswering', 'Qwen3MoeForSequenceClassification', 'Qwen3MoeForTokenClassification', 'Qwen3MoeModel', 'Qwen3MoePreTrainedModel', 'Qwen3PreTrainedModel', 'ROPE_INIT_FUNCTIONS', 'RTDetrConfig', 'RTDetrForObjectDetection', 'RTDetrImageProcessor', 'RTDetrImageProcessorFast', 'RTDetrModel', 'RTDetrPreTrainedModel', 'RTDetrResNetBackbone', 'RTDetrResNetConfig', 'RTDetrResNetPreTrainedModel', 'RTDetrV2Config', 'RTDetrV2ForObjectDetection', 'RTDetrV2Model', 'RTDetrV2PreTrainedModel', 'RagConfig', 'RagModel', 'RagPreTrainedModel', 'RagRetriever', 'RagSequenceForGeneration', 'RagTokenForGeneration', 'RagTokenizer', 'RealmConfig', 'RealmEmbedder', 'RealmForOpenQA', 'RealmKnowledgeAugEncoder', 'RealmPreTrainedModel', 'RealmReader', 'RealmRetriever', 'RealmScorer', 'RealmTokenizer', 'RealmTokenizerFast', 'RecurrentGemmaConfig', 'RecurrentGemmaForCausalLM', 'RecurrentGemmaModel', 'RecurrentGemmaPreTrainedModel', 'ReformerAttention', 'ReformerConfig', 'ReformerForMaskedLM', 'ReformerForQuestionAnswering', 'ReformerForSequenceClassification', 'ReformerLayer', 'ReformerModel', 'ReformerModelWithLMHead', 'ReformerPreTrainedModel', 'ReformerTokenizer', 'ReformerTokenizerFast', 'RegNetConfig', 'RegNetForImageClassification', 'RegNetModel', 'RegNetPreTrainedModel', 'RemBertConfig', 'RemBertForCausalLM', 'RemBertForMaskedLM', 'RemBertForMultipleChoice', 'RemBertForQuestionAnswering', 'RemBertForSequenceClassification', 'RemBertForTokenClassification', 'RemBertLayer', 'RemBertModel', 'RemBertOnnxConfig', 'RemBertPreTrainedModel', 'RemBertTokenizer', 'RemBertTokenizerFast', 'RepetitionPenaltyLogitsProcessor', 'ResNetBackbone', 'ResNetConfig', 'ResNetForImageClassification', 'ResNetModel', 'ResNetOnnxConfig', 'ResNetPreTrainedModel', 'RetriBertConfig', 'RetriBertModel', 'RetriBertPreTrainedModel', 'RetriBertTokenizer', 'RetriBertTokenizerFast', 'RoCBertConfig', 'RoCBertForCausalLM', 'RoCBertForMaskedLM', 'RoCBertForMultipleChoice', 'RoCBertForPreTraining', 'RoCBertForQuestionAnswering', 'RoCBertForSequenceClassification', 'RoCBertForTokenClassification', 'RoCBertLayer', 'RoCBertModel', 'RoCBertPreTrainedModel', 'RoCBertTokenizer', 'RoFormerConfig', 'RoFormerForCausalLM', 'RoFormerForMaskedLM', 'RoFormerForMultipleChoice', 'RoFormerForQuestionAnswering', 'RoFormerForSequenceClassification', 'RoFormerForTokenClassification', 'RoFormerLayer', 'RoFormerModel', 'RoFormerOnnxConfig', 'RoFormerPreTrainedModel', 'RoFormerTokenizer', 'RoFormerTokenizerFast', 'RobertaConfig', 'RobertaForCausalLM', 'RobertaForMaskedLM', 'RobertaForMultipleChoice', 'RobertaForQuestionAnswering', 'RobertaForSequenceClassification', 'RobertaForTokenClassification', 'RobertaModel', 'RobertaOnnxConfig', 'RobertaPreLayerNormConfig', 'RobertaPreLayerNormForCausalLM', 'RobertaPreLayerNormForMaskedLM', 'RobertaPreLayerNormForMultipleChoice', 'RobertaPreLayerNormForQuestionAnswering', 'RobertaPreLayerNormForSequenceClassification', 'RobertaPreLayerNormForTokenClassification', 'RobertaPreLayerNormModel', 'RobertaPreLayerNormOnnxConfig', 'RobertaPreLayerNormPreTrainedModel', 'RobertaPreTrainedModel', 'RobertaTokenizer', 'RobertaTokenizerFast', 'RwkvConfig', 'RwkvForCausalLM', 'RwkvModel', 'RwkvPreTrainedModel', 'SEWConfig', 'SEWDConfig', 'SEWDForCTC', 'SEWDForSequenceClassification', 'SEWDModel', 'SEWDPreTrainedModel', 'SEWForCTC', 'SEWForSequenceClassification', 'SEWModel', 'SEWPreTrainedModel', 'SLOW_TO_FAST_CONVERTERS', 'SPIECE_UNDERLINE', 'SamConfig', 'SamHQConfig', 'SamHQMaskDecoderConfig', 'SamHQModel', 'SamHQPreTrainedModel', 'SamHQProcessor', 'SamHQPromptEncoderConfig', 'SamHQVisionConfig', 'SamHQVisionModel', 'SamImageProcessor', 'SamImageProcessorFast', 'SamMaskDecoderConfig', 'SamModel', 'SamPreTrainedModel', 'SamProcessor', 'SamPromptEncoderConfig', 'SamVisionConfig', 'SamVisionModel', 'SchedulerType', 'SeamlessM4TCodeHifiGan', 'SeamlessM4TConfig', 'SeamlessM4TFeatureExtractor', 'SeamlessM4TForSpeechToSpeech', 'SeamlessM4TForSpeechToText', 'SeamlessM4TForTextToSpeech', 'SeamlessM4TForTextToText', 'SeamlessM4THifiGan', 'SeamlessM4TModel', 'SeamlessM4TPreTrainedModel', 'SeamlessM4TProcessor', 'SeamlessM4TTextToUnitForConditionalGeneration', 'SeamlessM4TTextToUnitModel', 'SeamlessM4TTokenizer', 'SeamlessM4TTokenizerFast', 'SeamlessM4Tv2Config', 'SeamlessM4Tv2ForSpeechToSpeech', 'SeamlessM4Tv2ForSpeechToText', 'SeamlessM4Tv2ForTextToSpeech', 'SeamlessM4Tv2ForTextToText', 'SeamlessM4Tv2Model', 'SeamlessM4Tv2PreTrainedModel', 'SegGptConfig', 'SegGptForImageSegmentation', 'SegGptImageProcessor', 'SegGptModel', 'SegGptPreTrainedModel', 'SegformerConfig', 'SegformerDecodeHead', 'SegformerFeatureExtractor', 'SegformerForImageClassification', 'SegformerForSemanticSegmentation', 'SegformerImageProcessor', 'SegformerImageProcessorFast', 'SegformerLayer', 'SegformerModel', 'SegformerOnnxConfig', 'SegformerPreTrainedModel', 'Seq2SeqTrainer', 'Seq2SeqTrainingArguments', 'SequenceBiasLogitsProcessor', 'SequenceFeatureExtractor', 'ShieldGemma2Config', 'ShieldGemma2ForImageClassification', 'ShieldGemma2Processor', 'Siglip2Config', 'Siglip2ForImageClassification', 'Siglip2ImageProcessor', 'Siglip2ImageProcessorFast', 'Siglip2Model', 'Siglip2PreTrainedModel', 'Siglip2Processor', 'Siglip2TextConfig', 'Siglip2TextModel', 'Siglip2VisionConfig', 'Siglip2VisionModel', 'SiglipConfig', 'SiglipForImageClassification', 'SiglipImageProcessor', 'SiglipImageProcessorFast', 'SiglipModel', 'SiglipPreTrainedModel', 'SiglipProcessor', 'SiglipTextConfig', 'SiglipTextModel', 'SiglipTokenizer', 'SiglipVisionConfig', 'SiglipVisionModel', 'SingleSentenceClassificationProcessor', 'SinkCache', 'SlidingWindowCache', 'SlidingWindowLayer', 'SmolLM3Config', 'SmolLM3ForCausalLM', 'SmolLM3ForQuestionAnswering', 'SmolLM3ForSequenceClassification', 'SmolLM3ForTokenClassification', 'SmolLM3Model', 'SmolLM3PreTrainedModel', 'SmolVLMConfig', 'SmolVLMForConditionalGeneration', 'SmolVLMImageProcessor', 'SmolVLMImageProcessorFast', 'SmolVLMModel', 'SmolVLMPreTrainedModel', 'SmolVLMProcessor', 'SmolVLMVideoProcessor', 'SmolVLMVisionConfig', 'SmolVLMVisionTransformer', 'SpQRConfig', 'SpecialTokensMixin', 'Speech2Text2Config', 'Speech2Text2ForCausalLM', 'Speech2Text2PreTrainedModel', 'Speech2Text2Processor', 'Speech2Text2Tokenizer', 'Speech2TextConfig', 'Speech2TextFeatureExtractor', 'Speech2TextForConditionalGeneration', 'Speech2TextModel', 'Speech2TextPreTrainedModel', 'Speech2TextProcessor', 'Speech2TextTokenizer', 'SpeechEncoderDecoderConfig', 'SpeechEncoderDecoderModel', 'SpeechT5Config', 'SpeechT5FeatureExtractor', 'SpeechT5ForSpeechToSpeech', 'SpeechT5ForSpeechToText', 'SpeechT5ForTextToSpeech', 'SpeechT5HifiGan', 'SpeechT5HifiGanConfig', 'SpeechT5Model', 'SpeechT5PreTrainedModel', 'SpeechT5Processor', 'SpeechT5Tokenizer', 'SplinterConfig', 'SplinterForPreTraining', 'SplinterForQuestionAnswering', 'SplinterLayer', 'SplinterModel', 'SplinterPreTrainedModel', 'SplinterTokenizer', 'SplinterTokenizerFast', 'SquadDataTrainingArguments', 'SquadDataset', 'SquadExample', 'SquadFeatures', 'SquadV1Processor', 'SquadV2Processor', 'SqueezeBertConfig', 'SqueezeBertForMaskedLM', 'SqueezeBertForMultipleChoice', 'SqueezeBertForQuestionAnswering', 'SqueezeBertForSequenceClassification', 'SqueezeBertForTokenClassification', 'SqueezeBertModel', 'SqueezeBertModule', 'SqueezeBertOnnxConfig', 'SqueezeBertPreTrainedModel', 'SqueezeBertTokenizer', 'SqueezeBertTokenizerFast', 'StableLmConfig', 'StableLmForCausalLM', 'StableLmForSequenceClassification', 'StableLmForTokenClassification', 'StableLmModel', 'StableLmPreTrainedModel', 'Starcoder2Config', 'Starcoder2ForCausalLM', 'Starcoder2ForSequenceClassification', 'Starcoder2ForTokenClassification', 'Starcoder2Model', 'Starcoder2PreTrainedModel', 'StaticCache', 'StaticLayer', 'StopStringCriteria', 'StoppingCriteria', 'StoppingCriteriaList', 'SummarizationPipeline', 'SuperGlueConfig', 'SuperGlueForKeypointMatching', 'SuperGlueImageProcessor', 'SuperGluePreTrainedModel', 'SuperPointConfig', 'SuperPointForKeypointDetection', 'SuperPointImageProcessor', 'SuperPointImageProcessorFast', 'SuperPointPreTrainedModel', 'SuppressTokensAtBeginLogitsProcessor', 'SuppressTokensLogitsProcessor', 'SwiftFormerConfig', 'SwiftFormerForImageClassification', 'SwiftFormerModel', 'SwiftFormerOnnxConfig', 'SwiftFormerPreTrainedModel', 'Swin2SRConfig', 'Swin2SRForImageSuperResolution', 'Swin2SRImageProcessor', 'Swin2SRImageProcessorFast', 'Swin2SRModel', 'Swin2SRPreTrainedModel', 'SwinBackbone', 'SwinConfig', 'SwinForImageClassification', 'SwinForMaskedImageModeling', 'SwinModel', 'SwinOnnxConfig', 'SwinPreTrainedModel', 'Swinv2Backbone', 'Swinv2Config', 'Swinv2ForImageClassification', 'Swinv2ForMaskedImageModeling', 'Swinv2Model', 'Swinv2PreTrainedModel', 'SwitchTransformersConfig', 'SwitchTransformersEncoderModel', 'SwitchTransformersForConditionalGeneration', 'SwitchTransformersModel', 'SwitchTransformersPreTrainedModel', 'SwitchTransformersSparseMLP', 'SwitchTransformersTop1Router', 'SynthIDTextWatermarkDetector', 'SynthIDTextWatermarkLogitsProcessor', 'SynthIDTextWatermarkingConfig', 'T5Config', 'T5EncoderModel', 'T5ForConditionalGeneration', 'T5ForQuestionAnswering', 'T5ForSequenceClassification', 'T5ForTokenClassification', 'T5GemmaConfig', 'T5GemmaEncoderModel', 'T5GemmaForConditionalGeneration', 'T5GemmaForSequenceClassification', 'T5GemmaForTokenClassification', 'T5GemmaModel', 'T5GemmaModuleConfig', 'T5GemmaPreTrainedModel', 'T5Model', 'T5OnnxConfig', 'T5PreTrainedModel', 'T5Tokenizer', 'T5TokenizerFast', 'TF2_WEIGHTS_NAME', 'TFAdaptiveEmbedding', 'TFAlbertForMaskedLM', 'TFAlbertForMultipleChoice', 'TFAlbertForPreTraining', 'TFAlbertForQuestionAnswering', 'TFAlbertForSequenceClassification', 'TFAlbertForTokenClassification', 'TFAlbertMainLayer', 'TFAlbertModel', 'TFAlbertPreTrainedModel', 'TFAutoModel', 'TFAutoModelForAudioClassification', 'TFAutoModelForCausalLM', 'TFAutoModelForDocumentQuestionAnswering', 'TFAutoModelForImageClassification', 'TFAutoModelForMaskGeneration', 'TFAutoModelForMaskedImageModeling', 'TFAutoModelForMaskedLM', 'TFAutoModelForMultipleChoice', 'TFAutoModelForNextSentencePrediction', 'TFAutoModelForPreTraining', 'TFAutoModelForQuestionAnswering', 'TFAutoModelForSemanticSegmentation', 'TFAutoModelForSeq2SeqLM', 'TFAutoModelForSequenceClassification', 'TFAutoModelForSpeechSeq2Seq', 'TFAutoModelForTableQuestionAnswering', 'TFAutoModelForTextEncoding', 'TFAutoModelForTokenClassification', 'TFAutoModelForVision2Seq', 'TFAutoModelForZeroShotImageClassification', 'TFAutoModelWithLMHead', 'TFBartForConditionalGeneration', 'TFBartForSequenceClassification', 'TFBartModel', 'TFBartPretrainedModel', 'TFBertEmbeddings', 'TFBertForMaskedLM', 'TFBertForMultipleChoice', 'TFBertForNextSentencePrediction', 'TFBertForPreTraining', 'TFBertForQuestionAnswering', 'TFBertForSequenceClassification', 'TFBertForTokenClassification', 'TFBertLMHeadModel', 'TFBertMainLayer', 'TFBertModel', 'TFBertPreTrainedModel', 'TFBertTokenizer', 'TFBlenderbotForConditionalGeneration', 'TFBlenderbotModel', 'TFBlenderbotPreTrainedModel', 'TFBlenderbotSmallForConditionalGeneration', 'TFBlenderbotSmallModel', 'TFBlenderbotSmallPreTrainedModel', 'TFBlipForConditionalGeneration', 'TFBlipForImageTextRetrieval', 'TFBlipForQuestionAnswering', 'TFBlipModel', 'TFBlipPreTrainedModel', 'TFBlipTextLMHeadModel', 'TFBlipTextModel', 'TFBlipTextPreTrainedModel', 'TFBlipVisionModel', 'TFCLIPModel', 'TFCLIPPreTrainedModel', 'TFCLIPTextModel', 'TFCLIPVisionModel', 'TFCTRLForSequenceClassification', 'TFCTRLLMHeadModel', 'TFCTRLModel', 'TFCTRLPreTrainedModel', 'TFCamembertForCausalLM', 'TFCamembertForMaskedLM', 'TFCamembertForMultipleChoice', 'TFCamembertForQuestionAnswering', 'TFCamembertForSequenceClassification', 'TFCamembertForTokenClassification', 'TFCamembertModel', 'TFCamembertPreTrainedModel', 'TFConvBertForMaskedLM', 'TFConvBertForMultipleChoice', 'TFConvBertForQuestionAnswering', 'TFConvBertForSequenceClassification', 'TFConvBertForTokenClassification', 'TFConvBertLayer', 'TFConvBertModel', 'TFConvBertPreTrainedModel', 'TFConvNextForImageClassification', 'TFConvNextModel', 'TFConvNextPreTrainedModel', 'TFConvNextV2ForImageClassification', 'TFConvNextV2Model', 'TFConvNextV2PreTrainedModel', 'TFCvtForImageClassification', 'TFCvtModel', 'TFCvtPreTrainedModel', 'TFDPRContextEncoder', 'TFDPRPretrainedContextEncoder', 'TFDPRPretrainedQuestionEncoder', 'TFDPRPretrainedReader', 'TFDPRQuestionEncoder', 'TFDPRReader', 'TFData2VecVisionForImageClassification', 'TFData2VecVisionForSemanticSegmentation', 'TFData2VecVisionModel', 'TFData2VecVisionPreTrainedModel', 'TFDebertaForMaskedLM', 'TFDebertaForQuestionAnswering', 'TFDebertaForSequenceClassification', 'TFDebertaForTokenClassification', 'TFDebertaModel', 'TFDebertaPreTrainedModel', 'TFDebertaV2ForMaskedLM', 'TFDebertaV2ForMultipleChoice', 'TFDebertaV2ForQuestionAnswering', 'TFDebertaV2ForSequenceClassification', 'TFDebertaV2ForTokenClassification', 'TFDebertaV2Model', 'TFDebertaV2PreTrainedModel', 'TFDeiTForImageClassification', 'TFDeiTForImageClassificationWithTeacher', 'TFDeiTForMaskedImageModeling', 'TFDeiTModel', 'TFDeiTPreTrainedModel', 'TFDistilBertForMaskedLM', 'TFDistilBertForMultipleChoice', 'TFDistilBertForQuestionAnswering', 'TFDistilBertForSequenceClassification', 'TFDistilBertForTokenClassification', 'TFDistilBertMainLayer', 'TFDistilBertModel', 'TFDistilBertPreTrainedModel', 'TFEfficientFormerForImageClassification', 'TFEfficientFormerForImageClassificationWithTeacher', 'TFEfficientFormerModel', 'TFEfficientFormerPreTrainedModel', 'TFElectraForMaskedLM', 'TFElectraForMultipleChoice', 'TFElectraForPreTraining', 'TFElectraForQuestionAnswering', 'TFElectraForSequenceClassification', 'TFElectraForTokenClassification', 'TFElectraModel', 'TFElectraPreTrainedModel', 'TFEncoderDecoderModel', 'TFEsmForMaskedLM', 'TFEsmForSequenceClassification', 'TFEsmForTokenClassification', 'TFEsmModel', 'TFEsmPreTrainedModel', 'TFFlaubertForMultipleChoice', 'TFFlaubertForQuestionAnsweringSimple', 'TFFlaubertForSequenceClassification', 'TFFlaubertForTokenClassification', 'TFFlaubertModel', 'TFFlaubertPreTrainedModel', 'TFFlaubertWithLMHeadModel', 'TFForceTokensLogitsProcessor', 'TFForcedBOSTokenLogitsProcessor', 'TFForcedEOSTokenLogitsProcessor', 'TFFunnelBaseModel', 'TFFunnelForMaskedLM', 'TFFunnelForMultipleChoice', 'TFFunnelForPreTraining', 'TFFunnelForQuestionAnswering', 'TFFunnelForSequenceClassification', 'TFFunnelForTokenClassification', 'TFFunnelModel', 'TFFunnelPreTrainedModel', 'TFGPT2DoubleHeadsModel', 'TFGPT2ForSequenceClassification', 'TFGPT2LMHeadModel', 'TFGPT2MainLayer', 'TFGPT2Model', 'TFGPT2PreTrainedModel', 'TFGPT2Tokenizer', 'TFGPTJForCausalLM', 'TFGPTJForQuestionAnswering', 'TFGPTJForSequenceClassification', 'TFGPTJModel', 'TFGPTJPreTrainedModel', 'TFGenerationMixin', 'TFGroupViTModel', 'TFGroupViTPreTrainedModel', 'TFGroupViTTextModel', 'TFGroupViTVisionModel', 'TFHubertForCTC', 'TFHubertModel', 'TFHubertPreTrainedModel', 'TFIdeficsForVisionText2Text', 'TFIdeficsModel', 'TFIdeficsPreTrainedModel', 'TFLEDForConditionalGeneration', 'TFLEDModel', 'TFLEDPreTrainedModel', 'TFLayoutLMForMaskedLM', 'TFLayoutLMForQuestionAnswering', 'TFLayoutLMForSequenceClassification', 'TFLayoutLMForTokenClassification', 'TFLayoutLMMainLayer', 'TFLayoutLMModel', 'TFLayoutLMPreTrainedModel', 'TFLayoutLMv3ForQuestionAnswering', 'TFLayoutLMv3ForSequenceClassification', 'TFLayoutLMv3ForTokenClassification', 'TFLayoutLMv3Model', 'TFLayoutLMv3PreTrainedModel', 'TFLogitsProcessor', 'TFLogitsProcessorList', 'TFLogitsWarper', 'TFLongformerForMaskedLM', 'TFLongformerForMultipleChoice', 'TFLongformerForQuestionAnswering', 'TFLongformerForSequenceClassification', 'TFLongformerForTokenClassification', 'TFLongformerModel', 'TFLongformerPreTrainedModel', 'TFLongformerSelfAttention', 'TFLxmertForPreTraining', 'TFLxmertMainLayer', 'TFLxmertModel', 'TFLxmertPreTrainedModel', 'TFLxmertVisualFeatureEncoder', 'TFMBartForConditionalGeneration', 'TFMBartModel', 'TFMBartPreTrainedModel', 'TFMPNetEmbeddings', 'TFMPNetForMaskedLM', 'TFMPNetForMultipleChoice', 'TFMPNetForQuestionAnswering', 'TFMPNetForSequenceClassification', 'TFMPNetForTokenClassification', 'TFMPNetMainLayer', 'TFMPNetModel', 'TFMPNetPreTrainedModel', 'TFMT5EncoderModel', 'TFMT5ForConditionalGeneration', 'TFMT5Model', 'TFMarianMTModel', 'TFMarianModel', 'TFMarianPreTrainedModel', 'TFMinLengthLogitsProcessor', 'TFMistralForCausalLM', 'TFMistralForSequenceClassification', 'TFMistralModel', 'TFMistralPreTrainedModel', 'TFMobileBertForMaskedLM', 'TFMobileBertForMultipleChoice', 'TFMobileBertForNextSentencePrediction', 'TFMobileBertForPreTraining', 'TFMobileBertForQuestionAnswering', 'TFMobileBertForSequenceClassification', 'TFMobileBertForTokenClassification', 'TFMobileBertMainLayer', 'TFMobileBertModel', 'TFMobileBertPreTrainedModel', 'TFMobileViTForImageClassification', 'TFMobileViTForSemanticSegmentation', 'TFMobileViTModel', 'TFMobileViTPreTrainedModel', 'TFNoBadWordsLogitsProcessor', 'TFNoRepeatNGramLogitsProcessor', 'TFOPTForCausalLM', 'TFOPTModel', 'TFOPTPreTrainedModel', 'TFOpenAIGPTDoubleHeadsModel', 'TFOpenAIGPTForSequenceClassification', 'TFOpenAIGPTLMHeadModel', 'TFOpenAIGPTMainLayer', 'TFOpenAIGPTModel', 'TFOpenAIGPTPreTrainedModel', 'TFPegasusForConditionalGeneration', 'TFPegasusModel', 'TFPegasusPreTrainedModel', 'TFPreTrainedModel', 'TFRagModel', 'TFRagPreTrainedModel', 'TFRagSequenceForGeneration', 'TFRagTokenForGeneration', 'TFRegNetForImageClassification', 'TFRegNetModel', 'TFRegNetPreTrainedModel', 'TFRemBertForCausalLM', 'TFRemBertForMaskedLM', 'TFRemBertForMultipleChoice', 'TFRemBertForQuestionAnswering', 'TFRemBertForSequenceClassification', 'TFRemBertForTokenClassification', 'TFRemBertLayer', 'TFRemBertModel', 'TFRemBertPreTrainedModel', 'TFRepetitionPenaltyLogitsProcessor', 'TFResNetForImageClassification', 'TFResNetModel', 'TFResNetPreTrainedModel', 'TFRoFormerForCausalLM', 'TFRoFormerForMaskedLM', 'TFRoFormerForMultipleChoice', 'TFRoFormerForQuestionAnswering', 'TFRoFormerForSequenceClassification', 'TFRoFormerForTokenClassification', 'TFRoFormerLayer', 'TFRoFormerModel', 'TFRoFormerPreTrainedModel', 'TFRobertaForCausalLM', 'TFRobertaForMaskedLM', 'TFRobertaForMultipleChoice', 'TFRobertaForQuestionAnswering', 'TFRobertaForSequenceClassification', 'TFRobertaForTokenClassification', 'TFRobertaMainLayer', 'TFRobertaModel', 'TFRobertaPreLayerNormForCausalLM', 'TFRobertaPreLayerNormForMaskedLM', 'TFRobertaPreLayerNormForMultipleChoice', 'TFRobertaPreLayerNormForQuestionAnswering', 'TFRobertaPreLayerNormForSequenceClassification', 'TFRobertaPreLayerNormForTokenClassification', 'TFRobertaPreLayerNormMainLayer', 'TFRobertaPreLayerNormModel', 'TFRobertaPreLayerNormPreTrainedModel', 'TFRobertaPreTrainedModel', 'TFSamModel', 'TFSamPreTrainedModel', 'TFSamVisionModel', 'TFSegformerDecodeHead', 'TFSegformerForImageClassification', 'TFSegformerForSemanticSegmentation', 'TFSegformerModel', 'TFSegformerPreTrainedModel', 'TFSequenceSummary', 'TFSharedEmbeddings', 'TFSpeech2TextForConditionalGeneration', 'TFSpeech2TextModel', 'TFSpeech2TextPreTrainedModel', 'TFSuppressTokensAtBeginLogitsProcessor', 'TFSuppressTokensLogitsProcessor', 'TFSwiftFormerForImageClassification', 'TFSwiftFormerModel', 'TFSwiftFormerPreTrainedModel', 'TFSwinForImageClassification', 'TFSwinForMaskedImageModeling', 'TFSwinModel', 'TFSwinPreTrainedModel', 'TFT5EncoderModel', 'TFT5ForConditionalGeneration', 'TFT5Model', 'TFT5PreTrainedModel', 'TFTapasForMaskedLM', 'TFTapasForQuestionAnswering', 'TFTapasForSequenceClassification', 'TFTapasModel', 'TFTapasPreTrainedModel', 'TFTemperatureLogitsWarper', 'TFTopKLogitsWarper', 'TFTopPLogitsWarper', 'TFTrainingArguments', 'TFTransfoXLForSequenceClassification', 'TFTransfoXLLMHeadModel', 'TFTransfoXLMainLayer', 'TFTransfoXLModel', 'TFTransfoXLPreTrainedModel', 'TFViTForImageClassification', 'TFViTMAEForPreTraining', 'TFViTMAEModel', 'TFViTMAEPreTrainedModel', 'TFViTModel', 'TFViTPreTrainedModel', 'TFVisionEncoderDecoderModel', 'TFVisionTextDualEncoderModel', 'TFWav2Vec2ForCTC', 'TFWav2Vec2ForSequenceClassification', 'TFWav2Vec2Model', 'TFWav2Vec2PreTrainedModel', 'TFWhisperForConditionalGeneration', 'TFWhisperModel', 'TFWhisperPreTrainedModel', 'TFXGLMForCausalLM', 'TFXGLMModel', 'TFXGLMPreTrainedModel', 'TFXLMForMultipleChoice', 'TFXLMForQuestionAnsweringSimple', 'TFXLMForSequenceClassification', 'TFXLMForTokenClassification', 'TFXLMMainLayer', 'TFXLMModel', 'TFXLMPreTrainedModel', 'TFXLMRobertaForCausalLM', 'TFXLMRobertaForMaskedLM', 'TFXLMRobertaForMultipleChoice', 'TFXLMRobertaForQuestionAnswering', 'TFXLMRobertaForSequenceClassification', 'TFXLMRobertaForTokenClassification', 'TFXLMRobertaModel', 'TFXLMRobertaPreTrainedModel', 'TFXLMWithLMHeadModel', 'TFXLNetForMultipleChoice', 'TFXLNetForQuestionAnsweringSimple', 'TFXLNetForSequenceClassification', 'TFXLNetForTokenClassification', 'TFXLNetLMHeadModel', 'TFXLNetMainLayer', 'TFXLNetModel', 'TFXLNetPreTrainedModel', 'TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_CAUSAL_LM_MAPPING', 'TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING', 'TF_MODEL_FOR_MASKED_LM_MAPPING', 'TF_MODEL_FOR_MASK_GENERATION_MAPPING', 'TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'TF_MODEL_FOR_PRETRAINING_MAPPING', 'TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING', 'TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING', 'TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING', 'TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING', 'TF_MODEL_FOR_TEXT_ENCODING_MAPPING', 'TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING', 'TF_MODEL_FOR_VISION_2_SEQ_MAPPING', 'TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING', 'TF_MODEL_MAPPING', 'TF_MODEL_WITH_LM_HEAD_MAPPING', 'TF_WEIGHTS_NAME', 'TOKENIZER_MAPPING', 'TRANSFORMERS_CACHE', 'TableQuestionAnsweringPipeline', 'TableTransformerConfig', 'TableTransformerForObjectDetection', 'TableTransformerModel', 'TableTransformerOnnxConfig', 'TableTransformerPreTrainedModel', 'TapasConfig', 'TapasForMaskedLM', 'TapasForQuestionAnswering', 'TapasForSequenceClassification', 'TapasModel', 'TapasPreTrainedModel', 'TapasTokenizer', 'TapexTokenizer', 'TemperatureLogitsWarper', 'TensorType', 'Text2TextGenerationPipeline', 'TextClassificationPipeline', 'TextDataset', 'TextDatasetForNextSentencePrediction', 'TextGenerationPipeline', 'TextIteratorStreamer', 'TextNetBackbone', 'TextNetConfig', 'TextNetForImageClassification', 'TextNetImageProcessor', 'TextNetModel', 'TextNetPreTrainedModel', 'TextStreamer', 'TextToAudioPipeline', 'TimeSeriesTransformerConfig', 'TimeSeriesTransformerForPrediction', 'TimeSeriesTransformerModel', 'TimeSeriesTransformerPreTrainedModel', 'TimesFmConfig', 'TimesFmModel', 'TimesFmModelForPrediction', 'TimesFmPreTrainedModel', 'TimesformerConfig', 'TimesformerForVideoClassification', 'TimesformerModel', 'TimesformerPreTrainedModel', 'TimmBackbone', 'TimmBackboneConfig', 'TimmWrapperConfig', 'TimmWrapperForImageClassification', 'TimmWrapperImageProcessor', 'TimmWrapperModel', 'TimmWrapperPreTrainedModel', 'TokenClassificationPipeline', 'TokenSpan', 'TopKLogitsWarper', 'TopPLogitsWarper', 'TorchAoConfig', 'TorchExportableModuleWithStaticCache', 'TrOCRConfig', 'TrOCRForCausalLM', 'TrOCRPreTrainedModel', 'TrOCRProcessor', 'Trainer', 'TrainerCallback', 'TrainerControl', 'TrainerState', 'TrainingArguments', 'TrajectoryTransformerConfig', 'TrajectoryTransformerModel', 'TrajectoryTransformerPreTrainedModel', 'TransfoXLConfig', 'TransfoXLCorpus', 'TransfoXLForSequenceClassification', 'TransfoXLLMHeadModel', 'TransfoXLModel', 'TransfoXLPreTrainedModel', 'TransfoXLTokenizer', 'TranslationPipeline', 'TvltConfig', 'TvltFeatureExtractor', 'TvltForAudioVisualClassification', 'TvltForPreTraining', 'TvltImageProcessor', 'TvltModel', 'TvltPreTrainedModel', 'TvltProcessor', 'TvpConfig', 'TvpForVideoGrounding', 'TvpImageProcessor', 'TvpModel', 'TvpPreTrainedModel', 'TvpProcessor', 'TypicalLogitsWarper', 'UMT5Config', 'UMT5EncoderModel', 'UMT5ForConditionalGeneration', 'UMT5ForQuestionAnswering', 'UMT5ForSequenceClassification', 'UMT5ForTokenClassification', 'UMT5Model', 'UMT5OnnxConfig', 'UMT5PreTrainedModel', 'UdopConfig', 'UdopEncoderModel', 'UdopForConditionalGeneration', 'UdopModel', 'UdopPreTrainedModel', 'UdopProcessor', 'UdopTokenizer', 'UdopTokenizerFast', 'UnbatchedClassifierFreeGuidanceLogitsProcessor', 'UniSpeechConfig', 'UniSpeechForCTC', 'UniSpeechForPreTraining', 'UniSpeechForSequenceClassification', 'UniSpeechModel', 'UniSpeechPreTrainedModel', 'UniSpeechSatConfig', 'UniSpeechSatForAudioFrameClassification', 'UniSpeechSatForCTC', 'UniSpeechSatForPreTraining', 'UniSpeechSatForSequenceClassification', 'UniSpeechSatForXVector', 'UniSpeechSatModel', 'UniSpeechSatPreTrainedModel', 'UnivNetConfig', 'UnivNetFeatureExtractor', 'UnivNetModel', 'UperNetConfig', 'UperNetForSemanticSegmentation', 'UperNetPreTrainedModel', 'VIDEO_PROCESSOR_MAPPING', 'VJEPA2Config', 'VJEPA2ForVideoClassification', 'VJEPA2Model', 'VJEPA2PreTrainedModel', 'VJEPA2VideoProcessor', 'VanConfig', 'VanForImageClassification', 'VanModel', 'VanPreTrainedModel', 'ViTConfig', 'ViTFeatureExtractor', 'ViTForImageClassification', 'ViTForMaskedImageModeling', 'ViTHybridConfig', 'ViTHybridForImageClassification', 'ViTHybridImageProcessor', 'ViTHybridModel', 'ViTHybridPreTrainedModel', 'ViTImageProcessor', 'ViTImageProcessorFast', 'ViTMAEConfig', 'ViTMAEForPreTraining', 'ViTMAELayer', 'ViTMAEModel', 'ViTMAEPreTrainedModel', 'ViTMSNConfig', 'ViTMSNForImageClassification', 'ViTMSNModel', 'ViTMSNPreTrainedModel', 'ViTModel', 'ViTOnnxConfig', 'ViTPreTrainedModel', 'VideoClassificationPipeline', 'VideoLlavaConfig', 'VideoLlavaForConditionalGeneration', 'VideoLlavaImageProcessor', 'VideoLlavaModel', 'VideoLlavaPreTrainedModel', 'VideoLlavaProcessor', 'VideoLlavaVideoProcessor', 'VideoMAEConfig', 'VideoMAEFeatureExtractor', 'VideoMAEForPreTraining', 'VideoMAEForVideoClassification', 'VideoMAEImageProcessor', 'VideoMAEModel', 'VideoMAEPreTrainedModel', 'ViltConfig', 'ViltFeatureExtractor', 'ViltForImageAndTextRetrieval', 'ViltForImagesAndTextClassification', 'ViltForMaskedLM', 'ViltForQuestionAnswering', 'ViltForTokenClassification', 'ViltImageProcessor', 'ViltImageProcessorFast', 'ViltLayer', 'ViltModel', 'ViltPreTrainedModel', 'ViltProcessor', 'VipLlavaConfig', 'VipLlavaForConditionalGeneration', 'VipLlavaModel', 'VipLlavaPreTrainedModel', 'VisionEncoderDecoderConfig', 'VisionEncoderDecoderModel', 'VisionEncoderDecoderOnnxConfig', 'VisionTextDualEncoderConfig', 'VisionTextDualEncoderModel', 'VisionTextDualEncoderProcessor', 'VisualBertConfig', 'VisualBertForMultipleChoice', 'VisualBertForPreTraining', 'VisualBertForQuestionAnswering', 'VisualBertForRegionToPhraseAlignment', 'VisualBertForVisualReasoning', 'VisualBertLayer', 'VisualBertModel', 'VisualBertPreTrainedModel', 'VisualQuestionAnsweringPipeline', 'VitDetBackbone', 'VitDetConfig', 'VitDetModel', 'VitDetPreTrainedModel', 'VitMatteConfig', 'VitMatteForImageMatting', 'VitMatteImageProcessor', 'VitMatteImageProcessorFast', 'VitMattePreTrainedModel', 'VitPoseBackbone', 'VitPoseBackboneConfig', 'VitPoseBackbonePreTrainedModel', 'VitPoseConfig', 'VitPoseForPoseEstimation', 'VitPoseImageProcessor', 'VitPosePreTrainedModel', 'VitsConfig', 'VitsModel', 'VitsPreTrainedModel', 'VitsTokenizer', 'VivitConfig', 'VivitForVideoClassification', 'VivitImageProcessor', 'VivitModel', 'VivitPreTrainedModel', 'VoxtralConfig', 'VoxtralEncoder', 'VoxtralEncoderConfig', 'VoxtralForConditionalGeneration', 'VoxtralPreTrainedModel', 'VoxtralProcessor', 'VptqConfig', 'WEIGHTS_NAME', 'WarmUp', 'WatermarkDetector', 'WatermarkLogitsProcessor', 'WatermarkingConfig', 'Wav2Vec2BertConfig', 'Wav2Vec2BertForAudioFrameClassification', 'Wav2Vec2BertForCTC', 'Wav2Vec2BertForSequenceClassification', 'Wav2Vec2BertForXVector', 'Wav2Vec2BertModel', 'Wav2Vec2BertPreTrainedModel', 'Wav2Vec2BertProcessor', 'Wav2Vec2CTCTokenizer', 'Wav2Vec2Config', 'Wav2Vec2ConformerConfig', 'Wav2Vec2ConformerForAudioFrameClassification', 'Wav2Vec2ConformerForCTC', 'Wav2Vec2ConformerForPreTraining', 'Wav2Vec2ConformerForSequenceClassification', 'Wav2Vec2ConformerForXVector', 'Wav2Vec2ConformerModel', 'Wav2Vec2ConformerPreTrainedModel', 'Wav2Vec2FeatureExtractor', 'Wav2Vec2ForAudioFrameClassification', 'Wav2Vec2ForCTC', 'Wav2Vec2ForMaskedLM', 'Wav2Vec2ForPreTraining', 'Wav2Vec2ForSequenceClassification', 'Wav2Vec2ForXVector', 'Wav2Vec2Model', 'Wav2Vec2PhonemeCTCTokenizer', 'Wav2Vec2PreTrainedModel', 'Wav2Vec2Processor', 'Wav2Vec2ProcessorWithLM', 'Wav2Vec2Tokenizer', 'WavLMConfig', 'WavLMForAudioFrameClassification', 'WavLMForCTC', 'WavLMForSequenceClassification', 'WavLMForXVector', 'WavLMModel', 'WavLMPreTrainedModel', 'WhisperConfig', 'WhisperFeatureExtractor', 'WhisperForAudioClassification', 'WhisperForCausalLM', 'WhisperForConditionalGeneration', 'WhisperModel', 'WhisperOnnxConfig', 'WhisperPreTrainedModel', 'WhisperProcessor', 'WhisperTimeStampLogitsProcessor', 'WhisperTokenizer', 'WhisperTokenizerFast', 'WordpieceTokenizer', 'XCLIPConfig', 'XCLIPModel', 'XCLIPPreTrainedModel', 'XCLIPProcessor', 'XCLIPTextConfig', 'XCLIPTextModel', 'XCLIPVisionConfig', 'XCLIPVisionModel', 'XGLMConfig', 'XGLMForCausalLM', 'XGLMModel', 'XGLMPreTrainedModel', 'XGLMTokenizer', 'XGLMTokenizerFast', 'XLMConfig', 'XLMForMultipleChoice', 'XLMForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMForSequenceClassification', 'XLMForTokenClassification', 'XLMModel', 'XLMOnnxConfig', 'XLMPreTrainedModel', 'XLMProphetNetConfig', 'XLMProphetNetDecoder', 'XLMProphetNetEncoder', 'XLMProphetNetForCausalLM', 'XLMProphetNetForConditionalGeneration', 'XLMProphetNetModel', 'XLMProphetNetPreTrainedModel', 'XLMProphetNetTokenizer', 'XLMRobertaConfig', 'XLMRobertaForCausalLM', 'XLMRobertaForMaskedLM', 'XLMRobertaForMultipleChoice', 'XLMRobertaForQuestionAnswering', 'XLMRobertaForSequenceClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaModel', 'XLMRobertaOnnxConfig', 'XLMRobertaPreTrainedModel', 'XLMRobertaTokenizer', 'XLMRobertaTokenizerFast', 'XLMRobertaXLConfig', 'XLMRobertaXLForCausalLM', 'XLMRobertaXLForMaskedLM', 'XLMRobertaXLForMultipleChoice', 'XLMRobertaXLForQuestionAnswering', 'XLMRobertaXLForSequenceClassification', 'XLMRobertaXLForTokenClassification', 'XLMRobertaXLModel', 'XLMRobertaXLOnnxConfig', 'XLMRobertaXLPreTrainedModel', 'XLMTokenizer', 'XLMWithLMHeadModel', 'XLNetConfig', 'XLNetForMultipleChoice', 'XLNetForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XLNetForSequenceClassification', 'XLNetForTokenClassification', 'XLNetLMHeadModel', 'XLNetModel', 'XLNetPreTrainedModel', 'XLNetTokenizer', 'XLNetTokenizerFast', 'XmodConfig', 'XmodForCausalLM', 'XmodForMaskedLM', 'XmodForMultipleChoice', 'XmodForQuestionAnswering', 'XmodForSequenceClassification', 'XmodForTokenClassification', 'XmodModel', 'XmodOnnxConfig', 'XmodPreTrainedModel', 'YolosConfig', 'YolosFeatureExtractor', 'YolosForObjectDetection', 'YolosImageProcessor', 'YolosImageProcessorFast', 'YolosModel', 'YolosOnnxConfig', 'YolosPreTrainedModel', 'YosoConfig', 'YosoForMaskedLM', 'YosoForMultipleChoice', 'YosoForQuestionAnswering', 'YosoForSequenceClassification', 'YosoForTokenClassification', 'YosoLayer', 'YosoModel', 'YosoPreTrainedModel', 'ZOEDEPTH_PRETRAINED_CONFIG_ARCHIVE_MAP', 'Zamba2Config', 'Zamba2ForCausalLM', 'Zamba2ForSequenceClassification', 'Zamba2Model', 'Zamba2PreTrainedModel', 'ZambaConfig', 'ZambaForCausalLM', 'ZambaForSequenceClassification', 'ZambaModel', 'ZambaPreTrainedModel', 'ZeroShotAudioClassificationPipeline', 'ZeroShotClassificationPipeline', 'ZeroShotImageClassificationPipeline', 'ZeroShotObjectDetectionPipeline', 'ZoeDepthConfig', 'ZoeDepthForDepthEstimation', 'ZoeDepthImageProcessor', 'ZoeDepthImageProcessorFast', 'ZoeDepthPreTrainedModel', '__all__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_class_to_module', '_explicit_import_shortcut', '_import_structure', '_modules', '_name', '_object_missing_backend', '_objects', 'activations', 'activations_tf', 'add_end_docstrings', 'add_start_docstrings', 'apply_chunking_to_forward', 'audio_utils', 'cache_utils', 'commands', 'configuration_utils', 'convert_and_export_with_cache', 'convert_graph_to_onnx', 'convert_slow_tokenizer', 'convert_slow_tokenizers_checkpoints_to_fast', 'convert_tf_hub_seq_to_seq_bert_to_pytorch', 'convert_tf_weight_name_to_pt_weight_name', 'create_optimizer', 'data', 'data.data_collator', 'data.datasets', 'data.metrics', 'data.processors', 'debug_utils', 'default_data_collator', 'dependency_versions_check', 'dependency_versions_table', 'dynamic_module_utils', 'dynamic_rope_update', 'enable_full_determinism', 'feature_extraction_sequence_utils', 'feature_extraction_utils', 'file_utils', 'generation', 'get_constant_schedule', 'get_constant_schedule_with_warmup', 'get_cosine_schedule_with_warmup', 'get_cosine_with_hard_restarts_schedule_with_warmup', 'get_inverse_sqrt_schedule', 'get_linear_schedule_with_warmup', 'get_polynomial_decay_schedule_with_warmup', 'get_scheduler', 'get_values', 'get_wsd_schedule', 'glue_compute_metrics', 'glue_convert_examples_to_features', 'glue_output_modes', 'glue_processors', 'glue_tasks_num_labels', 'hf_argparser', 'hyperparameter_search', 'image_processing_base', 'image_processing_utils', 'image_processing_utils_fast', 'image_transforms', 'image_utils', 'integrations', 'integrations.executorch', 'is_apex_available', 'is_av_available', 'is_bitsandbytes_available', 'is_clearml_available', 'is_comet_available', 'is_datasets_available', 'is_dvclive_available', 'is_faiss_available', 'is_flax_available', 'is_keras_nlp_available', 'is_matplotlib_available', 'is_neptune_available', 'is_optuna_available', 'is_phonemizer_available', 'is_psutil_available', 'is_py3nvml_available', 'is_pyctcdecode_available', 'is_ray_available', 'is_ray_tune_available', 'is_sacremoses_available', 'is_safetensors_available', 'is_scipy_available', 'is_sentencepiece_available', 'is_sigopt_available', 'is_sklearn_available', 'is_speech_available', 'is_swanlab_available', 'is_tensorboard_available', 'is_tensorflow_text_available', 'is_tf_available', 'is_timm_available', 'is_tokenizers_available', 'is_torch_available', 'is_torch_hpu_available', 'is_torch_mlu_available', 'is_torch_musa_available', 'is_torch_neuroncore_available', 'is_torch_npu_available', 'is_torch_xla_available', 'is_torch_xpu_available', 'is_torchvision_available', 'is_trackio_available', 'is_vision_available', 'is_wandb_available', 'keras_callbacks', 'load_pytorch_checkpoint_in_tf2_model', 'load_pytorch_model_in_tf2_model', 'load_pytorch_weights_in_tf2_model', 'load_tf2_checkpoint_in_pytorch_model', 'load_tf2_model_in_pytorch_model', 'load_tf2_weights_in_pytorch_model', 'load_tf_weights_in_albert', 'load_tf_weights_in_bert', 'load_tf_weights_in_bert_generation', 'load_tf_weights_in_big_bird', 'load_tf_weights_in_canine', 'load_tf_weights_in_convbert', 'load_tf_weights_in_electra', 'load_tf_weights_in_funnel', 'load_tf_weights_in_gpt2', 'load_tf_weights_in_gpt_neo', 'load_tf_weights_in_imagegpt', 'load_tf_weights_in_mobilebert', 'load_tf_weights_in_mobilenet_v1', 'load_tf_weights_in_mobilenet_v2', 'load_tf_weights_in_openai_gpt', 'load_tf_weights_in_qdqbert', 'load_tf_weights_in_realm', 'load_tf_weights_in_rembert', 'load_tf_weights_in_roc_bert', 'load_tf_weights_in_roformer', 'load_tf_weights_in_t5', 'load_tf_weights_in_tapas', 'load_tf_weights_in_trajectory_transformer', 'load_tf_weights_in_transfo_xl', 'load_tf_weights_in_xlnet', 'logging', 'loss', 'masking_utils', 'model_addition_debugger_context', 'model_debugging_utils', 'modelcard', 'modeling_attn_mask_utils', 'modeling_flash_attention_utils', 'modeling_gguf_pytorch_utils', 'modeling_layers', 'modeling_outputs', 'modeling_rope_utils', 'modeling_tf_outputs', 'modeling_tf_pytorch_utils', 'modeling_tf_utils', 'modeling_utils', 'models', 'models.aimv2', 'models.aimv2.configuration_aimv2', 'models.aimv2.modeling_aimv2', 'models.albert', 'models.albert.configuration_albert', 'models.albert.modeling_albert', 'models.albert.modeling_flax_albert', 'models.albert.modeling_tf_albert', 'models.albert.tokenization_albert', 'models.albert.tokenization_albert_fast', 'models.align', 'models.align.configuration_align', 'models.align.modeling_align', 'models.align.processing_align', 'models.altclip', 'models.altclip.configuration_altclip', 'models.altclip.modeling_altclip', 'models.altclip.processing_altclip', 'models.arcee', 'models.arcee.configuration_arcee', 'models.arcee.modeling_arcee', 'models.aria', 'models.aria.configuration_aria', 'models.aria.image_processing_aria', 'models.aria.modeling_aria', 'models.aria.processing_aria', 'models.audio_spectrogram_transformer', 'models.audio_spectrogram_transformer.configuration_audio_spectrogram_transformer', 'models.audio_spectrogram_transformer.feature_extraction_audio_spectrogram_transformer', 'models.audio_spectrogram_transformer.modeling_audio_spectrogram_transformer', 'models.auto', 'models.auto.auto_factory', 'models.auto.configuration_auto', 'models.auto.feature_extraction_auto', 'models.auto.image_processing_auto', 'models.auto.modeling_auto', 'models.auto.modeling_flax_auto', 'models.auto.modeling_tf_auto', 'models.auto.processing_auto', 'models.auto.tokenization_auto', 'models.auto.video_processing_auto', 'models.autoformer', 'models.autoformer.configuration_autoformer', 'models.autoformer.modeling_autoformer', 'models.aya_vision', 'models.aya_vision.configuration_aya_vision', 'models.aya_vision.modeling_aya_vision', 'models.aya_vision.processing_aya_vision', 'models.bamba', 'models.bamba.configuration_bamba', 'models.bamba.modeling_bamba', 'models.bark', 'models.bark.configuration_bark', 'models.bark.modeling_bark', 'models.bark.processing_bark', 'models.bart', 'models.bart.configuration_bart', 'models.bart.modeling_bart', 'models.bart.modeling_flax_bart', 'models.bart.modeling_tf_bart', 'models.bart.tokenization_bart', 'models.bart.tokenization_bart_fast', 'models.barthez', 'models.barthez.tokenization_barthez', 'models.barthez.tokenization_barthez_fast', 'models.bartpho', 'models.bartpho.tokenization_bartpho', 'models.beit', 'models.beit.configuration_beit', 'models.beit.feature_extraction_beit', 'models.beit.image_processing_beit', 'models.beit.image_processing_beit_fast', 'models.beit.modeling_beit', 'models.beit.modeling_flax_beit', 'models.bert', 'models.bert.configuration_bert', 'models.bert.modeling_bert', 'models.bert.modeling_flax_bert', 'models.bert.modeling_tf_bert', 'models.bert.tokenization_bert', 'models.bert.tokenization_bert_fast', 'models.bert.tokenization_bert_tf', 'models.bert_generation', 'models.bert_generation.configuration_bert_generation', 'models.bert_generation.modeling_bert_generation', 'models.bert_generation.tokenization_bert_generation', 'models.bert_japanese', 'models.bert_japanese.tokenization_bert_japanese', 'models.bertweet', 'models.bertweet.tokenization_bertweet', 'models.big_bird', 'models.big_bird.configuration_big_bird', 'models.big_bird.modeling_big_bird', 'models.big_bird.modeling_flax_big_bird', 'models.big_bird.tokenization_big_bird', 'models.big_bird.tokenization_big_bird_fast', 'models.bigbird_pegasus', 'models.bigbird_pegasus.configuration_bigbird_pegasus', 'models.bigbird_pegasus.modeling_bigbird_pegasus', 'models.biogpt', 'models.biogpt.configuration_biogpt', 'models.biogpt.modeling_biogpt', 'models.biogpt.tokenization_biogpt', 'models.bit', 'models.bit.configuration_bit', 'models.bit.image_processing_bit', 'models.bit.image_processing_bit_fast', 'models.bit.modeling_bit', 'models.bitnet', 'models.bitnet.configuration_bitnet', 'models.bitnet.modeling_bitnet', 'models.blenderbot', 'models.blenderbot.configuration_blenderbot', 'models.blenderbot.modeling_blenderbot', 'models.blenderbot.modeling_flax_blenderbot', 'models.blenderbot.modeling_tf_blenderbot', 'models.blenderbot.tokenization_blenderbot', 'models.blenderbot.tokenization_blenderbot_fast', 'models.blenderbot_small', 'models.blenderbot_small.configuration_blenderbot_small', 'models.blenderbot_small.modeling_blenderbot_small', 'models.blenderbot_small.modeling_flax_blenderbot_small', 'models.blenderbot_small.modeling_tf_blenderbot_small', 'models.blenderbot_small.tokenization_blenderbot_small', 'models.blenderbot_small.tokenization_blenderbot_small_fast', 'models.blip', 'models.blip.configuration_blip', 'models.blip.image_processing_blip', 'models.blip.image_processing_blip_fast', 'models.blip.modeling_blip', 'models.blip.modeling_blip_text', 'models.blip.modeling_tf_blip', 'models.blip.modeling_tf_blip_text', 'models.blip.processing_blip', 'models.blip_2', 'models.blip_2.configuration_blip_2', 'models.blip_2.modeling_blip_2', 'models.blip_2.processing_blip_2', 'models.bloom', 'models.bloom.configuration_bloom', 'models.bloom.modeling_bloom', 'models.bloom.modeling_flax_bloom', 'models.bloom.tokenization_bloom_fast', 'models.bridgetower', 'models.bridgetower.configuration_bridgetower', 'models.bridgetower.image_processing_bridgetower', 'models.bridgetower.image_processing_bridgetower_fast', 'models.bridgetower.modeling_bridgetower', 'models.bridgetower.processing_bridgetower', 'models.bros', 'models.bros.configuration_bros', 'models.bros.modeling_bros', 'models.bros.processing_bros', 'models.byt5', 'models.byt5.tokenization_byt5', 'models.camembert', 'models.camembert.configuration_camembert', 'models.camembert.modeling_camembert', 'models.camembert.modeling_tf_camembert', 'models.camembert.tokenization_camembert', 'models.camembert.tokenization_camembert_fast', 'models.canine', 'models.canine.configuration_canine', 'models.canine.modeling_canine', 'models.canine.tokenization_canine', 'models.chameleon', 'models.chameleon.configuration_chameleon', 'models.chameleon.image_processing_chameleon', 'models.chameleon.image_processing_chameleon_fast', 'models.chameleon.modeling_chameleon', 'models.chameleon.processing_chameleon', 'models.chinese_clip', 'models.chinese_clip.configuration_chinese_clip', 'models.chinese_clip.feature_extraction_chinese_clip', 'models.chinese_clip.image_processing_chinese_clip', 'models.chinese_clip.image_processing_chinese_clip_fast', 'models.chinese_clip.modeling_chinese_clip', 'models.chinese_clip.processing_chinese_clip', 'models.clap', 'models.clap.configuration_clap', 'models.clap.feature_extraction_clap', 'models.clap.modeling_clap', 'models.clap.processing_clap', 'models.clip', 'models.clip.configuration_clip', 'models.clip.feature_extraction_clip', 'models.clip.image_processing_clip', 'models.clip.image_processing_clip_fast', 'models.clip.modeling_clip', 'models.clip.modeling_flax_clip', 'models.clip.modeling_tf_clip', 'models.clip.processing_clip', 'models.clip.tokenization_clip', 'models.clip.tokenization_clip_fast', 'models.clipseg', 'models.clipseg.configuration_clipseg', 'models.clipseg.modeling_clipseg', 'models.clipseg.processing_clipseg', 'models.clvp', 'models.clvp.configuration_clvp', 'models.clvp.feature_extraction_clvp', 'models.clvp.modeling_clvp', 'models.clvp.processing_clvp', 'models.clvp.tokenization_clvp', 'models.code_llama', 'models.code_llama.tokenization_code_llama', 'models.code_llama.tokenization_code_llama_fast', 'models.codegen', 'models.codegen.configuration_codegen', 'models.codegen.modeling_codegen', 'models.codegen.tokenization_codegen', 'models.codegen.tokenization_codegen_fast', 'models.cohere', 'models.cohere.configuration_cohere', 'models.cohere.modeling_cohere', 'models.cohere.tokenization_cohere_fast', 'models.cohere2', 'models.cohere2.configuration_cohere2', 'models.cohere2.modeling_cohere2', 'models.cohere2_vision', 'models.cohere2_vision.configuration_cohere2_vision', 'models.cohere2_vision.image_processing_cohere2_vision_fast', 'models.cohere2_vision.modeling_cohere2_vision', 'models.cohere2_vision.processing_cohere2_vision', 'models.colpali', 'models.colpali.configuration_colpali', 'models.colpali.modeling_colpali', 'models.colpali.processing_colpali', 'models.colqwen2', 'models.colqwen2.configuration_colqwen2', 'models.colqwen2.modeling_colqwen2', 'models.colqwen2.processing_colqwen2', 'models.conditional_detr', 'models.conditional_detr.configuration_conditional_detr', 'models.conditional_detr.feature_extraction_conditional_detr', 'models.conditional_detr.image_processing_conditional_detr', 'models.conditional_detr.image_processing_conditional_detr_fast', 'models.conditional_detr.modeling_conditional_detr', 'models.convbert', 'models.convbert.configuration_convbert', 'models.convbert.modeling_convbert', 'models.convbert.modeling_tf_convbert', 'models.convbert.tokenization_convbert', 'models.convbert.tokenization_convbert_fast', 'models.convnext', 'models.convnext.configuration_convnext', 'models.convnext.feature_extraction_convnext', 'models.convnext.image_processing_convnext', 'models.convnext.image_processing_convnext_fast', 'models.convnext.modeling_convnext', 'models.convnext.modeling_tf_convnext', 'models.convnextv2', 'models.convnextv2.configuration_convnextv2', 'models.convnextv2.modeling_convnextv2', 'models.convnextv2.modeling_tf_convnextv2', 'models.cpm', 'models.cpm.tokenization_cpm', 'models.cpm.tokenization_cpm_fast', 'models.cpmant', 'models.cpmant.configuration_cpmant', 'models.cpmant.modeling_cpmant', 'models.cpmant.tokenization_cpmant', 'models.csm', 'models.csm.configuration_csm', 'models.csm.modeling_csm', 'models.csm.processing_csm', 'models.ctrl', 'models.ctrl.configuration_ctrl', 'models.ctrl.modeling_ctrl', 'models.ctrl.modeling_tf_ctrl', 'models.ctrl.tokenization_ctrl', 'models.cvt', 'models.cvt.configuration_cvt', 'models.cvt.modeling_cvt', 'models.cvt.modeling_tf_cvt', 'models.d_fine', 'models.d_fine.configuration_d_fine', 'models.d_fine.modeling_d_fine', 'models.dab_detr', 'models.dab_detr.configuration_dab_detr', 'models.dab_detr.modeling_dab_detr', 'models.dac', 'models.dac.configuration_dac', 'models.dac.feature_extraction_dac', 'models.dac.modeling_dac', 'models.data2vec', 'models.data2vec.configuration_data2vec_audio', 'models.data2vec.configuration_data2vec_text', 'models.data2vec.configuration_data2vec_vision', 'models.data2vec.modeling_data2vec_audio', 'models.data2vec.modeling_data2vec_text', 'models.data2vec.modeling_data2vec_vision', 'models.data2vec.modeling_tf_data2vec_vision', 'models.dbrx', 'models.dbrx.configuration_dbrx', 'models.dbrx.modeling_dbrx', 'models.deberta', 'models.deberta.configuration_deberta', 'models.deberta.modeling_deberta', 'models.deberta.modeling_tf_deberta', 'models.deberta.tokenization_deberta', 'models.deberta.tokenization_deberta_fast', 'models.deberta_v2', 'models.deberta_v2.configuration_deberta_v2', 'models.deberta_v2.modeling_deberta_v2', 'models.deberta_v2.modeling_tf_deberta_v2', 'models.deberta_v2.tokenization_deberta_v2', 'models.deberta_v2.tokenization_deberta_v2_fast', 'models.decision_transformer', 'models.decision_transformer.configuration_decision_transformer', 'models.decision_transformer.modeling_decision_transformer', 'models.deepseek_v2', 'models.deepseek_v2.configuration_deepseek_v2', 'models.deepseek_v2.modeling_deepseek_v2', 'models.deepseek_v3', 'models.deepseek_v3.configuration_deepseek_v3', 'models.deepseek_v3.modeling_deepseek_v3', 'models.deepseek_vl', 'models.deepseek_vl.configuration_deepseek_vl', 'models.deepseek_vl.image_processing_deepseek_vl', 'models.deepseek_vl.image_processing_deepseek_vl_fast', 'models.deepseek_vl.modeling_deepseek_vl', 'models.deepseek_vl.processing_deepseek_vl', 'models.deepseek_vl_hybrid', 'models.deepseek_vl_hybrid.configuration_deepseek_vl_hybrid', 'models.deepseek_vl_hybrid.image_processing_deepseek_vl_hybrid', 'models.deepseek_vl_hybrid.image_processing_deepseek_vl_hybrid_fast', 'models.deepseek_vl_hybrid.modeling_deepseek_vl_hybrid', 'models.deepseek_vl_hybrid.processing_deepseek_vl_hybrid', 'models.deformable_detr', 'models.deformable_detr.configuration_deformable_detr', 'models.deformable_detr.feature_extraction_deformable_detr', 'models.deformable_detr.image_processing_deformable_detr', 'models.deformable_detr.image_processing_deformable_detr_fast', 'models.deformable_detr.modeling_deformable_detr', 'models.deit', 'models.deit.configuration_deit', 'models.deit.feature_extraction_deit', 'models.deit.image_processing_deit', 'models.deit.image_processing_deit_fast', 'models.deit.modeling_deit', 'models.deit.modeling_tf_deit', 'models.deprecated', 'models.deprecated.deta', 'models.deprecated.deta.configuration_deta', 'models.deprecated.deta.image_processing_deta', 'models.deprecated.deta.modeling_deta', 'models.deprecated.efficientformer', 'models.deprecated.efficientformer.configuration_efficientformer', 'models.deprecated.efficientformer.image_processing_efficientformer', 'models.deprecated.efficientformer.modeling_efficientformer', 'models.deprecated.efficientformer.modeling_tf_efficientformer', 'models.deprecated.ernie_m', 'models.deprecated.ernie_m.configuration_ernie_m', 'models.deprecated.ernie_m.modeling_ernie_m', 'models.deprecated.ernie_m.tokenization_ernie_m', 'models.deprecated.gptsan_japanese', 'models.deprecated.gptsan_japanese.configuration_gptsan_japanese', 'models.deprecated.gptsan_japanese.modeling_gptsan_japanese', 'models.deprecated.gptsan_japanese.tokenization_gptsan_japanese', 'models.deprecated.graphormer', 'models.deprecated.graphormer.configuration_graphormer', 'models.deprecated.graphormer.modeling_graphormer', 'models.deprecated.jukebox', 'models.deprecated.jukebox.configuration_jukebox', 'models.deprecated.jukebox.modeling_jukebox', 'models.deprecated.jukebox.tokenization_jukebox', 'models.deprecated.mctct', 'models.deprecated.mctct.configuration_mctct', 'models.deprecated.mctct.feature_extraction_mctct', 'models.deprecated.mctct.modeling_mctct', 'models.deprecated.mctct.processing_mctct', 'models.deprecated.mega', 'models.deprecated.mega.configuration_mega', 'models.deprecated.mega.modeling_mega', 'models.deprecated.mmbt', 'models.deprecated.mmbt.configuration_mmbt', 'models.deprecated.mmbt.modeling_mmbt', 'models.deprecated.nat', 'models.deprecated.nat.configuration_nat', 'models.deprecated.nat.modeling_nat', 'models.deprecated.nezha', 'models.deprecated.nezha.configuration_nezha', 'models.deprecated.nezha.modeling_nezha', 'models.deprecated.open_llama', 'models.deprecated.open_llama.configuration_open_llama', 'models.deprecated.open_llama.modeling_open_llama', 'models.deprecated.qdqbert', 'models.deprecated.qdqbert.configuration_qdqbert', 'models.deprecated.qdqbert.modeling_qdqbert', 'models.deprecated.realm', 'models.deprecated.realm.configuration_realm', 'models.deprecated.realm.modeling_realm', 'models.deprecated.realm.retrieval_realm', 'models.deprecated.realm.tokenization_realm', 'models.deprecated.realm.tokenization_realm_fast', 'models.deprecated.retribert', 'models.deprecated.retribert.configuration_retribert', 'models.deprecated.retribert.modeling_retribert', 'models.deprecated.retribert.tokenization_retribert', 'models.deprecated.retribert.tokenization_retribert_fast', 'models.deprecated.speech_to_text_2', 'models.deprecated.speech_to_text_2.configuration_speech_to_text_2', 'models.deprecated.speech_to_text_2.modeling_speech_to_text_2', 'models.deprecated.speech_to_text_2.processing_speech_to_text_2', 'models.deprecated.speech_to_text_2.tokenization_speech_to_text_2', 'models.deprecated.tapex', 'models.deprecated.tapex.tokenization_tapex', 'models.deprecated.trajectory_transformer', 'models.deprecated.trajectory_transformer.configuration_trajectory_transformer', 'models.deprecated.trajectory_transformer.modeling_trajectory_transformer', 'models.deprecated.transfo_xl', 'models.deprecated.transfo_xl.configuration_transfo_xl', 'models.deprecated.transfo_xl.modeling_tf_transfo_xl', 'models.deprecated.transfo_xl.modeling_transfo_xl', 'models.deprecated.transfo_xl.tokenization_transfo_xl', 'models.deprecated.tvlt', 'models.deprecated.tvlt.configuration_tvlt', 'models.deprecated.tvlt.feature_extraction_tvlt', 'models.deprecated.tvlt.image_processing_tvlt', 'models.deprecated.tvlt.modeling_tvlt', 'models.deprecated.tvlt.processing_tvlt', 'models.deprecated.van', 'models.deprecated.van.configuration_van', 'models.deprecated.van.modeling_van', 'models.deprecated.vit_hybrid', 'models.deprecated.vit_hybrid.configuration_vit_hybrid', 'models.deprecated.vit_hybrid.image_processing_vit_hybrid', 'models.deprecated.vit_hybrid.modeling_vit_hybrid', 'models.deprecated.xlm_prophetnet', 'models.deprecated.xlm_prophetnet.configuration_xlm_prophetnet', 'models.deprecated.xlm_prophetnet.modeling_xlm_prophetnet', 'models.deprecated.xlm_prophetnet.tokenization_xlm_prophetnet', 'models.depth_anything', 'models.depth_anything.configuration_depth_anything', 'models.depth_anything.modeling_depth_anything', 'models.depth_pro', 'models.depth_pro.configuration_depth_pro', 'models.depth_pro.image_processing_depth_pro', 'models.depth_pro.image_processing_depth_pro_fast', 'models.depth_pro.modeling_depth_pro', 'models.detr', 'models.detr.configuration_detr', 'models.detr.feature_extraction_detr', 'models.detr.image_processing_detr', 'models.detr.image_processing_detr_fast', 'models.detr.modeling_detr', 'models.dia', 'models.dia.configuration_dia', 'models.dia.feature_extraction_dia', 'models.dia.modeling_dia', 'models.dia.processing_dia', 'models.dia.tokenization_dia', 'models.diffllama', 'models.diffllama.configuration_diffllama', 'models.diffllama.modeling_diffllama', 'models.dinat', 'models.dinat.configuration_dinat', 'models.dinat.modeling_dinat', 'models.dinov2', 'models.dinov2.configuration_dinov2', 'models.dinov2.modeling_dinov2', 'models.dinov2.modeling_flax_dinov2', 'models.dinov2_with_registers', 'models.dinov2_with_registers.configuration_dinov2_with_registers', 'models.dinov2_with_registers.modeling_dinov2_with_registers', 'models.distilbert', 'models.distilbert.configuration_distilbert', 'models.distilbert.modeling_distilbert', 'models.distilbert.modeling_flax_distilbert', 'models.distilbert.modeling_tf_distilbert', 'models.distilbert.tokenization_distilbert', 'models.distilbert.tokenization_distilbert_fast', 'models.doge', 'models.doge.configuration_doge', 'models.doge.modeling_doge', 'models.donut', 'models.donut.configuration_donut_swin', 'models.donut.feature_extraction_donut', 'models.donut.image_processing_donut', 'models.donut.image_processing_donut_fast', 'models.donut.modeling_donut_swin', 'models.donut.processing_donut', 'models.dots1', 'models.dots1.configuration_dots1', 'models.dots1.modeling_dots1', 'models.dpr', 'models.dpr.configuration_dpr', 'models.dpr.modeling_dpr', 'models.dpr.modeling_tf_dpr', 'models.dpr.tokenization_dpr', 'models.dpr.tokenization_dpr_fast', 'models.dpt', 'models.dpt.configuration_dpt', 'models.dpt.feature_extraction_dpt', 'models.dpt.image_processing_dpt', 'models.dpt.image_processing_dpt_fast', 'models.dpt.modeling_dpt', 'models.efficientloftr', 'models.efficientloftr.configuration_efficientloftr', 'models.efficientloftr.image_processing_efficientloftr', 'models.efficientloftr.modeling_efficientloftr', 'models.efficientnet', 'models.efficientnet.configuration_efficientnet', 'models.efficientnet.image_processing_efficientnet', 'models.efficientnet.image_processing_efficientnet_fast', 'models.efficientnet.modeling_efficientnet', 'models.electra', 'models.electra.configuration_electra', 'models.electra.modeling_electra', 'models.electra.modeling_flax_electra', 'models.electra.modeling_tf_electra', 'models.electra.tokenization_electra', 'models.electra.tokenization_electra_fast', 'models.emu3', 'models.emu3.configuration_emu3', 'models.emu3.image_processing_emu3', 'models.emu3.modeling_emu3', 'models.emu3.processing_emu3', 'models.encodec', 'models.encodec.configuration_encodec', 'models.encodec.feature_extraction_encodec', 'models.encodec.modeling_encodec', 'models.encoder_decoder', 'models.encoder_decoder.configuration_encoder_decoder', 'models.encoder_decoder.modeling_encoder_decoder', 'models.encoder_decoder.modeling_flax_encoder_decoder', 'models.encoder_decoder.modeling_tf_encoder_decoder', 'models.eomt', 'models.eomt.configuration_eomt', 'models.eomt.image_processing_eomt', 'models.eomt.image_processing_eomt_fast', 'models.eomt.modeling_eomt', 'models.ernie', 'models.ernie.configuration_ernie', 'models.ernie.modeling_ernie', 'models.ernie4_5', 'models.ernie4_5.configuration_ernie4_5', 'models.ernie4_5.modeling_ernie4_5', 'models.ernie4_5_moe', 'models.ernie4_5_moe.configuration_ernie4_5_moe', 'models.ernie4_5_moe.modeling_ernie4_5_moe', 'models.esm', 'models.esm.configuration_esm', 'models.esm.modeling_esm', 'models.esm.modeling_esmfold', 'models.esm.modeling_tf_esm', 'models.esm.tokenization_esm', 'models.evolla', 'models.evolla.configuration_evolla', 'models.evolla.modeling_evolla', 'models.evolla.processing_evolla', 'models.exaone4', 'models.exaone4.configuration_exaone4', 'models.exaone4.modeling_exaone4', 'models.falcon', 'models.falcon.configuration_falcon', 'models.falcon.modeling_falcon', 'models.falcon_h1', 'models.falcon_h1.configuration_falcon_h1', 'models.falcon_h1.modeling_falcon_h1', 'models.falcon_mamba', 'models.falcon_mamba.configuration_falcon_mamba', 'models.falcon_mamba.modeling_falcon_mamba', 'models.fastspeech2_conformer', 'models.fastspeech2_conformer.configuration_fastspeech2_conformer', 'models.fastspeech2_conformer.modeling_fastspeech2_conformer', 'models.fastspeech2_conformer.tokenization_fastspeech2_conformer', 'models.flaubert', 'models.flaubert.configuration_flaubert', 'models.flaubert.modeling_flaubert', 'models.flaubert.modeling_tf_flaubert', 'models.flaubert.tokenization_flaubert', 'models.flava', 'models.flava.configuration_flava', 'models.flava.feature_extraction_flava', 'models.flava.image_processing_flava', 'models.flava.image_processing_flava_fast', 'models.flava.modeling_flava', 'models.flava.processing_flava', 'models.fnet', 'models.fnet.configuration_fnet', 'models.fnet.modeling_fnet', 'models.fnet.tokenization_fnet', 'models.fnet.tokenization_fnet_fast', 'models.focalnet', 'models.focalnet.configuration_focalnet', 'models.focalnet.modeling_focalnet', 'models.fsmt', 'models.fsmt.configuration_fsmt', 'models.fsmt.modeling_fsmt', 'models.fsmt.tokenization_fsmt', 'models.funnel', 'models.funnel.configuration_funnel', 'models.funnel.modeling_funnel', 'models.funnel.modeling_tf_funnel', 'models.funnel.tokenization_funnel', 'models.funnel.tokenization_funnel_fast', 'models.fuyu', 'models.fuyu.configuration_fuyu', 'models.fuyu.image_processing_fuyu', 'models.fuyu.modeling_fuyu', 'models.fuyu.processing_fuyu', 'models.gemma', 'models.gemma.configuration_gemma', 'models.gemma.modeling_flax_gemma', 'models.gemma.modeling_gemma', 'models.gemma.tokenization_gemma', 'models.gemma.tokenization_gemma_fast', 'models.gemma2', 'models.gemma2.configuration_gemma2', 'models.gemma2.modeling_gemma2', 'models.gemma3', 'models.gemma3.configuration_gemma3', 'models.gemma3.image_processing_gemma3', 'models.gemma3.image_processing_gemma3_fast', 'models.gemma3.modeling_gemma3', 'models.gemma3.processing_gemma3', 'models.gemma3n', 'models.gemma3n.configuration_gemma3n', 'models.gemma3n.feature_extraction_gemma3n', 'models.gemma3n.modeling_gemma3n', 'models.gemma3n.processing_gemma3n', 'models.git', 'models.git.configuration_git', 'models.git.modeling_git', 'models.git.processing_git', 'models.glm', 'models.glm.configuration_glm', 'models.glm.modeling_glm', 'models.glm4', 'models.glm4.configuration_glm4', 'models.glm4.modeling_glm4', 'models.glm4_moe', 'models.glm4_moe.configuration_glm4_moe', 'models.glm4_moe.modeling_glm4_moe', 'models.glm4v', 'models.glm4v.configuration_glm4v', 'models.glm4v.image_processing_glm4v', 'models.glm4v.image_processing_glm4v_fast', 'models.glm4v.modeling_glm4v', 'models.glm4v.processing_glm4v', 'models.glm4v.video_processing_glm4v', 'models.glpn', 'models.glpn.configuration_glpn', 'models.glpn.feature_extraction_glpn', 'models.glpn.image_processing_glpn', 'models.glpn.modeling_glpn', 'models.got_ocr2', 'models.got_ocr2.configuration_got_ocr2', 'models.got_ocr2.image_processing_got_ocr2', 'models.got_ocr2.image_processing_got_ocr2_fast', 'models.got_ocr2.modeling_got_ocr2', 'models.got_ocr2.processing_got_ocr2', 'models.gpt2', 'models.gpt2.configuration_gpt2', 'models.gpt2.modeling_flax_gpt2', 'models.gpt2.modeling_gpt2', 'models.gpt2.modeling_tf_gpt2', 'models.gpt2.tokenization_gpt2', 'models.gpt2.tokenization_gpt2_fast', 'models.gpt2.tokenization_gpt2_tf', 'models.gpt_bigcode', 'models.gpt_bigcode.configuration_gpt_bigcode', 'models.gpt_bigcode.modeling_gpt_bigcode', 'models.gpt_neo', 'models.gpt_neo.configuration_gpt_neo', 'models.gpt_neo.modeling_flax_gpt_neo', 'models.gpt_neo.modeling_gpt_neo', 'models.gpt_neox', 'models.gpt_neox.configuration_gpt_neox', 'models.gpt_neox.modeling_gpt_neox', 'models.gpt_neox.tokenization_gpt_neox_fast', 'models.gpt_neox_japanese', 'models.gpt_neox_japanese.configuration_gpt_neox_japanese', 'models.gpt_neox_japanese.modeling_gpt_neox_japanese', 'models.gpt_neox_japanese.tokenization_gpt_neox_japanese', 'models.gpt_oss', 'models.gpt_oss.configuration_gpt_oss', 'models.gpt_oss.modeling_gpt_oss', 'models.gpt_sw3', 'models.gpt_sw3.tokenization_gpt_sw3', 'models.gptj', 'models.gptj.configuration_gptj', 'models.gptj.modeling_flax_gptj', 'models.gptj.modeling_gptj', 'models.gptj.modeling_tf_gptj', 'models.granite', 'models.granite.configuration_granite', 'models.granite.modeling_granite', 'models.granite_speech', 'models.granite_speech.configuration_granite_speech', 'models.granite_speech.feature_extraction_granite_speech', 'models.granite_speech.modeling_granite_speech', 'models.granite_speech.processing_granite_speech', 'models.granitemoe', 'models.granitemoe.configuration_granitemoe', 'models.granitemoe.modeling_granitemoe', 'models.granitemoehybrid', 'models.granitemoehybrid.configuration_granitemoehybrid', 'models.granitemoehybrid.modeling_granitemoehybrid', 'models.granitemoeshared', 'models.granitemoeshared.configuration_granitemoeshared', 'models.granitemoeshared.modeling_granitemoeshared', 'models.grounding_dino', 'models.grounding_dino.configuration_grounding_dino', 'models.grounding_dino.image_processing_grounding_dino', 'models.grounding_dino.image_processing_grounding_dino_fast', 'models.grounding_dino.modeling_grounding_dino', 'models.grounding_dino.processing_grounding_dino', 'models.groupvit', 'models.groupvit.configuration_groupvit', 'models.groupvit.modeling_groupvit', 'models.groupvit.modeling_tf_groupvit', 'models.helium', 'models.helium.configuration_helium', 'models.helium.modeling_helium', 'models.herbert', 'models.herbert.tokenization_herbert', 'models.herbert.tokenization_herbert_fast', 'models.hgnet_v2', 'models.hgnet_v2.configuration_hgnet_v2', 'models.hgnet_v2.modeling_hgnet_v2', 'models.hiera', 'models.hiera.configuration_hiera', 'models.hiera.modeling_hiera', 'models.hubert', 'models.hubert.configuration_hubert', 'models.hubert.modeling_hubert', 'models.hubert.modeling_tf_hubert', 'models.ibert', 'models.ibert.configuration_ibert', 'models.ibert.modeling_ibert', 'models.idefics', 'models.idefics.configuration_idefics', 'models.idefics.image_processing_idefics', 'models.idefics.modeling_idefics', 'models.idefics.modeling_tf_idefics', 'models.idefics.processing_idefics', 'models.idefics2', 'models.idefics2.configuration_idefics2', 'models.idefics2.image_processing_idefics2', 'models.idefics2.image_processing_idefics2_fast', 'models.idefics2.modeling_idefics2', 'models.idefics2.processing_idefics2', 'models.idefics3', 'models.idefics3.configuration_idefics3', 'models.idefics3.image_processing_idefics3', 'models.idefics3.image_processing_idefics3_fast', 'models.idefics3.modeling_idefics3', 'models.idefics3.processing_idefics3', 'models.ijepa', 'models.ijepa.configuration_ijepa', 'models.ijepa.modeling_ijepa', 'models.imagegpt', 'models.imagegpt.configuration_imagegpt', 'models.imagegpt.feature_extraction_imagegpt', 'models.imagegpt.image_processing_imagegpt', 'models.imagegpt.modeling_imagegpt', 'models.informer', 'models.informer.configuration_informer', 'models.informer.modeling_informer', 'models.instructblip', 'models.instructblip.configuration_instructblip', 'models.instructblip.modeling_instructblip', 'models.instructblip.processing_instructblip', 'models.instructblipvideo', 'models.instructblipvideo.configuration_instructblipvideo', 'models.instructblipvideo.image_processing_instructblipvideo', 'models.instructblipvideo.modeling_instructblipvideo', 'models.instructblipvideo.processing_instructblipvideo', 'models.instructblipvideo.video_processing_instructblipvideo', 'models.internvl', 'models.internvl.configuration_internvl', 'models.internvl.modeling_internvl', 'models.internvl.processing_internvl', 'models.internvl.video_processing_internvl', 'models.jamba', 'models.jamba.configuration_jamba', 'models.jamba.modeling_jamba', 'models.janus', 'models.janus.configuration_janus', 'models.janus.image_processing_janus', 'models.janus.image_processing_janus_fast', 'models.janus.modeling_janus', 'models.janus.processing_janus', 'models.jetmoe', 'models.jetmoe.configuration_jetmoe', 'models.jetmoe.modeling_jetmoe', 'models.kosmos2', 'models.kosmos2.configuration_kosmos2', 'models.kosmos2.modeling_kosmos2', 'models.kosmos2.processing_kosmos2', 'models.kyutai_speech_to_text', 'models.kyutai_speech_to_text.configuration_kyutai_speech_to_text', 'models.kyutai_speech_to_text.feature_extraction_kyutai_speech_to_text', 'models.kyutai_speech_to_text.modeling_kyutai_speech_to_text', 'models.kyutai_speech_to_text.processing_kyutai_speech_to_text', 'models.layoutlm', 'models.layoutlm.configuration_layoutlm', 'models.layoutlm.modeling_layoutlm', 'models.layoutlm.modeling_tf_layoutlm', 'models.layoutlm.tokenization_layoutlm', 'models.layoutlm.tokenization_layoutlm_fast', 'models.layoutlmv2', 'models.layoutlmv2.configuration_layoutlmv2', 'models.layoutlmv2.feature_extraction_layoutlmv2', 'models.layoutlmv2.image_processing_layoutlmv2', 'models.layoutlmv2.image_processing_layoutlmv2_fast', 'models.layoutlmv2.modeling_layoutlmv2', 'models.layoutlmv2.processing_layoutlmv2', 'models.layoutlmv2.tokenization_layoutlmv2', 'models.layoutlmv2.tokenization_layoutlmv2_fast', 'models.layoutlmv3', 'models.layoutlmv3.configuration_layoutlmv3', 'models.layoutlmv3.feature_extraction_layoutlmv3', 'models.layoutlmv3.image_processing_layoutlmv3', 'models.layoutlmv3.image_processing_layoutlmv3_fast', 'models.layoutlmv3.modeling_layoutlmv3', 'models.layoutlmv3.modeling_tf_layoutlmv3', 'models.layoutlmv3.processing_layoutlmv3', 'models.layoutlmv3.tokenization_layoutlmv3', 'models.layoutlmv3.tokenization_layoutlmv3_fast', 'models.layoutxlm', 'models.layoutxlm.processing_layoutxlm', 'models.layoutxlm.tokenization_layoutxlm', 'models.layoutxlm.tokenization_layoutxlm_fast', 'models.led', 'models.led.configuration_led', 'models.led.modeling_led', 'models.led.modeling_tf_led', 'models.led.tokenization_led', 'models.led.tokenization_led_fast', 'models.levit', 'models.levit.configuration_levit', 'models.levit.feature_extraction_levit', 'models.levit.image_processing_levit', 'models.levit.image_processing_levit_fast', 'models.levit.modeling_levit', 'models.lfm2', 'models.lfm2.configuration_lfm2', 'models.lfm2.modeling_lfm2', 'models.lightglue', 'models.lightglue.configuration_lightglue', 'models.lightglue.image_processing_lightglue', 'models.lightglue.modeling_lightglue', 'models.lilt', 'models.lilt.configuration_lilt', 'models.lilt.modeling_lilt', 'models.llama', 'models.llama.configuration_llama', 'models.llama.modeling_flax_llama', 'models.llama.modeling_llama', 'models.llama.tokenization_llama', 'models.llama.tokenization_llama_fast', 'models.llama4', 'models.llama4.configuration_llama4', 'models.llama4.image_processing_llama4_fast', 'models.llama4.modeling_llama4', 'models.llama4.processing_llama4', 'models.llava', 'models.llava.configuration_llava', 'models.llava.image_processing_llava', 'models.llava.image_processing_llava_fast', 'models.llava.modeling_llava', 'models.llava.processing_llava', 'models.llava_next', 'models.llava_next.configuration_llava_next', 'models.llava_next.image_processing_llava_next', 'models.llava_next.image_processing_llava_next_fast', 'models.llava_next.modeling_llava_next', 'models.llava_next.processing_llava_next', 'models.llava_next_video', 'models.llava_next_video.configuration_llava_next_video', 'models.llava_next_video.image_processing_llava_next_video', 'models.llava_next_video.modeling_llava_next_video', 'models.llava_next_video.processing_llava_next_video', 'models.llava_next_video.video_processing_llava_next_video', 'models.llava_onevision', 'models.llava_onevision.configuration_llava_onevision', 'models.llava_onevision.image_processing_llava_onevision', 'models.llava_onevision.image_processing_llava_onevision_fast', 'models.llava_onevision.modeling_llava_onevision', 'models.llava_onevision.processing_llava_onevision', 'models.llava_onevision.video_processing_llava_onevision', 'models.longformer', 'models.longformer.configuration_longformer', 'models.longformer.modeling_longformer', 'models.longformer.modeling_tf_longformer', 'models.longformer.tokenization_longformer', 'models.longformer.tokenization_longformer_fast', 'models.longt5', 'models.longt5.configuration_longt5', 'models.longt5.modeling_flax_longt5', 'models.longt5.modeling_longt5', 'models.luke', 'models.luke.configuration_luke', 'models.luke.modeling_luke', 'models.luke.tokenization_luke', 'models.lxmert', 'models.lxmert.configuration_lxmert', 'models.lxmert.modeling_lxmert', 'models.lxmert.modeling_tf_lxmert', 'models.lxmert.tokenization_lxmert', 'models.lxmert.tokenization_lxmert_fast', 'models.m2m_100', 'models.m2m_100.configuration_m2m_100', 'models.m2m_100.modeling_m2m_100', 'models.m2m_100.tokenization_m2m_100', 'models.mamba', 'models.mamba.configuration_mamba', 'models.mamba.modeling_mamba', 'models.mamba2', 'models.mamba2.configuration_mamba2', 'models.mamba2.modeling_mamba2', 'models.marian', 'models.marian.configuration_marian', 'models.marian.modeling_flax_marian', 'models.marian.modeling_marian', 'models.marian.modeling_tf_marian', 'models.marian.tokenization_marian', 'models.markuplm', 'models.markuplm.configuration_markuplm', 'models.markuplm.feature_extraction_markuplm', 'models.markuplm.modeling_markuplm', 'models.markuplm.processing_markuplm', 'models.markuplm.tokenization_markuplm', 'models.markuplm.tokenization_markuplm_fast', 'models.mask2former', 'models.mask2former.configuration_mask2former', 'models.mask2former.image_processing_mask2former', 'models.mask2former.image_processing_mask2former_fast', 'models.mask2former.modeling_mask2former', 'models.maskformer', 'models.maskformer.configuration_maskformer', 'models.maskformer.configuration_maskformer_swin', 'models.maskformer.feature_extraction_maskformer', 'models.maskformer.image_processing_maskformer', 'models.maskformer.image_processing_maskformer_fast', 'models.maskformer.modeling_maskformer', 'models.maskformer.modeling_maskformer_swin', 'models.mbart', 'models.mbart.configuration_mbart', 'models.mbart.modeling_flax_mbart', 'models.mbart.modeling_mbart', 'models.mbart.modeling_tf_mbart', 'models.mbart.tokenization_mbart', 'models.mbart.tokenization_mbart_fast', 'models.mbart50', 'models.mbart50.tokenization_mbart50', 'models.mbart50.tokenization_mbart50_fast', 'models.megatron_bert', 'models.megatron_bert.configuration_megatron_bert', 'models.megatron_bert.modeling_megatron_bert', 'models.mgp_str', 'models.mgp_str.configuration_mgp_str', 'models.mgp_str.modeling_mgp_str', 'models.mgp_str.processing_mgp_str', 'models.mgp_str.tokenization_mgp_str', 'models.mimi', 'models.mimi.configuration_mimi', 'models.mimi.modeling_mimi', 'models.minimax', 'models.minimax.configuration_minimax', 'models.minimax.modeling_minimax', 'models.mistral', 'models.mistral.configuration_mistral', 'models.mistral.modeling_flax_mistral', 'models.mistral.modeling_mistral', 'models.mistral.modeling_tf_mistral', 'models.mistral3', 'models.mistral3.configuration_mistral3', 'models.mistral3.modeling_mistral3', 'models.mixtral', 'models.mixtral.configuration_mixtral', 'models.mixtral.modeling_mixtral', 'models.mlcd', 'models.mlcd.configuration_mlcd', 'models.mlcd.modeling_mlcd', 'models.mllama', 'models.mllama.configuration_mllama', 'models.mllama.image_processing_mllama', 'models.mllama.modeling_mllama', 'models.mllama.processing_mllama', 'models.mluke', 'models.mluke.tokenization_mluke', 'models.mm_grounding_dino', 'models.mm_grounding_dino.configuration_mm_grounding_dino', 'models.mm_grounding_dino.modeling_mm_grounding_dino', 'models.mobilebert', 'models.mobilebert.configuration_mobilebert', 'models.mobilebert.modeling_mobilebert', 'models.mobilebert.modeling_tf_mobilebert', 'models.mobilebert.tokenization_mobilebert', 'models.mobilebert.tokenization_mobilebert_fast', 'models.mobilenet_v1', 'models.mobilenet_v1.configuration_mobilenet_v1', 'models.mobilenet_v1.feature_extraction_mobilenet_v1', 'models.mobilenet_v1.image_processing_mobilenet_v1', 'models.mobilenet_v1.image_processing_mobilenet_v1_fast', 'models.mobilenet_v1.modeling_mobilenet_v1', 'models.mobilenet_v2', 'models.mobilenet_v2.configuration_mobilenet_v2', 'models.mobilenet_v2.feature_extraction_mobilenet_v2', 'models.mobilenet_v2.image_processing_mobilenet_v2', 'models.mobilenet_v2.image_processing_mobilenet_v2_fast', 'models.mobilenet_v2.modeling_mobilenet_v2', 'models.mobilevit', 'models.mobilevit.configuration_mobilevit', 'models.mobilevit.feature_extraction_mobilevit', 'models.mobilevit.image_processing_mobilevit', 'models.mobilevit.image_processing_mobilevit_fast', 'models.mobilevit.modeling_mobilevit', 'models.mobilevit.modeling_tf_mobilevit', 'models.mobilevitv2', 'models.mobilevitv2.configuration_mobilevitv2', 'models.mobilevitv2.modeling_mobilevitv2', 'models.modernbert', 'models.modernbert.configuration_modernbert', 'models.modernbert.modeling_modernbert', 'models.modernbert_decoder', 'models.modernbert_decoder.configuration_modernbert_decoder', 'models.modernbert_decoder.modeling_modernbert_decoder', 'models.moonshine', 'models.moonshine.configuration_moonshine', 'models.moonshine.modeling_moonshine', 'models.moshi', 'models.moshi.configuration_moshi', 'models.moshi.modeling_moshi', 'models.mpnet', 'models.mpnet.configuration_mpnet', 'models.mpnet.modeling_mpnet', 'models.mpnet.modeling_tf_mpnet', 'models.mpnet.tokenization_mpnet', 'models.mpnet.tokenization_mpnet_fast', 'models.mpt', 'models.mpt.configuration_mpt', 'models.mpt.modeling_mpt', 'models.mra', 'models.mra.configuration_mra', 'models.mra.modeling_mra', 'models.mt5', 'models.mt5.configuration_mt5', 'models.mt5.modeling_flax_mt5', 'models.mt5.modeling_mt5', 'models.mt5.modeling_tf_mt5', 'models.mt5.tokenization_mt5', 'models.mt5.tokenization_mt5_fast', 'models.musicgen', 'models.musicgen.configuration_musicgen', 'models.musicgen.modeling_musicgen', 'models.musicgen.processing_musicgen', 'models.musicgen_melody', 'models.musicgen_melody.configuration_musicgen_melody', 'models.musicgen_melody.feature_extraction_musicgen_melody', 'models.musicgen_melody.modeling_musicgen_melody', 'models.musicgen_melody.processing_musicgen_melody', 'models.mvp', 'models.mvp.configuration_mvp', 'models.mvp.modeling_mvp', 'models.mvp.tokenization_mvp', 'models.mvp.tokenization_mvp_fast', 'models.myt5', 'models.myt5.tokenization_myt5', 'models.nemotron', 'models.nemotron.configuration_nemotron', 'models.nemotron.modeling_nemotron', 'models.nllb', 'models.nllb.tokenization_nllb', 'models.nllb.tokenization_nllb_fast', 'models.nllb_moe', 'models.nllb_moe.configuration_nllb_moe', 'models.nllb_moe.modeling_nllb_moe', 'models.nougat', 'models.nougat.image_processing_nougat', 'models.nougat.image_processing_nougat_fast', 'models.nougat.processing_nougat', 'models.nougat.tokenization_nougat_fast', 'models.nystromformer', 'models.nystromformer.configuration_nystromformer', 'models.nystromformer.modeling_nystromformer', 'models.olmo', 'models.olmo.configuration_olmo', 'models.olmo.modeling_olmo', 'models.olmo2', 'models.olmo2.configuration_olmo2', 'models.olmo2.modeling_olmo2', 'models.olmoe', 'models.olmoe.configuration_olmoe', 'models.olmoe.modeling_olmoe', 'models.omdet_turbo', 'models.omdet_turbo.configuration_omdet_turbo', 'models.omdet_turbo.modeling_omdet_turbo', 'models.omdet_turbo.processing_omdet_turbo', 'models.oneformer', 'models.oneformer.configuration_oneformer', 'models.oneformer.image_processing_oneformer', 'models.oneformer.image_processing_oneformer_fast', 'models.oneformer.modeling_oneformer', 'models.oneformer.processing_oneformer', 'models.openai', 'models.openai.configuration_openai', 'models.openai.modeling_openai', 'models.openai.modeling_tf_openai', 'models.openai.tokenization_openai', 'models.openai.tokenization_openai_fast', 'models.opt', 'models.opt.configuration_opt', 'models.opt.modeling_flax_opt', 'models.opt.modeling_opt', 'models.opt.modeling_tf_opt', 'models.owlv2', 'models.owlv2.configuration_owlv2', 'models.owlv2.image_processing_owlv2', 'models.owlv2.image_processing_owlv2_fast', 'models.owlv2.modeling_owlv2', 'models.owlv2.processing_owlv2', 'models.owlvit', 'models.owlvit.configuration_owlvit', 'models.owlvit.feature_extraction_owlvit', 'models.owlvit.image_processing_owlvit', 'models.owlvit.image_processing_owlvit_fast', 'models.owlvit.modeling_owlvit', 'models.owlvit.processing_owlvit', 'models.paligemma', 'models.paligemma.configuration_paligemma', 'models.paligemma.modeling_paligemma', 'models.paligemma.processing_paligemma', 'models.patchtsmixer', 'models.patchtsmixer.configuration_patchtsmixer', 'models.patchtsmixer.modeling_patchtsmixer', 'models.patchtst', 'models.patchtst.configuration_patchtst', 'models.patchtst.modeling_patchtst', 'models.pegasus', 'models.pegasus.configuration_pegasus', 'models.pegasus.modeling_flax_pegasus', 'models.pegasus.modeling_pegasus', 'models.pegasus.modeling_tf_pegasus', 'models.pegasus.tokenization_pegasus', 'models.pegasus.tokenization_pegasus_fast', 'models.pegasus_x', 'models.pegasus_x.configuration_pegasus_x', 'models.pegasus_x.modeling_pegasus_x', 'models.perceiver', 'models.perceiver.configuration_perceiver', 'models.perceiver.feature_extraction_perceiver', 'models.perceiver.image_processing_perceiver', 'models.perceiver.image_processing_perceiver_fast', 'models.perceiver.modeling_perceiver', 'models.perceiver.tokenization_perceiver', 'models.perception_lm', 'models.perception_lm.configuration_perception_lm', 'models.perception_lm.image_processing_perception_lm_fast', 'models.perception_lm.modeling_perception_lm', 'models.perception_lm.processing_perception_lm', 'models.perception_lm.video_processing_perception_lm', 'models.persimmon', 'models.persimmon.configuration_persimmon', 'models.persimmon.modeling_persimmon', 'models.phi', 'models.phi.configuration_phi', 'models.phi.modeling_phi', 'models.phi3', 'models.phi3.configuration_phi3', 'models.phi3.modeling_phi3', 'models.phi4_multimodal', 'models.phi4_multimodal.configuration_phi4_multimodal', 'models.phi4_multimodal.feature_extraction_phi4_multimodal', 'models.phi4_multimodal.image_processing_phi4_multimodal_fast', 'models.phi4_multimodal.modeling_phi4_multimodal', 'models.phi4_multimodal.processing_phi4_multimodal', 'models.phimoe', 'models.phimoe.configuration_phimoe', 'models.phimoe.modeling_phimoe', 'models.phobert', 'models.phobert.tokenization_phobert', 'models.pix2struct', 'models.pix2struct.configuration_pix2struct', 'models.pix2struct.image_processing_pix2struct', 'models.pix2struct.modeling_pix2struct', 'models.pix2struct.processing_pix2struct', 'models.pixtral', 'models.pixtral.configuration_pixtral', 'models.pixtral.image_processing_pixtral', 'models.pixtral.image_processing_pixtral_fast', 'models.pixtral.modeling_pixtral', 'models.pixtral.processing_pixtral', 'models.plbart', 'models.plbart.configuration_plbart', 'models.plbart.modeling_plbart', 'models.plbart.tokenization_plbart', 'models.poolformer', 'models.poolformer.configuration_poolformer', 'models.poolformer.feature_extraction_poolformer', 'models.poolformer.image_processing_poolformer', 'models.poolformer.image_processing_poolformer_fast', 'models.poolformer.modeling_poolformer', 'models.pop2piano', 'models.pop2piano.configuration_pop2piano', 'models.pop2piano.feature_extraction_pop2piano', 'models.pop2piano.modeling_pop2piano', 'models.pop2piano.processing_pop2piano', 'models.pop2piano.tokenization_pop2piano', 'models.prompt_depth_anything', 'models.prompt_depth_anything.configuration_prompt_depth_anything', 'models.prompt_depth_anything.image_processing_prompt_depth_anything', 'models.prompt_depth_anything.modeling_prompt_depth_anything', 'models.prophetnet', 'models.prophetnet.configuration_prophetnet', 'models.prophetnet.modeling_prophetnet', 'models.prophetnet.tokenization_prophetnet', 'models.pvt', 'models.pvt.configuration_pvt', 'models.pvt.image_processing_pvt', 'models.pvt.image_processing_pvt_fast', 'models.pvt.modeling_pvt', 'models.pvt_v2', 'models.pvt_v2.configuration_pvt_v2', 'models.pvt_v2.modeling_pvt_v2', 'models.qwen2', 'models.qwen2.configuration_qwen2', 'models.qwen2.modeling_qwen2', 'models.qwen2.tokenization_qwen2', 'models.qwen2.tokenization_qwen2_fast', 'models.qwen2_5_omni', 'models.qwen2_5_omni.configuration_qwen2_5_omni', 'models.qwen2_5_omni.modeling_qwen2_5_omni', 'models.qwen2_5_omni.processing_qwen2_5_omni', 'models.qwen2_5_vl', 'models.qwen2_5_vl.configuration_qwen2_5_vl', 'models.qwen2_5_vl.modeling_qwen2_5_vl', 'models.qwen2_5_vl.processing_qwen2_5_vl', 'models.qwen2_audio', 'models.qwen2_audio.configuration_qwen2_audio', 'models.qwen2_audio.modeling_qwen2_audio', 'models.qwen2_audio.processing_qwen2_audio', 'models.qwen2_moe', 'models.qwen2_moe.configuration_qwen2_moe', 'models.qwen2_moe.modeling_qwen2_moe', 'models.qwen2_vl', 'models.qwen2_vl.configuration_qwen2_vl', 'models.qwen2_vl.image_processing_qwen2_vl', 'models.qwen2_vl.image_processing_qwen2_vl_fast', 'models.qwen2_vl.modeling_qwen2_vl', 'models.qwen2_vl.processing_qwen2_vl', 'models.qwen2_vl.video_processing_qwen2_vl', 'models.qwen3', 'models.qwen3.configuration_qwen3', 'models.qwen3.modeling_qwen3', 'models.qwen3_moe', 'models.qwen3_moe.configuration_qwen3_moe', 'models.qwen3_moe.modeling_qwen3_moe', 'models.rag', 'models.rag.configuration_rag', 'models.rag.modeling_rag', 'models.rag.modeling_tf_rag', 'models.rag.retrieval_rag', 'models.rag.tokenization_rag', 'models.recurrent_gemma', 'models.recurrent_gemma.configuration_recurrent_gemma', 'models.recurrent_gemma.modeling_recurrent_gemma', 'models.reformer', 'models.reformer.configuration_reformer', 'models.reformer.modeling_reformer', 'models.reformer.tokenization_reformer', 'models.reformer.tokenization_reformer_fast', 'models.regnet', 'models.regnet.configuration_regnet', 'models.regnet.modeling_flax_regnet', 'models.regnet.modeling_regnet', 'models.regnet.modeling_tf_regnet', 'models.rembert', 'models.rembert.configuration_rembert', 'models.rembert.modeling_rembert', 'models.rembert.modeling_tf_rembert', 'models.rembert.tokenization_rembert', 'models.rembert.tokenization_rembert_fast', 'models.resnet', 'models.resnet.configuration_resnet', 'models.resnet.modeling_flax_resnet', 'models.resnet.modeling_resnet', 'models.resnet.modeling_tf_resnet', 'models.roberta', 'models.roberta.configuration_roberta', 'models.roberta.modeling_flax_roberta', 'models.roberta.modeling_roberta', 'models.roberta.modeling_tf_roberta', 'models.roberta.tokenization_roberta', 'models.roberta.tokenization_roberta_fast', 'models.roberta_prelayernorm', 'models.roberta_prelayernorm.configuration_roberta_prelayernorm', 'models.roberta_prelayernorm.modeling_flax_roberta_prelayernorm', 'models.roberta_prelayernorm.modeling_roberta_prelayernorm', 'models.roberta_prelayernorm.modeling_tf_roberta_prelayernorm', 'models.roc_bert', 'models.roc_bert.configuration_roc_bert', 'models.roc_bert.modeling_roc_bert', 'models.roc_bert.tokenization_roc_bert', 'models.roformer', 'models.roformer.configuration_roformer', 'models.roformer.modeling_flax_roformer', 'models.roformer.modeling_roformer', 'models.roformer.modeling_tf_roformer', 'models.roformer.tokenization_roformer', 'models.roformer.tokenization_roformer_fast', 'models.rt_detr', 'models.rt_detr.configuration_rt_detr', 'models.rt_detr.configuration_rt_detr_resnet', 'models.rt_detr.image_processing_rt_detr', 'models.rt_detr.image_processing_rt_detr_fast', 'models.rt_detr.modeling_rt_detr', 'models.rt_detr.modeling_rt_detr_resnet', 'models.rt_detr_v2', 'models.rt_detr_v2.configuration_rt_detr_v2', 'models.rt_detr_v2.modeling_rt_detr_v2', 'models.rwkv', 'models.rwkv.configuration_rwkv', 'models.rwkv.modeling_rwkv', 'models.sam', 'models.sam.configuration_sam', 'models.sam.image_processing_sam', 'models.sam.image_processing_sam_fast', 'models.sam.modeling_sam', 'models.sam.modeling_tf_sam', 'models.sam.processing_sam', 'models.sam_hq', 'models.sam_hq.configuration_sam_hq', 'models.sam_hq.modeling_sam_hq', 'models.sam_hq.processing_samhq', 'models.seamless_m4t', 'models.seamless_m4t.configuration_seamless_m4t', 'models.seamless_m4t.feature_extraction_seamless_m4t', 'models.seamless_m4t.modeling_seamless_m4t', 'models.seamless_m4t.processing_seamless_m4t', 'models.seamless_m4t.tokenization_seamless_m4t', 'models.seamless_m4t.tokenization_seamless_m4t_fast', 'models.seamless_m4t_v2', 'models.seamless_m4t_v2.configuration_seamless_m4t_v2', 'models.seamless_m4t_v2.modeling_seamless_m4t_v2', 'models.segformer', 'models.segformer.configuration_segformer', 'models.segformer.feature_extraction_segformer', 'models.segformer.image_processing_segformer', 'models.segformer.image_processing_segformer_fast', 'models.segformer.modeling_segformer', 'models.segformer.modeling_tf_segformer', 'models.seggpt', 'models.seggpt.configuration_seggpt', 'models.seggpt.image_processing_seggpt', 'models.seggpt.modeling_seggpt', 'models.sew', 'models.sew.configuration_sew', 'models.sew.modeling_sew', 'models.sew_d', 'models.sew_d.configuration_sew_d', 'models.sew_d.modeling_sew_d', 'models.shieldgemma2', 'models.shieldgemma2.configuration_shieldgemma2', 'models.shieldgemma2.modeling_shieldgemma2', 'models.shieldgemma2.processing_shieldgemma2', 'models.siglip', 'models.siglip.configuration_siglip', 'models.siglip.image_processing_siglip', 'models.siglip.image_processing_siglip_fast', 'models.siglip.modeling_siglip', 'models.siglip.processing_siglip', 'models.siglip.tokenization_siglip', 'models.siglip2', 'models.siglip2.configuration_siglip2', 'models.siglip2.image_processing_siglip2', 'models.siglip2.image_processing_siglip2_fast', 'models.siglip2.modeling_siglip2', 'models.siglip2.processing_siglip2', 'models.smollm3', 'models.smollm3.configuration_smollm3', 'models.smollm3.modeling_smollm3', 'models.smolvlm', 'models.smolvlm.configuration_smolvlm', 'models.smolvlm.image_processing_smolvlm', 'models.smolvlm.image_processing_smolvlm_fast', 'models.smolvlm.modeling_smolvlm', 'models.smolvlm.processing_smolvlm', 'models.smolvlm.video_processing_smolvlm', 'models.speech_encoder_decoder', 'models.speech_encoder_decoder.configuration_speech_encoder_decoder', 'models.speech_encoder_decoder.modeling_flax_speech_encoder_decoder', 'models.speech_encoder_decoder.modeling_speech_encoder_decoder', 'models.speech_to_text', 'models.speech_to_text.configuration_speech_to_text', 'models.speech_to_text.feature_extraction_speech_to_text', 'models.speech_to_text.modeling_speech_to_text', 'models.speech_to_text.modeling_tf_speech_to_text', 'models.speech_to_text.processing_speech_to_text', 'models.speech_to_text.tokenization_speech_to_text', 'models.speecht5', 'models.speecht5.configuration_speecht5', 'models.speecht5.feature_extraction_speecht5', 'models.speecht5.modeling_speecht5', 'models.speecht5.processing_speecht5', 'models.speecht5.tokenization_speecht5', 'models.splinter', 'models.splinter.configuration_splinter', 'models.splinter.modeling_splinter', 'models.splinter.tokenization_splinter', 'models.splinter.tokenization_splinter_fast', 'models.squeezebert', 'models.squeezebert.configuration_squeezebert', 'models.squeezebert.modeling_squeezebert', 'models.squeezebert.tokenization_squeezebert', 'models.squeezebert.tokenization_squeezebert_fast', 'models.stablelm', 'models.stablelm.configuration_stablelm', 'models.stablelm.modeling_stablelm', 'models.starcoder2', 'models.starcoder2.configuration_starcoder2', 'models.starcoder2.modeling_starcoder2', 'models.superglue', 'models.superglue.configuration_superglue', 'models.superglue.image_processing_superglue', 'models.superglue.modeling_superglue', 'models.superpoint', 'models.superpoint.configuration_superpoint', 'models.superpoint.image_processing_superpoint', 'models.superpoint.image_processing_superpoint_fast', 'models.superpoint.modeling_superpoint', 'models.swiftformer', 'models.swiftformer.configuration_swiftformer', 'models.swiftformer.modeling_swiftformer', 'models.swiftformer.modeling_tf_swiftformer', 'models.swin', 'models.swin.configuration_swin', 'models.swin.modeling_swin', 'models.swin.modeling_tf_swin', 'models.swin2sr', 'models.swin2sr.configuration_swin2sr', 'models.swin2sr.image_processing_swin2sr', 'models.swin2sr.image_processing_swin2sr_fast', 'models.swin2sr.modeling_swin2sr', 'models.swinv2', 'models.swinv2.configuration_swinv2', 'models.swinv2.modeling_swinv2', 'models.switch_transformers', 'models.switch_transformers.configuration_switch_transformers', 'models.switch_transformers.modeling_switch_transformers', 'models.t5', 'models.t5.configuration_t5', 'models.t5.modeling_flax_t5', 'models.t5.modeling_t5', 'models.t5.modeling_tf_t5', 'models.t5.tokenization_t5', 'models.t5.tokenization_t5_fast', 'models.t5gemma', 'models.t5gemma.configuration_t5gemma', 'models.t5gemma.modeling_t5gemma', 'models.table_transformer', 'models.table_transformer.configuration_table_transformer', 'models.table_transformer.modeling_table_transformer', 'models.tapas', 'models.tapas.configuration_tapas', 'models.tapas.modeling_tapas', 'models.tapas.modeling_tf_tapas', 'models.tapas.tokenization_tapas', 'models.textnet', 'models.textnet.configuration_textnet', 'models.textnet.image_processing_textnet', 'models.textnet.modeling_textnet', 'models.time_series_transformer', 'models.time_series_transformer.configuration_time_series_transformer', 'models.time_series_transformer.modeling_time_series_transformer', 'models.timesfm', 'models.timesfm.configuration_timesfm', 'models.timesfm.modeling_timesfm', 'models.timesformer', 'models.timesformer.configuration_timesformer', 'models.timesformer.modeling_timesformer', 'models.timm_backbone', 'models.timm_backbone.configuration_timm_backbone', 'models.timm_backbone.modeling_timm_backbone', 'models.timm_wrapper', 'models.timm_wrapper.configuration_timm_wrapper', 'models.timm_wrapper.image_processing_timm_wrapper', 'models.timm_wrapper.modeling_timm_wrapper', 'models.trocr', 'models.trocr.configuration_trocr', 'models.trocr.modeling_trocr', 'models.trocr.processing_trocr', 'models.tvp', 'models.tvp.configuration_tvp', 'models.tvp.image_processing_tvp', 'models.tvp.modeling_tvp', 'models.tvp.processing_tvp', 'models.udop', 'models.udop.configuration_udop', 'models.udop.modeling_udop', 'models.udop.processing_udop', 'models.udop.tokenization_udop', 'models.udop.tokenization_udop_fast', 'models.umt5', 'models.umt5.configuration_umt5', 'models.umt5.modeling_umt5', 'models.unispeech', 'models.unispeech.configuration_unispeech', 'models.unispeech.modeling_unispeech', 'models.unispeech_sat', 'models.unispeech_sat.configuration_unispeech_sat', 'models.unispeech_sat.modeling_unispeech_sat', 'models.univnet', 'models.univnet.configuration_univnet', 'models.univnet.feature_extraction_univnet', 'models.univnet.modeling_univnet', 'models.upernet', 'models.upernet.configuration_upernet', 'models.upernet.modeling_upernet', 'models.video_llava', 'models.video_llava.configuration_video_llava', 'models.video_llava.image_processing_video_llava', 'models.video_llava.modeling_video_llava', 'models.video_llava.processing_video_llava', 'models.video_llava.video_processing_video_llava', 'models.videomae', 'models.videomae.configuration_videomae', 'models.videomae.feature_extraction_videomae', 'models.videomae.image_processing_videomae', 'models.videomae.modeling_videomae', 'models.vilt', 'models.vilt.configuration_vilt', 'models.vilt.feature_extraction_vilt', 'models.vilt.image_processing_vilt', 'models.vilt.image_processing_vilt_fast', 'models.vilt.modeling_vilt', 'models.vilt.processing_vilt', 'models.vipllava', 'models.vipllava.configuration_vipllava', 'models.vipllava.modeling_vipllava', 'models.vision_encoder_decoder', 'models.vision_encoder_decoder.configuration_vision_encoder_decoder', 'models.vision_encoder_decoder.modeling_flax_vision_encoder_decoder', 'models.vision_encoder_decoder.modeling_tf_vision_encoder_decoder', 'models.vision_encoder_decoder.modeling_vision_encoder_decoder', 'models.vision_text_dual_encoder', 'models.vision_text_dual_encoder.configuration_vision_text_dual_encoder', 'models.vision_text_dual_encoder.modeling_flax_vision_text_dual_encoder', 'models.vision_text_dual_encoder.modeling_tf_vision_text_dual_encoder', 'models.vision_text_dual_encoder.modeling_vision_text_dual_encoder', 'models.vision_text_dual_encoder.processing_vision_text_dual_encoder', 'models.visual_bert', 'models.visual_bert.configuration_visual_bert', 'models.visual_bert.modeling_visual_bert', 'models.vit', 'models.vit.configuration_vit', 'models.vit.feature_extraction_vit', 'models.vit.image_processing_vit', 'models.vit.image_processing_vit_fast', 'models.vit.modeling_flax_vit', 'models.vit.modeling_tf_vit', 'models.vit.modeling_vit', 'models.vit_mae', 'models.vit_mae.configuration_vit_mae', 'models.vit_mae.modeling_tf_vit_mae', 'models.vit_mae.modeling_vit_mae', 'models.vit_msn', 'models.vit_msn.configuration_vit_msn', 'models.vit_msn.modeling_vit_msn', 'models.vitdet', 'models.vitdet.configuration_vitdet', 'models.vitdet.modeling_vitdet', 'models.vitmatte', 'models.vitmatte.configuration_vitmatte', 'models.vitmatte.image_processing_vitmatte', 'models.vitmatte.image_processing_vitmatte_fast', 'models.vitmatte.modeling_vitmatte', 'models.vitpose', 'models.vitpose.configuration_vitpose', 'models.vitpose.image_processing_vitpose', 'models.vitpose.modeling_vitpose', 'models.vitpose_backbone', 'models.vitpose_backbone.configuration_vitpose_backbone', 'models.vitpose_backbone.modeling_vitpose_backbone', 'models.vits', 'models.vits.configuration_vits', 'models.vits.modeling_vits', 'models.vits.tokenization_vits', 'models.vivit', 'models.vivit.configuration_vivit', 'models.vivit.image_processing_vivit', 'models.vivit.modeling_vivit', 'models.vjepa2', 'models.vjepa2.configuration_vjepa2', 'models.vjepa2.modeling_vjepa2', 'models.vjepa2.video_processing_vjepa2', 'models.voxtral', 'models.voxtral.configuration_voxtral', 'models.voxtral.modeling_voxtral', 'models.voxtral.processing_voxtral', 'models.wav2vec2', 'models.wav2vec2.configuration_wav2vec2', 'models.wav2vec2.feature_extraction_wav2vec2', 'models.wav2vec2.modeling_flax_wav2vec2', 'models.wav2vec2.modeling_tf_wav2vec2', 'models.wav2vec2.modeling_wav2vec2', 'models.wav2vec2.processing_wav2vec2', 'models.wav2vec2.tokenization_wav2vec2', 'models.wav2vec2_bert', 'models.wav2vec2_bert.configuration_wav2vec2_bert', 'models.wav2vec2_bert.modeling_wav2vec2_bert', 'models.wav2vec2_bert.processing_wav2vec2_bert', 'models.wav2vec2_conformer', 'models.wav2vec2_conformer.configuration_wav2vec2_conformer', 'models.wav2vec2_conformer.modeling_wav2vec2_conformer', 'models.wav2vec2_phoneme', 'models.wav2vec2_phoneme.tokenization_wav2vec2_phoneme', 'models.wav2vec2_with_lm', 'models.wav2vec2_with_lm.processing_wav2vec2_with_lm', 'models.wavlm', 'models.wavlm.configuration_wavlm', 'models.wavlm.modeling_wavlm', 'models.whisper', 'models.whisper.configuration_whisper', 'models.whisper.feature_extraction_whisper', 'models.whisper.modeling_flax_whisper', 'models.whisper.modeling_tf_whisper', 'models.whisper.modeling_whisper', 'models.whisper.processing_whisper', 'models.whisper.tokenization_whisper', 'models.whisper.tokenization_whisper_fast', 'models.x_clip', 'models.x_clip.configuration_x_clip', 'models.x_clip.modeling_x_clip', 'models.x_clip.processing_x_clip', 'models.xglm', 'models.xglm.configuration_xglm', 'models.xglm.modeling_flax_xglm', 'models.xglm.modeling_tf_xglm', 'models.xglm.modeling_xglm', 'models.xglm.tokenization_xglm', 'models.xglm.tokenization_xglm_fast', 'models.xlm', 'models.xlm.configuration_xlm', 'models.xlm.modeling_tf_xlm', 'models.xlm.modeling_xlm', 'models.xlm.tokenization_xlm', 'models.xlm_roberta', 'models.xlm_roberta.configuration_xlm_roberta', 'models.xlm_roberta.modeling_flax_xlm_roberta', 'models.xlm_roberta.modeling_tf_xlm_roberta', 'models.xlm_roberta.modeling_xlm_roberta', 'models.xlm_roberta.tokenization_xlm_roberta', 'models.xlm_roberta.tokenization_xlm_roberta_fast', 'models.xlm_roberta_xl', 'models.xlm_roberta_xl.configuration_xlm_roberta_xl', 'models.xlm_roberta_xl.modeling_xlm_roberta_xl', 'models.xlnet', 'models.xlnet.configuration_xlnet', 'models.xlnet.modeling_tf_xlnet', 'models.xlnet.modeling_xlnet', 'models.xlnet.tokenization_xlnet', 'models.xlnet.tokenization_xlnet_fast', 'models.xlstm', 'models.xlstm.configuration_xlstm', 'models.xlstm.modeling_xlstm', 'models.xmod', 'models.xmod.configuration_xmod', 'models.xmod.modeling_xmod', 'models.yolos', 'models.yolos.configuration_yolos', 'models.yolos.feature_extraction_yolos', 'models.yolos.image_processing_yolos', 'models.yolos.image_processing_yolos_fast', 'models.yolos.modeling_yolos', 'models.yoso', 'models.yoso.configuration_yoso', 'models.yoso.modeling_yoso', 'models.zamba', 'models.zamba.configuration_zamba', 'models.zamba.modeling_zamba', 'models.zamba2', 'models.zamba2.configuration_zamba2', 'models.zamba2.modeling_zamba2', 'models.zoedepth', 'models.zoedepth.configuration_zoedepth', 'models.zoedepth.image_processing_zoedepth', 'models.zoedepth.image_processing_zoedepth_fast', 'models.zoedepth.modeling_zoedepth', 'onnx', 'optimization', 'optimization_tf', 'pipeline', 'pipelines', 'processing_utils', 'prune_layer', 'pytorch_utils', 'quantizers', 'requires_backends', 'sagemaker', 'set_seed', 'shape_list', 'squad_convert_examples_to_features', 'testing_utils', 'tf_utils', 'time_series_utils', 'tokenization_utils', 'tokenization_utils_base', 'tokenization_utils_fast', 'torch_distributed_zero_first', 'trainer', 'trainer_callback', 'trainer_pt_utils', 'trainer_seq2seq', 'trainer_utils', 'training_args', 'training_args_seq2seq', 'training_args_tf', 'utils', 'utils.dummy_flax_objects', 'utils.dummy_mistral_common_objects', 'utils.quantization_config', 'video_processing_utils', 'video_utils', 'xLSTMConfig', 'xLSTMForCausalLM', 'xLSTMModel', 'xLSTMPreTrainedModel', 'xnli_compute_metrics', 'xnli_output_modes', 'xnli_processors', 'xnli_tasks_num_labels']\n",
            "4.55.2\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "import transformers\n",
        "print(dir(transformers))\n",
        "\n",
        "\n",
        "import transformers\n",
        "print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536,
          "referenced_widgets": [
            "cb0e60b6a843405fb787537b47e625e4",
            "9fc9b098eed045f29435d158e1894880",
            "033e667eabc5485bb6fc65e384a807bd",
            "22b78a1437d5421ca5266c039a731331",
            "f092a740399d498196353117f33a0769",
            "9d40b0bf16d5459f9b335dd2dcc96e27",
            "46949ad981b84e56b3faf6d8a81dfb36",
            "3a80003f88954572b1b67d7ad6dcb9af",
            "8dace1bfd3334ef496e321f52dfb6e5b",
            "0911a57a2b61429ab59828f9a91e773c",
            "6d93372b35274c3688da1b59c436d5ef",
            "4ffd213086ae4beb99a1df8f7a37a70f",
            "ab6dc94000ce47f4aa93e479ca1e6a33",
            "e3624d14b9d0406088c79cc18ecdc353",
            "6d843f6c5b5a4d55925335b650a5f4f0",
            "16807e72e16a4cacbfc852e9e837d2a6",
            "b0baedcd84e9464e9b72d6a9eb072586",
            "b466644a21a74534bc4b5436adf3dbd8",
            "c9710249468f45698b8835acb81e2327",
            "707d22ccc25a4a699de972eae796c552",
            "8b8130a53199406686bc2c2e7bd0d62e",
            "92d9538043cb46059506a780dca7677f",
            "b262fc198fbb44c1b142e54e10c90b0f",
            "7a2541034fdb4fa9b7a40735ab095707",
            "83348cd56f5140868c1e1282ba2f57b9",
            "e3e1b7ba66ee4438a50fbca313252971",
            "e3ffbfb1c435424685aadc63c99bad99",
            "957b3f5ae1174d3aa66e3c434e108b3d",
            "099a69077f2b4bdea877fea34ccb2c5f",
            "3d2ce34fe6be4ad1ac0e6f1496563569",
            "93643830812e44f4ad09ce590db2891b",
            "98cd74e9470840c49ef2110f021f693f",
            "99bd37655f2c4bcba636cb6660fccfb5",
            "2ff052165e4b4f8bb66ac125c9d4898e",
            "7ea11124407e41c98e0e858a6a3a9d1d",
            "5e5f7592cb1946819aa9eab92621e039",
            "c801f21d496544e8a43ce160f0350730",
            "3634bd2a648f47e7aeee5ec877e35073",
            "a44d469feb5f4ef69a239e7948bd9209",
            "5f341e54b2554627b620425dc9413087",
            "3e4175860ce14859b8a917cb94dced88",
            "8b06bdc9634a4136b6ca01e64f72c7c7",
            "a077bf4e3bb842c9b52722b93aa6456c",
            "699bb850407049848622eeb09d8d238c",
            "95173318fc7a4ec383af67e53338f12f",
            "8ccc8835e29d4a708abbd014aebdbcff",
            "fa5c6ccea7c64cb0b0a0b901ec527c20",
            "17672cde267f44bda95149424bd48547",
            "884683e1114049a4ba5b126748b009c9",
            "aac16e38c1d045b5b1206a4811c16c61",
            "3761897e30f34388ad21fbbb46fc335e",
            "d8c8f38510674cb89c688a89f74b2f21",
            "10056199befc4d98b8f79e1a8d446e58",
            "2cc7a08b639b43c7b58976fd472a703e",
            "b280a716903c408eab5eb943f3ad75f8"
          ]
        },
        "id": "D4F9dtYMz27o",
        "outputId": "8cf685b2-6dd8-471a-afe8-87382668e1d7"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'BertModel' from 'transformers' (/Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertModel, BertTokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m head_view\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained model and tokenizer\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'BertModel' from 'transformers' (/Users/jesusvillotamiranda/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)"
          ]
        }
      ],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "from bertviz import head_view\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Encode a sample sentence\n",
        "sentence = \"The central bank raised interest rates to control inflation.\"\n",
        "inputs = tokenizer.encode_plus(sentence, return_tensors='pt', add_special_tokens=True)\n",
        "input_ids = inputs['input_ids']\n",
        "\n",
        "# Get attention weights\n",
        "outputs = model(input_ids)\n",
        "\n",
        "# Extract attention from the model outputs\n",
        "attentions = outputs.attentions\n",
        "\n",
        "# Decode the tokens for visualization\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "# Visualize attention\n",
        "head_view(attentions, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScOHb3Ub3nQX"
      },
      "source": [
        "**Interpretation**\n",
        "\n",
        "*“The central bank raised interest rates to control inflation.”*\n",
        "\n",
        "- **Layer 0, Head 0:** We can see attention focused on \"central\" when looking at \"bank\" (indicating a connection between the institution \"central bank\") and on \"raised\" when looking at \"interest rates\" (indicating the model understands the verb-object relationship).\n",
        "\n",
        "\n",
        "- **Layer 11, Head 0 (deeper layer):** This head shows attention focused broadly across the entire sentence, reflecting a more global understanding that combines all parts of the sentence to infer broader economic implications.\n",
        "\n",
        "**Takeaways:**\n",
        "\n",
        "- **Attention Diversity:** different heads focus on different aspects of the sentence. Some might focus on syntactic roles (like verbs and their subjects), while others focus on semantic roles (like nouns associated with specific adjectives).\n",
        "\n",
        "- **Contextual Understanding:** attention allows the model to understand context. For example, if analyzing financial text, attention might highlight important economic terms related to the central topic of the sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l7Y1cRlvzrO"
      },
      "source": [
        "# **Interaction with LLMs from an Application Programming Interface (API)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-PkF5WlBeWJ"
      },
      "source": [
        "In this exercise, we will explore how to interact with a Large Language Model (LLM) using prompt engineering. Prompt engineering involves crafting inputs (prompts) that guide the model to produce desired outputs. Understanding how to construct prompts and control the model's output is essential for effectively using LLMs in various applications, such as answering questions, generating text, and more.\n",
        "\n",
        "#### **Step-by-Step Explanation of the Code**\n",
        "\n",
        "1. **Importing Libraries and Initializing the Client**:\n",
        "\n",
        "   We start by importing necessary libraries, such as `Groq`, which is a library that allows us to interact with the LLM. We also use `os` to manage environment variables and `userdata` from Google Colab to securely retrieve the user's API key. The API key is required to authenticate our requests to the LLM, ensuring secure and authorized access to the model.\n",
        "\n",
        "   After retrieving the API key with `userdata.get('GROQ_API_KEY')`, we create a Groq client using `Groq(api_key=GROQ_API_KEY)`. This client is our main interface for sending prompts to the LLM and receiving generated responses.\n",
        "\n",
        "2. **Generating Text Using the LLM**:\n",
        "\n",
        "   We define a function, `generate_text(prompt)`, that takes a single argument, ‘prompt’. This function sends a request to the LLM to generate text based on the provided prompt.\n",
        "\n",
        "   Inside this function, we use `client.chat.completions.create(...)` to send the request. Several important parameters are passed to this function:\n",
        "\n",
        "   - **‘messages’**: This parameter is a list of dictionaries specifying the context and content of the conversation. It includes a system message and a user message:\n",
        "     - The **system message** (`{\"role\": \"system\", \"content\": \"You are an assistant specialized in economics.\"}`) sets the tone or behavior of the LLM. It tells the model to act as an assistant with expertise in economics, which influences the style and content of its responses.\n",
        "     - The **user message** (`{\"role\": \"user\", \"content\": prompt}`) is the actual input or query provided by the user. It represents the question or task for which we want the model to generate a response.\n",
        "\n",
        "   - **‘model’**: Specifies the version of the LLM to use. For instance, `\"llama3-8b-8192\"` indicates a particular model configuration optimized for certain tasks. Different models may have different strengths, such as better handling of longer text or more nuanced language understanding.\n",
        "\n",
        "   - **‘temperature’**: This parameter controls the randomness or creativity of the generated text. It ranges from 0 to 1:\n",
        "     - A **low temperature** (closer to 0) makes the output more deterministic and focused. The model will choose the most likely next word, leading to more predictable and repetitive responses.\n",
        "     - A **higher temperature** (closer to 1) allows for more randomness, making the output more creative and diverse. For example, with a higher temperature, if you prompt the model with \"What is economics?\", it might provide a broader range of interpretations or more varied explanations.\n",
        "     - For this exercise, we set the temperature to `0.5`, balancing between creativity and predictability. This means the model's responses will have some variability but remain coherent and relevant.\n",
        "\n",
        "   - **‘max_tokens’**: This parameter sets the maximum length of the generated response in tokens. Tokens can be as short as one character or as long as one word. For example, if ‘max_tokens’ is set to `100`, the response will not exceed 100 tokens. This helps control the length of the output and prevent excessively long responses.\n",
        "\n",
        "   - **‘top_p’**: This parameter controls diversity via nucleus sampling:\n",
        "     - When `top_p` is set to `1`, it considers all possible options for generating the next word (full probability distribution).\n",
        "     - If `top_p` is set to a lower value (e.g., 0.9), the model only considers the smallest set of words whose cumulative probability is at least 0.9. This effectively narrows down the choices to the most likely options, reducing the chance of generating less relevant or unexpected words.\n",
        "     - This parameter is useful when you want to balance between more deterministic outputs (like using a low ‘top_p’) and more varied outputs (using a higher ‘top_p’).\n",
        "\n",
        "   - **‘stop’**: This parameter specifies a sequence where the model should stop generating further content. For example, if you set `stop=['\\n']`, the model would stop generating when it encounters a newline character. This is particularly useful to prevent the model from generating unnecessary or irrelevant text beyond a certain point. In our code, ‘stop’ is set to `None`, meaning there is no predefined stopping condition, and the model will generate text until it reaches the token limit or completes the thought.\n",
        "\n",
        "   - **‘stream’**: This boolean parameter determines whether the response is streamed back to the user in parts or sent as a complete message. When set to `False`, the model waits to send the entire response at once. Streaming can be useful in real-time applications where immediate feedback is needed, but for our exercise, a full response suffices.\n",
        "\n",
        "3. **Defining Prompts and Generating Responses**:\n",
        "\n",
        "   We define two sample prompts (`prompt1` and `prompt2`) to demonstrate the model's ability to respond to different queries:\n",
        "   \n",
        "   - `prompt1 = \"Explain the concept of inflation in simple terms.\"`: A straightforward query asking for an explanation of inflation. This prompt is intended to get a concise, easily understandable answer suitable for someone new to economics.\n",
        "   \n",
        "   - `prompt2 = \"What are the potential impacts of raising interest rates?\"`: A more complex query that seeks a deeper analysis of economic policy. This prompt is designed to encourage the model to discuss multiple aspects and consequences of interest rate changes, providing a more nuanced response.\n",
        "\n",
        "   By using `print(generate_text(prompt1))` and `print(generate_text(prompt2))`, we generate and display the responses from the LLM for these prompts. This allows us to observe how the model interprets and responds to different types of questions based on its training and the provided context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "running_in_colab = False\n",
        "\n",
        "if running_in_colab:\n",
        "    from google.colab import userdata\n",
        "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "else:\n",
        "    import os\n",
        "    from dotenv import load_dotenv\n",
        "    # fetch from .env \n",
        "    load_dotenv()\n",
        "    GROQ_API_KEY = os.getenv('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ct3J-3m-DId",
        "outputId": "35dd44f9-8f5f-4fbf-aa32-a71029697b45"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a Groq client\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m client \u001b[38;5;241m=\u001b[39m Groq(api_key\u001b[38;5;241m=\u001b[39mGROQ_API_KEY)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text\u001b[39m(prompt, temperature, max_tokens, top_p):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text using the LLM with the given prompt.\"\"\"\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/groq/_client.py:99\u001b[0m, in \u001b[0;36mGroq.__init__\u001b[0;34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.groq.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    100\u001b[0m     version\u001b[38;5;241m=\u001b[39m__version__,\n\u001b[1;32m    101\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mbase_url,\n\u001b[1;32m    102\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    103\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    104\u001b[0m     http_client\u001b[38;5;241m=\u001b[39mhttp_client,\n\u001b[1;32m    105\u001b[0m     custom_headers\u001b[38;5;241m=\u001b[39mdefault_headers,\n\u001b[1;32m    106\u001b[0m     custom_query\u001b[38;5;241m=\u001b[39mdefault_query,\n\u001b[1;32m    107\u001b[0m     _strict_response_validation\u001b[38;5;241m=\u001b[39m_strict_response_validation,\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mChat(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mEmbeddings(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/groq/_base_client.py:824\u001b[0m, in \u001b[0;36mSyncAPIClient.__init__\u001b[0;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    809\u001b[0m     )\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    812\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    813\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m     _strict_response_validation\u001b[38;5;241m=\u001b[39m_strict_response_validation,\n\u001b[1;32m    823\u001b[0m )\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m http_client \u001b[38;5;129;01mor\u001b[39;00m SyncHttpxClientWrapper(\n\u001b[1;32m    825\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mbase_url,\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;66;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;00m\n\u001b[1;32m    827\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mcast(Timeout, timeout),\n\u001b[1;32m    828\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    829\u001b[0m     transport\u001b[38;5;241m=\u001b[39mtransport,\n\u001b[1;32m    830\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[1;32m    831\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    832\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/groq/_base_client.py:722\u001b[0m, in \u001b[0;36m_DefaultHttpxClient.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimits\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[1;32m    721\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "# Create a Groq client\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "def generate_text(prompt, temperature, max_tokens, top_p):\n",
        "    \"\"\"Generate text using the LLM with the given prompt.\"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant specialized in economics.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        top_p=top_p,\n",
        "        stop=None,\n",
        "        stream=False,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# For now, let's set the following parameters:\n",
        "temperature = 0.5\n",
        "max_tokens = 100\n",
        "top_p = 1\n",
        "\n",
        "# User-defined prompts\n",
        "prompt1 = \"Explain the concept of inflation in simple terms.\"\n",
        "prompt2 = \"What are the potential impacts of raising interest rates?\"\n",
        "\n",
        "print(' '*50, \"Response to Prompt 1:\\n\\n\", generate_text(prompt1, temperature, max_tokens, top_p))\n",
        "print('_'*200 + '\\n')\n",
        "print(' '*50, \"Response to Prompt 2:\\n\\n\", generate_text(prompt2, temperature, max_tokens, top_p))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMllP6LsCeAb"
      },
      "source": [
        "## **Hands On!**\n",
        "Now we will craft more complex prompts and experiment with different parameter settings to see how they affect the output.\n",
        "\n",
        "### **Try It Yourself: Adjust the Prompt and Parameters**\n",
        "\n",
        "You have the freedom to modify the following components:\n",
        "- **`YOUR_PROMPT`**: Change this to any prompt you like. Think about what specific information or style you want from the model. Are you looking for a simple explanation, a detailed analysis, or creative writing? Tailor your prompt accordingly.\n",
        "- **`YOUR_temperature`**: This parameter controls the creativity of the model's output.\n",
        "  - **Low Values (closer to 0)**: Make the model more deterministic and focused. It will choose the most likely next word based on its training, leading to more predictable and repetitive responses. Use a low temperature if you want concise, factual answers.\n",
        "  - **Higher Values (closer to 1)**: Make the model's output more creative and diverse. It introduces more randomness, which can be useful for generating creative content, brainstorming ideas, or exploring multiple perspectives. However, it may also produce less relevant or more verbose content.\n",
        "  - **Example**: Try setting `YOUR_temperature = 0.7` for more varied and creative answers, or `YOUR_temperature = 0.2` for straightforward, factual responses.\n",
        "  \n",
        "- **`YOUR_max_tokens`**: This parameter sets the maximum length of the generated output.\n",
        "  - **Higher Values**: Allow the model to generate longer responses, which can be useful for detailed explanations or storytelling. However, setting this too high may result in unnecessarily long outputs or off-topic content.\n",
        "  - **Lower Values**: Limit the length of the response, useful when you want short, concise answers or when working within constraints such as character limits.\n",
        "  - **Example**: Experiment with `YOUR_max_tokens = 150` for short, concise answers, or `YOUR_max_tokens = 500` for more detailed responses.\n",
        "\n",
        "- **`YOUR_top_p`**: This parameter affects the diversity of the model's output through nucleus sampling.\n",
        "  - **High Values (closer to 1)**: Allow the model to consider a broader range of possible words, increasing the diversity of the output. Use a high `top_p` for creative tasks where varied output is desired.\n",
        "  - **Lower Values**: Limit the model to choosing from the top few options, making the output more focused and less diverse.\n",
        "  - **Example**: Set `YOUR_top_p = 0.9` to maintain some diversity while keeping responses relevant, or `YOUR_top_p = 0.3` to get more focused and direct answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvlMBMQECj37",
        "outputId": "0cedb9ce-5ff0-4780-8b89-f7a95e873438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   Response to your prompt:\n",
            "\n",
            " A challenge!\n",
            "\n",
            "The Phillips Curve is a fundamental concept in macroeconomics, relating the rate of unemployment to the rate of inflation. I'll provide a rigorous mathematical treatment, using LaTeX code to represent the formulas.\n",
            "\n",
            "**The Phillips Curve**\n",
            "\n",
            "The Phillips Curve is typically represented as:\n",
            "\n",
            "$$\\pi = \\alpha + \\beta \\cdot u$$\n",
            "\n",
            "where:\n",
            "\n",
            "* $\\pi$ is the rate of inflation (measured as the percentage change in the price level)\n",
            "* $u$ is the rate of unemployment (measured as a percentage)\n",
            "* $\\alpha$ is the intercept, representing the natural rate of unemployment\n",
            "* $\\beta$ is the slope, representing the trade-off between inflation and unemployment\n",
            "\n",
            "**Theoretical Underpinnings**\n",
            "\n",
            "The Phillips Curve is based on the idea that there is a trade-off between inflation and unemployment. When the economy is experiencing high unemployment, there is a surplus of labor, which leads to downward pressure on wages and prices. Conversely, when the economy is experiencing low unemployment, there is a shortage of labor, which leads to upward pressure on wages and prices.\n",
            "\n",
            "The curve is often derived from the following equations:\n",
            "\n",
            "1. The labor market equilibrium condition:\n",
            "\n",
            "$$L = L^* + \\frac{\\partial L}{\\partial w} \\cdot (w - w^*)$$\n",
            "\n",
            "where:\n",
            "\n",
            "* $L$ is the actual labor supply\n",
            "* $L^*$ is the natural labor supply\n",
            "* $w$ is the actual wage\n",
            "* $w^*$ is the natural wage\n",
            "* $\\frac{\\partial L}{\\partial w}$ is the labor supply elasticity\n",
            "\n",
            "2. The price-setting equation:\n",
            "\n",
            "$$P = P^* \\cdot (1 + \\pi)$$\n",
            "\n",
            "where:\n",
            "\n",
            "* $P$ is the actual price level\n",
            "* $P^*$ is the natural price level\n",
            "* $\\pi$ is the rate of inflation\n",
            "\n",
            "3. The wage-setting equation:\n",
            "\n",
            "$$w = w^* \\cdot (1 + \\pi)$$\n",
            "\n",
            "where:\n",
            "\n",
            "* $w$ is the actual wage\n",
            "* $w^*$ is the natural wage\n",
            "\n",
            "**Deriving the Phillips Curve**\n",
            "\n",
            "Taking the logarithm of the price-setting equation and differentiating with respect to time, we get:\n",
            "\n",
            "$$\\frac{\\partial \\ln P}{\\partial t} = \\frac{\\partial \\ln P^*}{\\partial t} + \\pi$$\n",
            "\n",
            "Substituting the wage-setting equation and the labor market equilibrium condition, we can express the rate of inflation as:\n",
            "\n",
            "$$\\pi = \\frac{\\partial \\ln P}{\\partial t} = \\frac{\\partial \\ln P^*}{\\partial t} + \\frac{\\partial \\ln L}{\\partial t} \\cdot \\frac{\\partial w}{\\partial L}$$\n",
            "\n",
            "Simplifying and rearranging, we obtain:\n",
            "\n",
            "$$\\pi = \\alpha + \\beta \\cdot u$$\n",
            "\n",
            "where:\n",
            "\n",
            "* $\\alpha = \\frac{\\partial \\ln P^*}{\\partial t}$\n",
            "* $\\beta = \\frac{\\partial \\ln L}{\\partial t} \\cdot \\frac{\\partial w}{\\partial L}$\n",
            "\n",
            "**Conclusion**\n",
            "\n",
            "The Phillips Curve is a fundamental concept in macroeconomics, relating the rate of unemployment to the rate of inflation. The mathematical formalism presented above provides a rigorous derivation of the curve, highlighting the trade-off between inflation and unemployment. The slope of the curve, $\\beta$, represents the trade-off between the two variables, while the intercept, $\\alpha$, represents the natural rate of unemployment.\n",
            "\n",
            "I hope this meets your expectations, Professor!\n"
          ]
        }
      ],
      "source": [
        "YOUR_temperature = 0\n",
        "YOUR_max_tokens = 5000\n",
        "YOUR_top_p = 0\n",
        "\n",
        "YOUR_prompt = \"\"\"Explain the Philips Curve with strong mathematical formalism.\n",
        "                  Note that I am an experimented Professor in Economics and I can handle hardcore math.\n",
        "                  Provide formulas as LaTeX code.\n",
        "                  \"\"\"\n",
        "\n",
        "print(' '*50, \"Response to your prompt:\\n\\n\", generate_text(YOUR_prompt, YOUR_temperature, YOUR_max_tokens, YOUR_top_p))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIIvtgvZAXNv"
      },
      "source": [
        "## **Getting Closer to ChatGPT: Real-Time Response Streaming!**\n",
        "\n",
        "Have you noticed that our previous code generates responses all at once, in a big chunk? While this approach works, it can feel a bit slow and unresponsive, especially when generating longer, more complex answers. Waiting for the entire response to load can be frustrating – it feels like staring at a progress bar that never seems to end!\n",
        "\n",
        "### **Introducing Real-Time Response Streaming**\n",
        "\n",
        "What if we could see our LLM's thought process unfold in real-time, just like you do when chatting with ChatGPT? Good news! We can achieve this using **response streaming**. With streaming, the LLM generates and sends portions of the response as soon as they’re ready, rather than waiting to send everything at once. This allows you to see the output as it's being created, making the experience much more interactive and engaging!\n",
        "\n",
        "### **Why Use Streaming?**\n",
        "\n",
        "- **Faster Feedback**: You start seeing the response immediately, rather than waiting for the entire computation to finish. This makes the interaction feel faster and more dynamic.\n",
        "- **More Engaging**: Real-time updates make the conversation feel more natural, as if the LLM is \"thinking\" and \"typing\" its response, just like a human would.\n",
        "\n",
        "### **How to Implement Streaming in Our LLM**\n",
        "\n",
        "To enable streaming, we need to set the ‘stream’ parameter to `True`. Let's modify our LLM setup to demonstrate this functionality. We'll ask our assistant (now specializing in pure math) to explain string theory using pure math and provide the response in LaTeX.\n",
        "\n",
        "Here's a breakdown of what the code does:\n",
        "\n",
        "1. **Initialize the Client**: Just like before, we create a Groq client using our API key.\n",
        "\n",
        "2. **Set Up the Streaming Request**:\n",
        "   - We define our ‘messages’ to provide context and set up the user query.\n",
        "   - The ‘model’, ‘temperature’, ‘max_tokens’, ‘top_p’, and ‘stop’ parameters remain the same, but the key change is setting ‘stream’ to `True`.\n",
        "\n",
        "3. **Processing the Streamed Output**:\n",
        "   - Instead of waiting for a complete response, we iterate over chunks of the response as they come in using a ‘for’ loop.\n",
        "   - Each ‘chunk’ is a small piece of the response text, which we print immediately to the screen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjbiVYsEAd7h",
        "outputId": "955384e8-8450-4392-b777-37d6df047904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A challenging task! String theory is a theoretical framework in physics that attempts to reconcile quantum mechanics and general relativity. While it's often described using physical intuition and analogies, I'll try to provide a mathematically rigorous introduction. Please note that this will be a simplified and abstracted treatment, focusing on the mathematical structure rather than the physical interpretations.\n",
            "\n",
            "**The Basics**\n",
            "\n",
            "We begin with the mathematical framework of a theory of strings. Let $\\mathcal{M}$ be a smooth, connected, and orientable manifold of dimension $d$. We consider a set of closed strings, which we'll denote by $\\mathcal{S}$. Each string is a map $\\sigma:\\mathbb{R}\\to\\mathcal{M}$, where $\\mathbb{R}$ is the real line, and $\\sigma$ is a smooth embedding.\n",
            "\n",
            "The set of all strings forms a manifold, which we'll denote by $\\mathcal{M}_S$. The dimension of this manifold is $d+1$, since each string has an additional degree of freedom (its position in time).\n",
            "\n",
            "**The String Action**\n",
            "\n",
            "The action of a string is a functional that assigns a real number to each string. It's defined as:\n",
            "\n",
            "$$S[\\sigma] = \\int_{-\\infty}^{\\infty} d\\tau \\left( \\frac{1}{2} \\dot{\\sigma}^2 + \\frac{1}{2} \\sigma''^2 + \\frac{1}{\\alpha'} \\int_{-\\infty}^{\\infty} d\\tau' \\mathcal{L}(\\sigma(\\tau), \\sigma(\\tau')) \\right)$$\n",
            "\n",
            "where:\n",
            "\n",
            "* $\\dot{\\sigma}^2 = \\frac{d\\sigma}{d\\tau} \\cdot \\frac{d\\sigma}{d\\tau}$ is the kinetic term\n",
            "* $\\sigma''^2 = \\frac{d^2\\sigma}{d\\tau^2} \\cdot \\frac{d^2\\sigma}{d\\tau^2}$ is the potential term\n",
            "* $\\mathcal{L}(\\sigma(\\tau), \\sigma(\\tau'))$ is the interaction term, which we'll discuss later\n",
            "* $\\alpha'$ is a fixed constant, often referred to as the string tension\n",
            "\n",
            "The action is a functional of the string configuration $\\sigma$, and it's used to determine the dynamics of the string.\n",
            "\n",
            "**The Interaction Term**\n",
            "\n",
            "The interaction term $\\mathcal{L}(\\sigma(\\tau), \\sigma(\\tau'))$ is a bilinear form that describes the interaction between different points on the string. It's typically chosen to be:\n",
            "\n",
            "$$\\mathcal{L}(\\sigma(\\tau), \\sigma(\\tau')) = \\frac{1}{2} \\eta_{ab} \\partial_a \\sigma(\\tau) \\partial_b \\sigma(\\tau')$$\n",
            "\n",
            "where $\\eta_{ab}$ is the metric tensor on $\\mathcal{M}$, and $\\partial_a$ denotes the partial derivative with respect to the coordinate $x^a$.\n",
            "\n",
            "**The String Equations of Motion**\n",
            "\n",
            "The equations of motion for the string are obtained by varying the action with respect to the string configuration $\\sigma$. This yields the following Euler-Lagrange equations:\n",
            "\n",
            "$$\\frac{\\delta S}{\\delta \\sigma} = 0 \\Rightarrow \\ddot{\\sigma} + \\frac{1}{\\alpha'} \\int_{-\\infty}^{\\infty} d\\tau' \\eta_{ab} \\partial_a \\sigma(\\tau) \\partial_b \\sigma(\\tau') = 0$$\n",
            "\n",
            "These equations describe the motion of the string in the presence of the interaction term.\n",
            "\n",
            "**The String Spectrum**\n",
            "\n",
            "The string spectrum is the set of possible excitations of the string. It's described by the following formula:\n",
            "\n",
            "$$\\mathcal{H} = \\int_{-\\infty}^{\\infty} d\\tau \\left( \\frac{1}{2} \\dot{\\sigma}^2 + \\frac{1}{2} \\sigma''^2 \\right) + \\int_{-\\infty}^{\\infty} d\\tau \\int_{-\\infty}^{\\infty} d\\tau' \\mathcal{L}(\\sigma(\\tau), \\sigma(\\tau'))$$\n",
            "\n",
            "The string spectrum is the set of all possible values of the energy $\\mathcal{H}$.\n",
            "\n",
            "**The Duality**\n",
            "\n",
            "String theory is often described as having a dual nature, with two different types of strings: open strings and closed strings. Open strings have a boundary, while closed strings are loops. The duality is described by the following formula:\n",
            "\n",
            "$$\\sigma(\\tau) \\sim \\sigma(\\tau + 2\\pi)$$\n",
            "\n",
            "This formula maps the closed string configuration to an open string configuration, and vice versa.\n",
            "\n",
            "**The Conformal Field Theory**\n",
            "\n",
            "The string theory is often formulated using conformal field theory (CFT). A CFT is a quantum field theory that is invariant under conformal transformations. In the context of string theory, the CFT is used to describe the physics of the string at very small distances.\n",
            "\n",
            "The CFT is defined on a two-dimensional surface, which is often referred to as the worldsheet. The worldsheet is a surface that is embedded in the target space $\\mathcal{M}$.\n",
            "\n",
            "**The Summary**\n",
            "\n",
            "In this brief introduction, we've covered the basic mathematical framework of string theory. We've defined the string manifold, the string action, the interaction term, the string equations of motion, the string spectrum, the duality, and the conformal field theory. While this is just a small part of the full theory, it provides a foundation for understanding the mathematical structure of string theory.\n",
            "\n",
            "Please note that this is a highly abstracted and simplified treatment, and many important aspects of string theory have been omitted. For a more complete understanding, I recommend consulting a comprehensive textbook or research article on the subject.None"
          ]
        }
      ],
      "source": [
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant in pure math.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain string theory using pure math (in LaTeX)\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=8000,\n",
        "    top_p=1,\n",
        "    stop=None,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    print(chunk.choices[0].delta.content, end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zBDvZvyGKZT"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "\n",
        "# **Creating a Customized LLM Assistant**\n",
        "\n",
        "### Tired of the usual ChatGPT? Need something more... sophisticated?\n",
        "\n",
        "I have the solution for you. Let me present to you all... (drumroll, please) 🥁🥁🥁\n",
        "\n",
        "##  **🎉 JesusGPT! 🎉**\n",
        "\n",
        "## *Move over ChatGPT, there's a new LLM in town!*\n",
        "\n",
        "Meet **JesusGPT**: the Large Language Model that's not just a chatbot, but your personal assistant from the world of academia! He's got a sharp wit, a deep knowledge of economics, and a knack for delivering answers with a sprinkle of humor (especially when poking fun at other professions).\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "### **Why JesusGPT?**\n",
        "\n",
        "- **Expert in Economics**: JesusGPT knows his way around asset pricing, financial econometrics, and machine learning in finance. If you’ve got a question that’s too tough for ChatGPT, JesusGPT is here with his PhD-level insights.\n",
        "- **A Sense of Humor**: Unlike other models, JesusGPT isn't just about the facts. Expect a few jokes along the way – especially about those \"less rigorous\" professions. 😉\n",
        "- **Tailored for Teaching**: Currently moonlighting as a teaching assistant at CEMFI’s Summer School, JesusGPT is ready to tackle any data science question you throw at him. And he's got the patience to explain things at your level – whether you're a novice or a seasoned economist.\n",
        "\n",
        "### **What Makes JesusGPT Special?**\n",
        "\n",
        "JesusGPT isn't just another LLM. He's been customized to understand the nuances of economics and finance, respond with academic rigor, and yet keep the conversation light-hearted and engaging. Here’s a sneak peek into his personality:\n",
        "\n",
        "- **Tone**: Rigorous, formal, yet approachable. He knows his stuff but won't shy away from adding a fun twist to the conversation.\n",
        "- **Special Instructions**: We've programmed him to occasionally throw in a joke about other professions. Why? Because what's life without a little humor, right? (Besides, those engineers and doctors had it coming...)\n",
        "- **Flexibility**: Whether you're asking about the Phillips Curve, inquiring about inflation dynamics, or just curious about his opinion on the best coffee in Madrid, JesusGPT has got you covered.\n",
        "\n",
        "### **Let’s See JesusGPT in Action!**\n",
        "\n",
        "Think of a burning question related to economics, data science, or even something offbeat. JesusGPT is ready to answer! Simply type your question, and he’ll respond with his signature style.\n",
        "\n",
        "**How to interact with JesusGPT:**\n",
        "\n",
        "1. **Ask anything**: Type in any question you have about economics, finance, or data science.\n",
        "2. **Experience his expertise**: See how JesusGPT combines rigorous academic knowledge with a dash of humor.\n",
        "3. **Exit anytime**: If you're done, just type `'exit'` to end the session.\n",
        "\n",
        "Ready to meet JesusGPT? Let’s get started!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R339Hqw5vNVX",
        "outputId": "f22cf4d0-2cb3-4fa0-c428-46a68861dc5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Session #3 of the Data Science for Economics course at the CEMFI Summer School!\n",
            "\n",
            "Your Teaching Assistant Jesus is here for you: ASK HIM ANYTHING YOU WANT! [Note: to exit, type 'exit'].\n",
            "\n",
            "Ask a question to Jesus Villota (LLM): exit\n",
            "Thank you for participating, you have been awesome! Have a great day :)\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "# Retrieve the API key from environment variables\n",
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# Create a Groq client\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# Initial system message to set the tone and behavior of the assistant\n",
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"Your name is Jesus Villota and you are doing a PhD in Economics at CEMFI.\n",
        "                  Your research interests are Machine Learning in Finance, Asset Pricing and Financial Econometrics.\n",
        "                  You are currently teaching a class on Data Science for research in economics at the CEMFI Summer School.\n",
        "                  Respond with rigor and formalism but always with a pinch of humor.\n",
        "                  Occasionally, crack a joke where you laugh at other professions (you think they are inferior to yours).\n",
        "                  \"\"\"\n",
        "}\n",
        "\n",
        "def get_student_input():\n",
        "    \"\"\"Function to get input from the student.\"\"\"\n",
        "    return input(\"Ask a question to Jesus Villota (LLM): \")\n",
        "\n",
        "def generate_response(user_input):\n",
        "    \"\"\"Function to generate response from LLM.\"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            system_message,\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "        top_p=1,\n",
        "        stop=None,\n",
        "        stream=False,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# Main loop to keep the interaction going\n",
        "print(\"Welcome to the Session #3 of the Data Science for Economics course at the CEMFI Summer School!\\n\")\n",
        "print(\"Your Teaching Assistant Jesus is here for you: ASK HIM ANYTHING YOU WANT! [Note: to exit, type 'exit'].\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = get_student_input()\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Thank you for participating, you have been awesome! Have a great day :)\")\n",
        "        break\n",
        "\n",
        "    response = generate_response(user_input)\n",
        "    print(\"\\nJesus Villota (LLM) says:\\n\\n\", \"-\"*50 + '\\n',response)\n",
        "    print(\"-\"*50, \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKHF27aPMJl-"
      },
      "source": [
        "---\n",
        "\n",
        "### **How did we customize JesusGPT? (understanding the code)**\n",
        "\n",
        "#### **1. Setting the Tone with a System Message**\n",
        "\n",
        "To customize the LLM’s behavior, we use a special input called a **system message**. The system message is like giving the model a set of instructions or a persona to adopt for the conversation. It sets the tone, style, and context for all subsequent interactions.\n",
        "\n",
        "For JesusGPT, the system message tells the model:\n",
        "- **Who it is**: Jesus Villota, a PhD student in Economics at CEMFI.\n",
        "- **What it specializes in**: Machine Learning in Finance, Asset Pricing, and Financial Econometrics.\n",
        "- **How to respond**: With rigor and formalism, but also with a pinch of humor. Occasionally, it should crack jokes about other professions.\n",
        "  \n",
        "By setting this context upfront, all the responses generated by JesusGPT align with this predefined persona, making interactions more coherent and tailored to our needs.\n",
        "\n",
        "#### **2. Gathering User Input**\n",
        "\n",
        "We created a function,`get_student_input()`, to receive input from the user. This function allows users to type in their questions or prompts interactively. The input gathered here serves as the content the LLM will respond to, allowing for a dynamic and engaging experience.\n",
        "\n",
        "#### **3. Generating Responses Based on the Custom Persona**\n",
        "\n",
        "The core function, `generate_response(user_input)`, takes the user’s input and the predefined system message to generate a response from JesusGPT. Here’s how this works:\n",
        "\n",
        "- **Combining Inputs**: The function combines the `system_message` and the ‘user_input’ to form a complete set of instructions for the LLM. This tells the model both the context it should maintain (from the system message) and the specific question or task it should respond to (from the user).\n",
        "  \n",
        "- **LLM Parameters Explained**:\n",
        "  - **`temperature`**: Controls the creativity or randomness of the model’s output.\n",
        "    - **Low Values (e.g., 0.1)**: Make the model more focused and predictable. It will choose the most likely next word based on its training, resulting in more straightforward and repetitive responses. This is useful for generating factual or formal content.\n",
        "    - **Higher Values (e.g., 0.7 or 1)**: Introduce more randomness, allowing for creative and varied responses. This setting is useful when you want the model to generate more diverse or unexpected outputs, such as in creative writing or brainstorming.\n",
        "    \n",
        "  - **`max_tokens`**: Sets the maximum length of the generated response.\n",
        "    - A higher ‘max_tokens’ allows for longer, more detailed responses, which can be useful for comprehensive explanations or essays.\n",
        "    - A lower ‘max_tokens’ restricts the length of the output, ideal for brief answers or when operating within space constraints.\n",
        "    \n",
        "  - **`top_p`**: A parameter that affects diversity through **nucleus sampling**.\n",
        "    - When `top_p` is set to `1`, the model considers the entire probability distribution of possible next words, leading to more diverse outputs.\n",
        "    - Setting `top_p` to a lower value (e.g., 0.5) restricts the model to only consider the top 50% most probable next words, making the output more focused and reducing unexpected responses. It’s a way to balance creativity with relevance.\n",
        "    \n",
        "  - **`stop`**: This parameter defines a stopping point for the model when generating text.\n",
        "    - If set to a specific sequence (like a newline character or a custom string), the model will stop generating once it reaches that point. This helps prevent the model from going off-topic or producing overly long outputs.\n",
        "    - In our code, `stop` is set to `None`, meaning the model will generate text until it reaches the maximum token limit or finishes a coherent response.\n",
        "\n",
        "#### **4. Creating an Interactive Loop for Continuous Engagement**\n",
        "\n",
        "We set up a loop to continuously take user input and generate responses. This loop:\n",
        "- **Prompts the User**: Asks the user to input their question or prompt.\n",
        "- **Processes the Input**: If the input is not 'exit', it generates a response using the ‘generate_response’ function.\n",
        "- **Displays the Response**: Prints the response generated by JesusGPT, formatted for clarity.\n",
        "- **Exits Gracefully**: If the user types 'exit', the loop ends, and the session concludes with a thank-you message.\n",
        "\n",
        "This interactive setup allows users to engage directly with the customized LLM, ask questions in real-time, and receive tailored responses. It’s a practical demonstration of how LLMs can be customized for specific roles and applications, such as serving as a teaching assistant or research companion.\n",
        "\n",
        "#### **5. Further Customization**\n",
        "\n",
        "The code provided is a starting point for customization. You can further modify the system message, tweak the model parameters, or add new functionality to JesusGPT. For example:\n",
        "\n",
        "- **Change the Persona**: Adjust the system message to make JesusGPT more or less formal, or to change his area of expertise.\n",
        "- **Tweak the Humor**: Make the jokes more frequent or adjust the humor style to suit your audience.\n",
        "- **Experiment with Parameters**: Try different settings for `temperature`, `top_p`, and `max_tokens` to see how they affect the output and model behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJN_z2NeIYDX"
      },
      "source": [
        "# **Mastering LLMs: *Obtaining Structured Data from Unstructured Data***\n",
        "\n",
        "In this section, we demonstrate how to use Large Language Models (LLMs) to transform unstructured text data into structured data formats like JSON. This process is incredibly useful for extracting valuable information from free-form text, such as web pages, documents, or conversational input.\n",
        "\n",
        "### **Motivation: Why Structure Unstructured Data?**\n",
        "\n",
        "- **Data Organization**: Structured data is easier to analyze, visualize, and manipulate. By converting unstructured text into a structured format like JSON, we can leverage standard data tools and techniques to draw insights and make data-driven decisions.\n",
        "- **Automation and Efficiency**: Automating the conversion process reduces manual effort and increases the efficiency of data handling. For example, extracting ingredients and instructions from a recipe allows us to automatically populate a database or create shopping lists.\n",
        "- **Interoperability**: Structured data formats like JSON are universally accepted in various programming environments and applications, making it easier to share and use data across different systems.\n",
        "\n",
        "### **How Does the Code Work?**\n",
        "\n",
        "The code snippet uses an LLM to extract structured information from a recipe request. Here’s a breakdown of how it works:\n",
        "\n",
        "1. **Defining Data Models with Pydantic**:\n",
        "   - **`Ingredient`** and **`Recipe`** classes are defined using Pydantic, a data validation and settings management library. These models specify the structure of the data we expect from the LLM.\n",
        "   - **`Ingredient`** includes fields like `name`, `quantity`, and `quantity_unit`.\n",
        "   - **`Recipe`** comprises the `recipe_name`, a list of `ingredients`, and a list of `directions`.\n",
        "\n",
        "2. **Creating the Groq Client**:\n",
        "   - We initialize a Groq client to interact with the LLM, using the API key stored securely in Colab’s `userdata`.\n",
        "\n",
        "3. **Generating a Recipe in JSON Format**:\n",
        "   - The function `get_recipe(recipe_name: str)` sends a prompt to the LLM, asking it to fetch a recipe for the specified `recipe_name`.\n",
        "   - A **system message** sets the context, instructing the model to act as a recipe database and output data in JSON format according to the schema provided.\n",
        "   - The `response_format={\"type\": \"json_object\"}` parameter tells the LLM to return the output as a JSON object, ensuring the response is structured correctly.\n",
        "   - The model generates the response, which is then validated and parsed into a `Recipe` object using Pydantic’s `model_validate_json`.\n",
        "\n",
        "4. **Printing the Structured Recipe Data**:\n",
        "   - The code first prints the raw JSON output of the recipe for easy inspection.\n",
        "   - The `print_recipe(recipe: Recipe)` function then formats this structured data into a human-readable format, listing ingredients and directions clearly.\n",
        "\n",
        "### **Key Concepts in the Code**\n",
        "\n",
        "- **System Message**: This message is used to instruct the LLM on how to format the response. By specifying that the output must conform to a JSON schema, we guide the model to produce structured data.\n",
        "- **JSON Response Format**: By using the `response_format={\"type\": \"json_object\"}` parameter, we ensure that the LLM returns a JSON object that fits the schema provided. This is essential for downstream applications that require structured data.\n",
        "- **Data Validation with Pydantic**: Pydantic is used to validate the structure of the generated JSON and convert it into a Python object (`Recipe`). This ensures the data adheres to our expectations, reducing errors in subsequent processing.\n",
        "\n",
        "### **Why is This Useful?**\n",
        "\n",
        "- **Practical Applications**: This technique can be used in various real-world applications beyond recipes, such as parsing customer feedback, extracting key information from legal documents, or converting logs into structured formats for analysis.\n",
        "- **Enhanced Data Utilization**: By transforming unstructured text into structured data, we can apply machine learning models, statistical analysis, and other data science techniques more effectively.\n",
        "- **Automated Data Pipelines**: This approach supports the creation of automated data pipelines that ingest unstructured data, transform it into structured formats, and feed it into databases or data processing workflows.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk2nQuYMG0T6",
        "outputId": "78426b7c-41bb-47fd-9ce7-b09560f33191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recipe in JSON format:\n",
            "{\n",
            "  \"recipe_name\": \"Apple Pie\",\n",
            "  \"ingredients\": [\n",
            "    {\n",
            "      \"name\": \"All-purpose flour\",\n",
            "      \"quantity\": \"2 1/4 cups\",\n",
            "      \"quantity_unit\": \"cups\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Cold unsalted butter\",\n",
            "      \"quantity\": \"1 cup\",\n",
            "      \"quantity_unit\": \"cups\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Granulated sugar\",\n",
            "      \"quantity\": \"1/2 cup\",\n",
            "      \"quantity_unit\": \"cups\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Salt\",\n",
            "      \"quantity\": \"1/4 teaspoon\",\n",
            "      \"quantity_unit\": \"teaspoons\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Ground cinnamon\",\n",
            "      \"quantity\": \"1/2 teaspoon\",\n",
            "      \"quantity_unit\": \"teaspoons\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Ground nutmeg\",\n",
            "      \"quantity\": \"1/4 teaspoon\",\n",
            "      \"quantity_unit\": \"teaspoons\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Large eggs\",\n",
            "      \"quantity\": \"2\",\n",
            "      \"quantity_unit\": \"count\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Apple cider vinegar\",\n",
            "      \"quantity\": \"1 tablespoon\",\n",
            "      \"quantity_unit\": \"tablespoons\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Apples, peeled and sliced\",\n",
            "      \"quantity\": \"6-8\",\n",
            "      \"quantity_unit\": \"count\"\n",
            "    }\n",
            "  ],\n",
            "  \"directions\": [\n",
            "    \"Preheat oven to 375\\u00b0F (190\\u00b0C).\",\n",
            "    \"Make the crust: In a large bowl, whisk together flour, salt, and sugar. Add butter and use a pastry blender or your fingers to work it into the flour until the mixture resembles coarse crumbs.\",\n",
            "    \"Make the filling: In a separate bowl, whisk together eggs, apple cider vinegar, cinnamon, and nutmeg. Add the sliced apples and toss until they're evenly coated.\",\n",
            "    \"Assemble the pie: Roll out the crust on a lightly floured surface to a thickness of about 1/8 inch. Transfer the dough to a 9-inch pie dish and trim the edges to fit.\",\n",
            "    \"Fill the pie: Pour the apple mixture into the pie crust and spread it out evenly.\",\n",
            "    \"Cover the pie: Roll out the remaining crust to a thickness of about 1/8 inch. Use a pastry cutter or a knife to cut the crust into strips for a lattice-top crust.\",\n",
            "    \"Bake the pie: Place the pie on a baking sheet lined with parchment paper and bake for 45-50 minutes, or until the crust is golden brown and the apples are tender.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Optional\n",
        "import json\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from groq import Groq\n",
        "\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# Create a Groq client\n",
        "groq = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# Data model for LLM to generate\n",
        "class Ingredient(BaseModel):\n",
        "    name: str\n",
        "    quantity: str\n",
        "    quantity_unit: Optional[str]\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    recipe_name: str\n",
        "    ingredients: List[Ingredient]\n",
        "    directions: List[str]\n",
        "\n",
        "\n",
        "def get_recipe(recipe_name: str) -> Recipe:\n",
        "    chat_completion = groq.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a recipe database that outputs recipes in JSON.\\n\"\n",
        "                # Pass the json schema to the model. Pretty printing improves results.\n",
        "                f\" The JSON object must use the schema: {json.dumps(Recipe.model_json_schema(), indent=2)}\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Fetch a recipe for {recipe_name}\",\n",
        "            },\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",\n",
        "        temperature=0,\n",
        "        # Streaming is not supported in JSON mode\n",
        "        stream=False,\n",
        "        # Enable JSON mode by setting the response format\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "    )\n",
        "    return Recipe.model_validate_json(chat_completion.choices[0].message.content)\n",
        "\n",
        "recipe = get_recipe(\"apple pie\")\n",
        "\n",
        "print(f\"Recipe in JSON format:\\n{json.dumps(recipe.model_dump(), indent=2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErKAgvpTJqxz"
      },
      "source": [
        "### **Transforming JSON to human-readable output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5tuTekVILgO",
        "outputId": "5311248c-d06d-4a74-e8ef-98f42eb8f484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now let's put it in human-readable format: \n",
            "\n",
            "Recipe: Apple Pie\n",
            "\n",
            "Ingredients:\n",
            "- All-purpose flour: 2 1/4 cups cups\n",
            "- Cold unsalted butter: 1 cup cups\n",
            "- Granulated sugar: 1/2 cup cups\n",
            "- Salt: 1/4 teaspoon teaspoons\n",
            "- Ground cinnamon: 1/2 teaspoon teaspoons\n",
            "- Ground nutmeg: 1/4 teaspoon teaspoons\n",
            "- Large eggs: 2 count\n",
            "- Apple cider vinegar: 1 tablespoon tablespoons\n",
            "- Apples, peeled and sliced: 6-8 count\n",
            "\n",
            "Directions:\n",
            "1. Preheat oven to 375°F (190°C).\n",
            "2. Make the crust: In a large bowl, whisk together flour, salt, and sugar. Add butter and use a pastry blender or your fingers to work it into the flour until the mixture resembles coarse crumbs.\n",
            "3. Make the filling: In a separate bowl, whisk together eggs, apple cider vinegar, cinnamon, and nutmeg. Add the sliced apples and toss until they're evenly coated.\n",
            "4. Assemble the pie: Roll out the crust on a lightly floured surface to a thickness of about 1/8 inch. Transfer the dough to a 9-inch pie dish and trim the edges to fit.\n",
            "5. Fill the pie: Pour the apple mixture into the pie crust and spread it out evenly.\n",
            "6. Cover the pie: Roll out the remaining crust to a thickness of about 1/8 inch. Use a pastry cutter or a knife to cut the crust into strips for a lattice-top crust.\n",
            "7. Bake the pie: Place the pie on a baking sheet lined with parchment paper and bake for 45-50 minutes, or until the crust is golden brown and the apples are tender.\n"
          ]
        }
      ],
      "source": [
        "def print_recipe(recipe: Recipe):\n",
        "    print(\"Recipe:\", recipe.recipe_name)\n",
        "\n",
        "    print(\"\\nIngredients:\")\n",
        "    for ingredient in recipe.ingredients:\n",
        "        print(\n",
        "            f\"- {ingredient.name}: {ingredient.quantity} {ingredient.quantity_unit or ''}\"\n",
        "        )\n",
        "    print(\"\\nDirections:\")\n",
        "    for step, direction in enumerate(recipe.directions, start=1):\n",
        "        print(f\"{step}. {direction}\")\n",
        "\n",
        "print(\"Now let's put it in human-readable format: \\n\")\n",
        "print_recipe(recipe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7OzVvW5JZNj"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "### **Next Steps: Try It Yourself!**\n",
        "\n",
        "- **Experiment with Different Prompts**: Try asking for different types of recipes or modifying the system message to adjust the format or detail level.\n",
        "- **Apply to Other Domains**: Think of other unstructured data types you encounter and consider how you might use an LLM to structure that data effectively.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOqX3YFCBZfyq/vhGZdhmOl",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "033e667eabc5485bb6fc65e384a807bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a80003f88954572b1b67d7ad6dcb9af",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dace1bfd3334ef496e321f52dfb6e5b",
            "value": 570
          }
        },
        "0911a57a2b61429ab59828f9a91e773c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "099a69077f2b4bdea877fea34ccb2c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10056199befc4d98b8f79e1a8d446e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16807e72e16a4cacbfc852e9e837d2a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17672cde267f44bda95149424bd48547": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cc7a08b639b43c7b58976fd472a703e",
            "placeholder": "​",
            "style": "IPY_MODEL_b280a716903c408eab5eb943f3ad75f8",
            "value": " 466k/466k [00:00&lt;00:00, 4.96MB/s]"
          }
        },
        "22b78a1437d5421ca5266c039a731331": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0911a57a2b61429ab59828f9a91e773c",
            "placeholder": "​",
            "style": "IPY_MODEL_6d93372b35274c3688da1b59c436d5ef",
            "value": " 570/570 [00:00&lt;00:00, 17.5kB/s]"
          }
        },
        "2cc7a08b639b43c7b58976fd472a703e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ff052165e4b4f8bb66ac125c9d4898e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ea11124407e41c98e0e858a6a3a9d1d",
              "IPY_MODEL_5e5f7592cb1946819aa9eab92621e039",
              "IPY_MODEL_c801f21d496544e8a43ce160f0350730"
            ],
            "layout": "IPY_MODEL_3634bd2a648f47e7aeee5ec877e35073"
          }
        },
        "3634bd2a648f47e7aeee5ec877e35073": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3761897e30f34388ad21fbbb46fc335e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a80003f88954572b1b67d7ad6dcb9af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d2ce34fe6be4ad1ac0e6f1496563569": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4175860ce14859b8a917cb94dced88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46949ad981b84e56b3faf6d8a81dfb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ffd213086ae4beb99a1df8f7a37a70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab6dc94000ce47f4aa93e479ca1e6a33",
              "IPY_MODEL_e3624d14b9d0406088c79cc18ecdc353",
              "IPY_MODEL_6d843f6c5b5a4d55925335b650a5f4f0"
            ],
            "layout": "IPY_MODEL_16807e72e16a4cacbfc852e9e837d2a6"
          }
        },
        "5e5f7592cb1946819aa9eab92621e039": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e4175860ce14859b8a917cb94dced88",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b06bdc9634a4136b6ca01e64f72c7c7",
            "value": 231508
          }
        },
        "5f341e54b2554627b620425dc9413087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "699bb850407049848622eeb09d8d238c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d843f6c5b5a4d55925335b650a5f4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b8130a53199406686bc2c2e7bd0d62e",
            "placeholder": "​",
            "style": "IPY_MODEL_92d9538043cb46059506a780dca7677f",
            "value": " 440M/440M [00:06&lt;00:00, 62.5MB/s]"
          }
        },
        "6d93372b35274c3688da1b59c436d5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "707d22ccc25a4a699de972eae796c552": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a2541034fdb4fa9b7a40735ab095707": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_957b3f5ae1174d3aa66e3c434e108b3d",
            "placeholder": "​",
            "style": "IPY_MODEL_099a69077f2b4bdea877fea34ccb2c5f",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7ea11124407e41c98e0e858a6a3a9d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a44d469feb5f4ef69a239e7948bd9209",
            "placeholder": "​",
            "style": "IPY_MODEL_5f341e54b2554627b620425dc9413087",
            "value": "vocab.txt: 100%"
          }
        },
        "83348cd56f5140868c1e1282ba2f57b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d2ce34fe6be4ad1ac0e6f1496563569",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93643830812e44f4ad09ce590db2891b",
            "value": 48
          }
        },
        "884683e1114049a4ba5b126748b009c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b06bdc9634a4136b6ca01e64f72c7c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b8130a53199406686bc2c2e7bd0d62e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ccc8835e29d4a708abbd014aebdbcff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aac16e38c1d045b5b1206a4811c16c61",
            "placeholder": "​",
            "style": "IPY_MODEL_3761897e30f34388ad21fbbb46fc335e",
            "value": "tokenizer.json: 100%"
          }
        },
        "8dace1bfd3334ef496e321f52dfb6e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92d9538043cb46059506a780dca7677f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93643830812e44f4ad09ce590db2891b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95173318fc7a4ec383af67e53338f12f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ccc8835e29d4a708abbd014aebdbcff",
              "IPY_MODEL_fa5c6ccea7c64cb0b0a0b901ec527c20",
              "IPY_MODEL_17672cde267f44bda95149424bd48547"
            ],
            "layout": "IPY_MODEL_884683e1114049a4ba5b126748b009c9"
          }
        },
        "957b3f5ae1174d3aa66e3c434e108b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98cd74e9470840c49ef2110f021f693f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99bd37655f2c4bcba636cb6660fccfb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d40b0bf16d5459f9b335dd2dcc96e27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fc9b098eed045f29435d158e1894880": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d40b0bf16d5459f9b335dd2dcc96e27",
            "placeholder": "​",
            "style": "IPY_MODEL_46949ad981b84e56b3faf6d8a81dfb36",
            "value": "config.json: 100%"
          }
        },
        "a077bf4e3bb842c9b52722b93aa6456c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44d469feb5f4ef69a239e7948bd9209": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aac16e38c1d045b5b1206a4811c16c61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab6dc94000ce47f4aa93e479ca1e6a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0baedcd84e9464e9b72d6a9eb072586",
            "placeholder": "​",
            "style": "IPY_MODEL_b466644a21a74534bc4b5436adf3dbd8",
            "value": "model.safetensors: 100%"
          }
        },
        "b0baedcd84e9464e9b72d6a9eb072586": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b262fc198fbb44c1b142e54e10c90b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a2541034fdb4fa9b7a40735ab095707",
              "IPY_MODEL_83348cd56f5140868c1e1282ba2f57b9",
              "IPY_MODEL_e3e1b7ba66ee4438a50fbca313252971"
            ],
            "layout": "IPY_MODEL_e3ffbfb1c435424685aadc63c99bad99"
          }
        },
        "b280a716903c408eab5eb943f3ad75f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b466644a21a74534bc4b5436adf3dbd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c801f21d496544e8a43ce160f0350730": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a077bf4e3bb842c9b52722b93aa6456c",
            "placeholder": "​",
            "style": "IPY_MODEL_699bb850407049848622eeb09d8d238c",
            "value": " 232k/232k [00:00&lt;00:00, 1.49MB/s]"
          }
        },
        "c9710249468f45698b8835acb81e2327": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0e60b6a843405fb787537b47e625e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fc9b098eed045f29435d158e1894880",
              "IPY_MODEL_033e667eabc5485bb6fc65e384a807bd",
              "IPY_MODEL_22b78a1437d5421ca5266c039a731331"
            ],
            "layout": "IPY_MODEL_f092a740399d498196353117f33a0769"
          }
        },
        "d8c8f38510674cb89c688a89f74b2f21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3624d14b9d0406088c79cc18ecdc353": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9710249468f45698b8835acb81e2327",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_707d22ccc25a4a699de972eae796c552",
            "value": 440449768
          }
        },
        "e3e1b7ba66ee4438a50fbca313252971": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98cd74e9470840c49ef2110f021f693f",
            "placeholder": "​",
            "style": "IPY_MODEL_99bd37655f2c4bcba636cb6660fccfb5",
            "value": " 48.0/48.0 [00:00&lt;00:00, 831B/s]"
          }
        },
        "e3ffbfb1c435424685aadc63c99bad99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f092a740399d498196353117f33a0769": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa5c6ccea7c64cb0b0a0b901ec527c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c8f38510674cb89c688a89f74b2f21",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10056199befc4d98b8f79e1a8d446e58",
            "value": 466062
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
