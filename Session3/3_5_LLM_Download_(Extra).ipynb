{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55e3b1a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jesusvillota/CSS_DataScience_2025/blob/main/Session3/3_5_LLM_Download_(Extra).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60450c7",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 880px; margin: 20px auto 22px; padding: 0px; border-radius: 18px; border: 1px solid #e5e7eb; background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%); box-shadow: 0 8px 26px rgba(0,0,0,0.06); overflow: hidden;\">\n",
    "\n",
    "  <!-- Banner Header -->\n",
    "  <div style=\"padding: 34px 32px 14px; text-align: center; line-height: 1.38;\">\n",
    "    <div style=\"font-size: 13px; letter-spacing: 0.14em; text-transform: uppercase; color: #6b7280; font-weight: bold; margin-bottom: 5px;\">\n",
    "      Session #3\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      LLMs\n",
    "    </div>\n",
    "    <div style=\"font-size: 26px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      Extra: Downloading LLMs locally\n",
    "    </div>\n",
    "    <div style=\"font-size: 16.5px; color: #374151; font-style: italic; margin-bottom: 0;\">\n",
    "      Using Textual Data in Empirical Monetary Economics\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Logo Section -->\n",
    "  <div style=\"background: none; text-align: center; margin: 30px 0 10px;\">\n",
    "    <img src=\"https://www.cemfi.es/images/Logo-Azul.png\" alt=\"CEMFI Logo\" style=\"width: 158px; filter: drop-shadow(0 2px 12px rgba(56,84,156,0.05)); margin-bottom: 0;\">\n",
    "  </div>\n",
    "\n",
    "  <!-- Name -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1.22em; font-weight: bold; margin-bottom: 0px;\">\n",
    "    Jesus Villota Miranda © 2025\n",
    "  </div>\n",
    "\n",
    "  <!-- Contact info -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1em; margin-top: 7px; margin-bottom: 20px;\">\n",
    "    <a href=\"mailto:jesus.villota@cemfi.edu.es\" style=\"color: #38549c; text-decoration: none; margin-right:8px;\" title=\"Email\">\n",
    "      <!-- <img src=\"https://cdn-icons-png.flaticon.com/512/11679/11679732.png\" alt=\"Email\" style=\"width:18px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      jesus.villota@cemfi.edu.es\n",
    "    </a>\n",
    "    <span style=\"color:#9fa7bd;\">|</span>\n",
    "    <a href=\"https://www.linkedin.com/in/jesusvillotamiranda/\" target=\"_blank\" style=\"color: #38549c; text-decoration: none; margin-left:7px;\" title=\"LinkedIn\">\n",
    "      <!-- <img src=\"https://1.bp.blogspot.com/-onvhHUdW1Us/YI52e9j4eKI/AAAAAAAAE4c/6s9wzOpIDYcAo4YmTX1Qg51OlwMFmilFACLcBGAsYHQ/s1600/Logo%2BLinkedin.png\" alt=\"LinkedIn\" style=\"width:17px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      LinkedIn\n",
    "    </a>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf74892",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_in_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179da4fe",
   "metadata": {},
   "source": [
    "Even though there is an \"Open in Colab\" option, this notebook is intended to be run locally in your computer, as we are trying to download and run the model files directly on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f819f6",
   "metadata": {},
   "source": [
    "## Why run LLMs locally?\n",
    "\n",
    "Running small Large Language Models (LLMs) on your own machine is great for:\n",
    "- **Privacy**: your data stays on device.\n",
    "- **Cost control**: no API bills while experimenting.\n",
    "- **Offline/edge use**: works without internet.\n",
    "- **Reproducibility & customization**: full control over versions and files.\n",
    "\n",
    "In this short, hands-on lesson you will:\n",
    "- Download a compact, CPU-friendly model: Google's `gemma-3-270m` from Hugging Face.\n",
    "- Locate the downloaded files on disk using Terminal and Finder.\n",
    "- Understand the typical files in a model repository and what they do.\n",
    "\n",
    "**Hugging Face model card**: https://huggingface.co/google/gemma-3-270m\n",
    "\n",
    "### Prerequisites\n",
    "- Python installed (3.9+ recommended) and a working internet connection.\n",
    "- The package `huggingface_hub` installed. If not, install from Terminal:\n",
    "  - macOS/Linux: `pip install -U huggingface_hub`\n",
    "- A few hundred MB of free disk space.\n",
    "- Optional but recommended: a free Hugging Face account and CLI login (`huggingface-cli login`) if a model requires access/terms acceptance.\n",
    "\n",
    "**Note**: `gemma-3-270m` is intentionally small so it downloads fast and runs on typical laptops. It's ideal for learning how local LLM tooling works. Some people call these type of \"small\" models Small Language Models (SLMs) as opposed to the typical monsters with billions of parameters, which are often referred to as Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69066f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running_in_colab:\n",
    "    ! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2add6ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b32669b784c409ab4a2ace84f7af2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded at: /Users/jesusvillotamiranda/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/9b0cfec892e2bc2afd938c98eabe4e4a7b1e0ca1\n"
     ]
    }
   ],
   "source": [
    "download_model = True\n",
    "\n",
    "if download_model:\n",
    "    # Download a small, CPU-friendly model locally using Hugging Face Hub\n",
    "    # Note: Some models require accepting terms or logging in via `huggingface-cli login`.\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    model_id = \"google/gemma-3-270m\"\n",
    "\n",
    "    # This downloads the full model repo to a local cache folder and returns the path\n",
    "    local_dir = snapshot_download(model_id)\n",
    "\n",
    "    print(f\"Model downloaded at: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69918f5f",
   "metadata": {},
   "source": [
    "## DeepSeek‑R1‑Distill‑Qwen‑1.5B — distilled, in plain English\n",
    "\n",
    "### What is model distillation?\n",
    "\n",
    "- A **large** \"teacher\" model trains a **smaller** \"student\" model to mimic its outputs.\n",
    "- The student keeps most of the teacher's skill but is lighter, faster, and cheaper to run.\n",
    "- **Why it matters**: near‑teacher quality without the hardware and cost of huge models.\n",
    "\n",
    "### How it applies here\n",
    "\n",
    "- DeepSeek‑R1‑Distill‑Qwen‑1.5B is a 1.5B‑parameter student distilled from DeepSeek‑R1's reasoning models, built on the Qwen‑2.5 family.\n",
    "- You get R1‑style reasoning patterns (step‑by‑step, self‑checking) in a compact model that runs on typical laptops.\n",
    "\n",
    "### The only details you really need\n",
    "\n",
    "- **Size**: ~1.5B params → practical for local demos, teaching, and prototypes.\n",
    "- **Context**: long context support helps with longer prompts/doc chunks.\n",
    "- **Usage**: chat‑style prompting; ask it to \"think step by step\" for math/logic tasks.\n",
    "- **Sampling**: temperature ~0.5–0.7 often yields clearer, less repetitive answers.\n",
    "- **License**: permissive for classroom and projects (still review the model card).\n",
    "\n",
    "\n",
    "In summary, DeepSeek‑R1‑Distill‑Qwen‑1.5B brings R1‑style reasoning to a laptop‑friendly model. Below you'll download it from the Hub and locate it on disk for local use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef5481a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac617e796d141fcb8b653adae323580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded at: /Users/jesusvillotamiranda/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562\n"
     ]
    }
   ],
   "source": [
    "download_model = True\n",
    "\n",
    "if download_model:\n",
    "    from huggingface_hub import snapshot_download\n",
    "    model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    local_dir = snapshot_download(model_id)\n",
    "    print(f\"Model downloaded at: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b320e",
   "metadata": {},
   "source": [
    "## Where did the model download to?\n",
    "\n",
    "The function `snapshot_download` saves the full Hugging Face repository to your local cache. On macOS the default path is inside your user cache directory, for example:\n",
    "\n",
    "```\n",
    "/Users/<your-username>/.cache/huggingface/hub/models--<model>/snapshots/<commit-hash>\n",
    "```\n",
    "\n",
    "In my case:\n",
    "\n",
    "```\n",
    "/Users/jesusvillotamiranda/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/9b0cfec892e2bc2afd938c98eabe4e4a7b1e0ca1\n",
    "\n",
    "/Users/jesusvillotamiranda/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562\n",
    "```\n",
    "\n",
    "**Important**: Your exact path will be printed by the previous code cell. Use that path in the steps below.\n",
    "\n",
    "### Option A: Use Terminal (macOS/Linux)\n",
    "1. Open Terminal.\n",
    "2. Change directory to your printed path. Example:\n",
    "   ```bash\n",
    "   cd /Users/yourname/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/<commit-hash>\n",
    "   ```\n",
    "3. List files with sizes:\n",
    "   ```bash\n",
    "   ls -lh\n",
    "   ```\n",
    "\n",
    "**Tip**: press Tab to auto-complete long paths. If you get \"No such file or directory,\" double-check spaces and the exact hash.\n",
    "\n",
    "### Option B: Use Finder (macOS) or File Explorer (Windows)\n",
    "1. In Finder, press Cmd + Shift + G (Go to Folder...).\n",
    "2. Paste the path you saw printed (the snapshot folder).\n",
    "3. Press Enter to open the folder with the downloaded model files.\n",
    "\n",
    "Below are screenshots showing the Hugging Face cache structure and the model folder contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064fdb4",
   "metadata": {},
   "source": [
    "### Exploring your local Hugging Face cache\n",
    "\n",
    "In your Hugging Face cache (`~/.cache/huggingface/hub`), you'll find all models you've downloaded. In my case:\n",
    "\n",
    "![Finder view 1](images/finder_1.png)\n",
    "\n",
    "Now, drilling down into the `gemma-3-270m` snapshot folder, you'll see the actual model artifacts:\n",
    "\n",
    "![Finder view 2](images/finder_2.png)\n",
    "\n",
    "Common files you'll encounter in model repos:\n",
    "\n",
    "| File Name                       | Purpose/Description                                                                                      |\n",
    "|----------------------------------|---------------------------------------------------------------------------------------------------------|\n",
    "| `config.json`                    | Model architecture and hyperparameters used by libraries like Transformers.                             |\n",
    "| `tokenizer.json` / `tokenizer.model` / `tokenizer_config.json` | Vocabulary and rules for turning text into tokens.                                 |\n",
    "| `model.safetensors` or `pytorch_model.bin` | The neural network weights. `safetensors` is preferred for safety and speed.         |\n",
    "| `generation_config.json`         | Default generation parameters (temperature, max_new_tokens, etc.) used by convenience APIs.             |\n",
    "| `README.md` and license files    | Model card and licensing terms. Always review licensing before redistribution or commercial use.         |\n",
    "\n",
    "These files together are what frameworks load to run the model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99ca7c",
   "metadata": {},
   "source": [
    "## A quick mental model: how all the pieces fit\n",
    "\n",
    "- Hugging Face Hub is like Git for models: each model repo has versions (commits) and files.\n",
    "- `snapshot_download` fetches a specific snapshot (commit) to your local cache.\n",
    "- Loading a model later only needs the local path; no re-download unless you change versions.\n",
    "- `tokenizer` turns text into tokens; the model turns tokens into the next-token probabilities; a generation loop produces text.\n",
    "- `config` and `generation_config` tell libraries how to reconstruct the model and generate text sensibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b4528",
   "metadata": {},
   "source": [
    "## Running the model with Transformers (CPU-friendly)\n",
    "\n",
    "Once downloaded, you can load the model from the local path using `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b8b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if running_in_colab:\n",
    "    ! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d27487",
   "metadata": {},
   "source": [
    "Each model has its own way of interacting with transformers. For example, some models may require specific input formats or additional preprocessing steps. Always refer to the model's documentation for details on how to use it effectively.\n",
    "\n",
    "1. Go to the model page, and click on \"Use this model\"\n",
    "\n",
    "![Model page](images/model_page.png)\n",
    "\n",
    "2. Follow the instructions provided on the model page to integrate it into your application.\n",
    "\n",
    "![Pipeline](images/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a531366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Set parameters for comparison --\n",
    "PROMPT = \"What do economists do?\"\n",
    "MAX_TOKENS = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "527ca4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do economists do? The answer is usually nothing, unless there’s a big discrepancy with reality and you think your research might help.  The more complex this question is, the more it becomes a reason for you to stop research and find alternative sources of solutions. In one example, my colleague Dan Soper is working on a novel solution for a smallpox virus, using RNA viruses as tools to make RNA vaccines. But first he makes a great deal of changes, not just in the study of these viruses, but also in his approach to how he builds them. He starts from the idea that RNA viruses tend to be much more complex than protein viruses and that they are much more resistant to infections than they seem to be. But he still has to get all the molecular details right, and his approach has caused him a great deal of criticism.\n",
      "\n",
      "For an experiment we call “an RNA experiment,” the first part of the work is very straightforward. We make a sample from a single cell in a laboratory, remove the nucleus, and then divide the cell into two sections. We know that the nuclei are all the same size, so all we need to know is that they each carry a copy of the RNA, or mRNA, that they’re inside. Then we divide the cell into three sections. We know that the RNA in one section is transcribed to the DNA in the next section, so we know that the RNA in each section is the same size.  So we know that each section has the same content:\n",
      "\n",
      "<strong>Section 1:</strong> mRNA <strong>—</strong> <strong>The mRNA copy of the DNA in each section.</strong>\n",
      "\n",
      "<strong>Section 2:</strong> DNA <strong>—</strong> <strong>The mRNA copy of the RNA in each section.</strong>\n",
      "\n",
      "<strong>Section 3:</strong> RNA <strong>—</strong> <strong>The mRNA copy of the DNA in each section.</strong>\n",
      "\n",
      "<strong>Section 4:</strong> DNA <strong>—</strong> <strong>The mRNA copy\n"
     ]
    }
   ],
   "source": [
    "# google--gemma-3-270m\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gemma_model_path = \"/Users/jesusvillotamiranda/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/9b0cfec892e2bc2afd938c98eabe4e4a7b1e0ca1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(gemma_model_path, trust_remote_code=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=MAX_TOKENS)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a086310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I'm trying to figure out what economists do. I remember from my basic classes that economists study economics, but I'm not entirely sure what that entails. I think it's about how societies function, right? Like, things like markets, production, and resource allocation. But I'm not clear on the specific areas they study. \n",
      "\n",
      "Let me start by breaking down the term. \"Economists\" comes from the Greek word \"ekmonos,\" which means the study of money, but I think in economics it's more about the allocation of resources. So, maybe economists look at how money is used in the economy. \n",
      "\n",
      "I remember something about microeconomics and macroeconomics. Microeconomics must be about individual markets, like how a company decides what to produce or how to allocate resources within a company. Macroeconomics, on the other hand, deals with the whole economy, including things like inflation, unemployment, and government policies. \n",
      "\n",
      "Economists use different tools and methods to study these areas. I think the main tools are supply and demand, cost-benefit analysis, and game theory. Let me think about each of these. \n",
      "\n",
      "Supply and demand is pretty straightforward. It's about how prices are determined by the interaction between what producers are willing to sell and what consumers are willing to buy. If the supply increases and demand decreases, the price goes up. I think that's how markets work. \n",
      "\n",
      "Cost-benefit analysis involves looking at the costs and benefits of a project or policy. For example, if building a new highway is expensive but creates jobs, it might be worth it. But I'm not sure how detailed that gets. Maybe it's more about comparing the net benefits to the costs. \n",
      "\n",
      "Game theory is a bit more complex. I think it's used to analyze strategic interactions between individuals, businesses, or countries. Like, if two companies are deciding whether to enter a market, game theory can help predict their decisions based on what they\n"
     ]
    }
   ],
   "source": [
    "# deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "deepseek_model = \"/Users/jesusvillotamiranda/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(deepseek_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(deepseek_model)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": PROMPT},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=MAX_TOKENS)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
