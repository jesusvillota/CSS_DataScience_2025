{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55e3b1a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jesusvillota/CSS_DataScience_2025/blob/main/Session3/3_5_LLM_Download_(Extra).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60450c7",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 880px; margin: 20px auto 22px; padding: 0px; border-radius: 18px; border: 1px solid #e5e7eb; background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%); box-shadow: 0 8px 26px rgba(0,0,0,0.06); overflow: hidden;\">\n",
    "\n",
    "  <!-- Banner Header -->\n",
    "  <div style=\"padding: 34px 32px 14px; text-align: center; line-height: 1.38;\">\n",
    "    <div style=\"font-size: 13px; letter-spacing: 0.14em; text-transform: uppercase; color: #6b7280; font-weight: bold; margin-bottom: 5px;\">\n",
    "      Session #3\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      LLMs\n",
    "    </div>\n",
    "    <div style=\"font-size: 26px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      Extra: Downloading LLMs locally\n",
    "    </div>\n",
    "    <div style=\"font-size: 16.5px; color: #374151; font-style: italic; margin-bottom: 0;\">\n",
    "      Data Science for Economics: Mastering Unstructured Data\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Logo Section -->\n",
    "  <div style=\"background: none; text-align: center; margin: 30px 0 10px;\">\n",
    "    <img src=\"https://www.cemfi.es/images/Logo-Azul.png\" alt=\"CEMFI Logo\" style=\"width: 158px; filter: drop-shadow(0 2px 12px rgba(56,84,156,0.05)); margin-bottom: 0;\">\n",
    "  </div>\n",
    "\n",
    "  <!-- Name -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1.22em; font-weight: bold; margin-bottom: 0px;\">\n",
    "    Jesus Villota Miranda © 2025\n",
    "  </div>\n",
    "\n",
    "  <!-- Contact info -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1em; margin-top: 7px; margin-bottom: 20px;\">\n",
    "    <a href=\"mailto:jesus.villota@cemfi.edu.es\" style=\"color: #38549c; text-decoration: none; margin-right:8px;\" title=\"Email\">\n",
    "      <!-- <img src=\"https://cdn-icons-png.flaticon.com/512/11679/11679732.png\" alt=\"Email\" style=\"width:18px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      jesus.villota@cemfi.edu.es\n",
    "    </a>\n",
    "    <span style=\"color:#9fa7bd;\">|</span>\n",
    "    <a href=\"https://www.linkedin.com/in/jesusvillotamiranda/\" target=\"_blank\" style=\"color: #38549c; text-decoration: none; margin-left:7px;\" title=\"LinkedIn\">\n",
    "      <!-- <img src=\"https://1.bp.blogspot.com/-onvhHUdW1Us/YI52e9j4eKI/AAAAAAAAE4c/6s9wzOpIDYcAo4YmTX1Qg51OlwMFmilFACLcBGAsYHQ/s1600/Logo%2BLinkedin.png\" alt=\"LinkedIn\" style=\"width:17px; vertical-align:middle; margin-right:5px;\"> -->\n",
    "      LinkedIn\n",
    "    </a>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179da4fe",
   "metadata": {},
   "source": [
    "Even though there is an \"Open in Colab\" option, this notebook is intended to be run locally in your computer, as we are trying to download and run the model files directly on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f819f6",
   "metadata": {},
   "source": [
    "## Why run LLMs locally?\n",
    "\n",
    "Running small Large Language Models (LLMs) on your own machine is great for:\n",
    "- **Privacy**: your data stays on device.\n",
    "- **Cost control**: no API bills while experimenting.\n",
    "- **Offline/edge use**: works without internet.\n",
    "- **Reproducibility & customization**: full control over versions and files.\n",
    "\n",
    "In this short, hands-on lesson you will:\n",
    "- Download a compact, CPU-friendly model: Google's `gemma-3-270m` from Hugging Face.\n",
    "- Locate the downloaded files on disk using Terminal and Finder.\n",
    "- Understand the typical files in a model repository and what they do.\n",
    "\n",
    "**Hugging Face model card**: https://huggingface.co/google/gemma-3-270m\n",
    "\n",
    "### Prerequisites\n",
    "- Python installed (3.9+ recommended) and a working internet connection.\n",
    "- The package `huggingface_hub` installed. If not, install from Terminal:\n",
    "  - macOS/Linux: `pip install -U huggingface_hub`\n",
    "- A few hundred MB of free disk space.\n",
    "- Optional but recommended: a free Hugging Face account and CLI login (`huggingface-cli login`) if a model requires access/terms acceptance.\n",
    "\n",
    "**Note**: `gemma-3-270m` is intentionally small so it downloads fast and runs on typical laptops. It's ideal for learning how local LLM tooling works. Some people call these type of \"small\" models Small Language Models (SLMs) as opposed to the typical monsters with billions of parameters, which are often referred to as Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69066f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2add6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model = False\n",
    "\n",
    "if download_model:\n",
    "    # Download a small, CPU-friendly model locally using Hugging Face Hub\n",
    "    # Note: Some models require accepting terms or logging in via `huggingface-cli login`.\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    model_id = \"google/gemma-3-270m\"\n",
    "\n",
    "    # This downloads the full model repo to a local cache folder and returns the path\n",
    "    local_dir = snapshot_download(model_id)\n",
    "\n",
    "    print(f\"Model downloaded at: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69918f5f",
   "metadata": {},
   "source": [
    "## DeepSeek‑R1‑Distill‑Qwen‑1.5B — distilled, in plain English\n",
    "\n",
    "### What is model distillation?\n",
    "\n",
    "- A **large** \"teacher\" model trains a **smaller** \"student\" model to mimic its outputs.\n",
    "- The student keeps most of the teacher's skill but is lighter, faster, and cheaper to run.\n",
    "- **Why it matters**: near‑teacher quality without the hardware and cost of huge models.\n",
    "\n",
    "### How it applies here\n",
    "\n",
    "- DeepSeek‑R1‑Distill‑Qwen‑1.5B is a 1.5B‑parameter student distilled from DeepSeek‑R1's reasoning models, built on the Qwen‑2.5 family.\n",
    "- You get R1‑style reasoning patterns (step‑by‑step, self‑checking) in a compact model that runs on typical laptops.\n",
    "\n",
    "### The only details you really need\n",
    "\n",
    "- **Size**: ~1.5B params → practical for local demos, teaching, and prototypes.\n",
    "- **Context**: long context support helps with longer prompts/doc chunks.\n",
    "- **Usage**: chat‑style prompting; ask it to \"think step by step\" for math/logic tasks.\n",
    "- **Sampling**: temperature ~0.5–0.7 often yields clearer, less repetitive answers.\n",
    "- **License**: permissive for classroom and projects (still review the model card).\n",
    "\n",
    "\n",
    "In summary, DeepSeek‑R1‑Distill‑Qwen‑1.5B brings R1‑style reasoning to a laptop‑friendly model. Below you'll download it from the Hub and locate it on disk for local use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef5481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_model = False\n",
    "\n",
    "if download_model:\n",
    "    from huggingface_hub import snapshot_download\n",
    "    model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    local_dir = snapshot_download(model_id)\n",
    "    print(f\"Model downloaded at: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b320e",
   "metadata": {},
   "source": [
    "## Where did the model download to?\n",
    "\n",
    "The function `snapshot_download` saves the full Hugging Face repository to your local cache. On macOS the default path is inside your user cache directory, for example:\n",
    "\n",
    "```\n",
    "/Users/<your-username>/.cache/huggingface/hub/models--<model>/snapshots/<commit-hash>\n",
    "```\n",
    "\n",
    "In my case:\n",
    "\n",
    "```\n",
    "/Users/jesusvillotamiranda/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/9b0cfec892e2bc2afd938c98eabe4e4a7b1e0ca1\n",
    "\n",
    "/Users/jesusvillotamiranda/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562\n",
    "```\n",
    "\n",
    "**Important**: Your exact path will be printed by the previous code cell. Use that path in the steps below.\n",
    "\n",
    "### Option A: Use Terminal (macOS/Linux)\n",
    "1. Open Terminal.\n",
    "2. Change directory to your printed path. Example:\n",
    "   ```bash\n",
    "   cd /Users/yourname/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/<commit-hash>\n",
    "   ```\n",
    "3. List files with sizes:\n",
    "   ```bash\n",
    "   ls -lh\n",
    "   ```\n",
    "\n",
    "**Tip**: press Tab to auto-complete long paths. If you get \"No such file or directory,\" double-check spaces and the exact hash.\n",
    "\n",
    "### Option B: Use Finder (macOS) or File Explorer (Windows)\n",
    "1. In Finder, press Cmd + Shift + G (Go to Folder...).\n",
    "2. Paste the path you saw printed (the snapshot folder).\n",
    "3. Press Enter to open the folder with the downloaded model files.\n",
    "\n",
    "Below are screenshots showing the Hugging Face cache structure and the model folder contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064fdb4",
   "metadata": {},
   "source": [
    "### Exploring your local Hugging Face cache\n",
    "\n",
    "In your Hugging Face cache (`~/.cache/huggingface/hub`), you'll find all models you've downloaded. In my case:\n",
    "\n",
    "![Finder view 1](images/finder_1.png)\n",
    "\n",
    "Now, drilling down into the `gemma-3-270m` snapshot folder, you'll see the actual model artifacts:\n",
    "\n",
    "![Finder view 2](images/finder_2.png)\n",
    "\n",
    "Common files you'll encounter in model repos:\n",
    "\n",
    "| File Name                       | Purpose/Description                                                                                      |\n",
    "|----------------------------------|---------------------------------------------------------------------------------------------------------|\n",
    "| `config.json`                    | Model architecture and hyperparameters used by libraries like Transformers.                             |\n",
    "| `tokenizer.json` / `tokenizer.model` / `tokenizer_config.json` | Vocabulary and rules for turning text into tokens.                                 |\n",
    "| `model.safetensors` or `pytorch_model.bin` | The neural network weights. `safetensors` is preferred for safety and speed.         |\n",
    "| `generation_config.json`         | Default generation parameters (temperature, max_new_tokens, etc.) used by convenience APIs.             |\n",
    "| `README.md` and license files    | Model card and licensing terms. Always review licensing before redistribution or commercial use.         |\n",
    "\n",
    "These files together are what frameworks load to run the model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99ca7c",
   "metadata": {},
   "source": [
    "## A quick mental model: how all the pieces fit\n",
    "\n",
    "- Hugging Face Hub is like Git for models: each model repo has versions (commits) and files.\n",
    "- `snapshot_download` fetches a specific snapshot (commit) to your local cache.\n",
    "- Loading a model later only needs the local path; no re-download unless you change versions.\n",
    "- `tokenizer` turns text into tokens; the model turns tokens into the next-token probabilities; a generation loop produces text.\n",
    "- `config` and `generation_config` tell libraries how to reconstruct the model and generate text sensibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b4528",
   "metadata": {},
   "source": [
    "## Running the model with Transformers (CPU-friendly)\n",
    "\n",
    "Once downloaded, you can load the model from the local path using `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d27487",
   "metadata": {},
   "source": [
    "Each model has its own way of interacting with transformers. For example, some models may require specific input formats or additional preprocessing steps. Always refer to the model's documentation for details on how to use it effectively.\n",
    "\n",
    "1. Go to the model page, and click on \"Use this model\"\n",
    "\n",
    "![Model page](images/model_page.png)\n",
    "\n",
    "2. Follow the instructions provided on the model page to integrate it into your application.\n",
    "\n",
    "![Pipeline](images/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a531366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Set parameters for comparison --\n",
    "PROMPT = \"What do economists do?\"\n",
    "MAX_TOKENS = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ca4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do economists do? They analyze the economic data to see if things are actually happening and adjust their thinking or theories. They do this in a number of ways, but the most common method used is to calculate the GDP. And this is also known as a GDP projection.\n",
      "\n",
      "The idea behind GDP is that we know that the world is getting richer because more people work, eat, and save. But we have to figure out where that growth has come from, so that the economy can produce and save more money. If we do that, we will be able to estimate the economy’s impact on the money supply. This is a fairly simple math equation that we can use to calculate GDP.\n",
      "\n",
      "<h2>What is a Gini Coefficient?</h2>\n",
      "\n",
      "The Gini coefficient is a measure of the income equality in a country. One of the many uses is to find out what percentage of the population lives in poverty. It helps the population understand their status in society. It can also be used for better budgeting in a budget process. A Gini coefficient is calculated by dividing the income of a population by the population itself. Let’s take a look at it a little more closely.\n",
      "\n",
      "The Gini coefficient is usually given with one side of a triangle. So in other words, this shows the inequality between poverty and wealth. Here is a look at the table.\n",
      "\n",
      "<h3>What is a Gini Coefficient?</h3>\n",
      "\n",
      "As you can see, the inequality in the Gini coefficient is less than 0.2413. This means that the percentage of the population lives in poverty is less than 2.413. It shows how much inequality has been increased in a country.\n",
      "\n",
      "The Gini coefficient is often referred to as the wealth inequality coefficient. This is because it shows the percentage of the total population that lives in poverty. The Gini coefficient is calculated from the GDP per capita.\n",
      "\n",
      "<h2>What Is the Gini Coefficient Coefficient?</h2>\n",
      "\n",
      "In a\n"
     ]
    }
   ],
   "source": [
    "# google--gemma-3-270m\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gemma_model_path = \"/Users/jesusvillotamiranda/.cache/huggingface/hub/models--google--gemma-3-270m/snapshots/9b0cfec892e2bc2afd938c98eabe4e4a7b1e0ca1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(gemma_model_path, trust_remote_code=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=MAX_TOKENS)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a086310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out what economists do. I remember reading that economists study economics, but I'm not entirely sure what that includes. I think it's about how people spend money and make decisions. Maybe they look at things like markets, people, and businesses. But I'm not certain. Let me try to break it down.\n",
      "\n",
      "First, I know that economics is about the production, distribution, and consumption of resources. So, economists probably look into how people use money and resources. Maybe they analyze things like inflation or interest rates. But why would they do that? Well, maybe to understand how economies work and make policies. I've heard terms like GDP, GDP per capita, and all that. I think GDP is the total production of goods and services in a country, and GDP per capita is that divided by the population. That makes sense because it shows how much each person is contributing to the economy.\n",
      "\n",
      "Then there's the concept of supply and demand. I remember that from school. It's about how prices are determined by the balance between what people are willing to buy and what they can afford. So, if there's more supply than demand, prices might go down, and if demand exceeds supply, prices might go up. That's crucial for markets, right?\n",
      "\n",
      "I also think about the role of government in the economy. Maybe economists study how government policies affect the economy. For example, tax policies, fiscal policy, or regulations. They might look at how these policies impact things like GDP growth, unemployment, or inflation. That could be important for policymakers to make informed decisions.\n",
      "\n",
      "Economists might also study international economics, which involves looking at how different countries interact with each other in the global economy. This could include things like trade balances, currency fluctuations, and international trade policies. It's relevant because the global economy is complex and influenced by many factors.\n",
      "\n",
      "Another area could be labor economics, which deals with how workers are paid, how much they work, and\n"
     ]
    }
   ],
   "source": [
    "# deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "deepseek_model = \"/Users/jesusvillotamiranda/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(deepseek_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(deepseek_model)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": PROMPT},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=MAX_TOKENS)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_course_cemfi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
